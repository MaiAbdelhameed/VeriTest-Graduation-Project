{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "!pip install torch_geometric -q\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "from torch_geometric.utils import to_dense_adj, add_self_loops\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import time\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DATA_PATH = \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['done\\\\adder11.txt', 'done\\\\adder12.txt', 'done\\\\adder13.txt', 'done\\\\adder14.txt', 'done\\\\adder15.txt', 'done\\\\adder16.txt', 'done\\\\adder17.txt', 'done\\\\adder18.txt', 'done\\\\adder19.txt', 'done\\\\adder2.txt', 'done\\\\adder20.txt', 'done\\\\adder5.txt', 'done\\\\adder6.txt', 'done\\\\adder8.txt', 'done\\\\ALU10.txt', 'done\\\\ALU13.txt', 'done\\\\ALU14.txt', 'done\\\\ALU15.txt', 'done\\\\ALU2.txt', 'done\\\\ALU6.txt', 'done\\\\ALU7.txt', 'done\\\\ALU8.txt', 'done\\\\ALU9.txt', 'done\\\\comparator1.txt', 'done\\\\comparator13.txt', 'done\\\\comparator14.txt', 'done\\\\comparator15.txt', 'done\\\\comparator16.txt', 'done\\\\comparator17.txt', 'done\\\\comparator18.txt', 'done\\\\comparator19.txt', 'done\\\\comparator2.txt', 'done\\\\comparator20.txt', 'done\\\\comparator21.txt', 'done\\\\comparator22.txt', 'done\\\\comparator23.txt', 'done\\\\comparator6.txt', 'done\\\\comparator7.txt', 'done\\\\comparator8.txt', 'done\\\\comparator9.txt', 'done\\\\decoder1.txt', 'done\\\\decoder11.txt', 'done\\\\decoder12.txt', 'done\\\\decoder13.txt', 'done\\\\decoder14.txt', 'done\\\\decoder15.txt', 'done\\\\decoder16.txt', 'done\\\\decoder17.txt', 'done\\\\decoder18.txt', 'done\\\\decoder19.txt', 'done\\\\decoder2.txt', 'done\\\\decoder20.txt', 'done\\\\decoder21.txt', 'done\\\\decoder22.txt', 'done\\\\decoder23.txt', 'done\\\\decoder24.txt', 'done\\\\decoder25.txt', 'done\\\\decoder26.txt', 'done\\\\decoder27.txt', 'done\\\\decoder28.txt', 'done\\\\decoder29.txt', 'done\\\\decoder3.txt', 'done\\\\decoder30.txt', 'done\\\\decoder31.txt', 'done\\\\decoder32.txt', 'done\\\\decoder4.txt', 'done\\\\encoder1.txt', 'done\\\\encoder10.txt', 'done\\\\encoder11.txt', 'done\\\\encoder12.txt', 'done\\\\encoder13.txt', 'done\\\\encoder14.txt', 'done\\\\encoder15.txt', 'done\\\\encoder16.txt', 'done\\\\encoder17.txt', 'done\\\\encoder18.txt', 'done\\\\encoder19.txt', 'done\\\\encoder2.txt', 'done\\\\encoder20.txt', 'done\\\\encoder21.txt', 'done\\\\encoder24.txt', 'done\\\\encoder25.txt', 'done\\\\encoder3.txt', 'done\\\\encoder4.txt', 'done\\\\encoder5.txt', 'done\\\\encoder6.txt', 'done\\\\encoder7.txt', 'done\\\\encoder8.txt', 'done\\\\encoder9.txt', 'done\\\\mult1.txt', 'done\\\\mult10.txt', 'done\\\\mult11.txt', 'done\\\\mult12.txt', 'done\\\\mult13.txt', 'done\\\\mult14.txt', 'done\\\\mult15.txt', 'done\\\\mult16.txt', 'done\\\\mult17.txt', 'done\\\\mult18.txt', 'done\\\\mult19.txt', 'done\\\\mult2.txt', 'done\\\\mult3.txt', 'done\\\\mult30.txt', 'done\\\\mult31.txt', 'done\\\\mult32.txt', 'done\\\\mult33.txt', 'done\\\\mult34.txt', 'done\\\\mult35.txt', 'done\\\\mult4.txt', 'done\\\\mult5.txt', 'done\\\\mult6.txt', 'done\\\\mult8.txt', 'done\\\\mult9.txt', 'done\\\\mux1.txt', 'done\\\\mux10.txt', 'done\\\\mux11.txt', 'done\\\\mux12.txt', 'done\\\\mux13.txt', 'done\\\\mux14.txt', 'done\\\\mux15.txt', 'done\\\\mux16.txt', 'done\\\\mux17.txt', 'done\\\\mux18.txt', 'done\\\\mux19.txt', 'done\\\\mux2.txt', 'done\\\\mux20.txt', 'done\\\\mux21.txt', 'done\\\\mux22.txt', 'done\\\\mux23.txt', 'done\\\\mux24.txt', 'done\\\\mux25.txt', 'done\\\\mux26.txt', 'done\\\\mux3.txt', 'done\\\\mux4.txt', 'done\\\\mux5.txt', 'done\\\\mux6.txt', 'done\\\\mux7.txt', 'done\\\\mux8.txt', 'done\\\\mux9.txt', 'done\\\\pe1.txt', 'done\\\\pe14.txt', 'done\\\\pe15.txt', 'done\\\\pe16.txt', 'done\\\\pe17.txt', 'done\\\\pe18.txt', 'done\\\\pe19.txt', 'done\\\\pe2.txt', 'done\\\\pe20.txt', 'done\\\\pe21.txt', 'done\\\\pe22.txt', 'done\\\\pe3.txt', 'done\\\\pe4.txt', 'done\\\\pe5.txt', 'done\\\\pe6.txt', 'done\\\\pe7.txt', 'done\\\\sub1.txt', 'done\\\\sub10.txt', 'done\\\\sub11.txt', 'done\\\\sub12.txt', 'done\\\\sub2.txt', 'done\\\\sub5.txt', 'done\\\\sub7.txt', 'done\\\\sub8.txt', 'done\\\\sub9.txt']\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "def get_files_in_folder(folder_path):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "# Replace 'folder_path' with the path to the folder you want to read files from\n",
    "verilog_files = get_files_in_folder(DATA_PATH)\n",
    "with open('verilog_files.txt', 'w') as f:\n",
    "    json.dump(verilog_files, f)\n",
    "\n",
    "print(verilog_files)\n",
    "print(len(verilog_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Shuffle the dataset in-place\n",
    "# random.shuffle(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                # edge_atr = loaded_data[2]\n",
    "                label = loaded_data[3]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                # edge_atr = torch.tensor(edge_atr, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                num_nodes = x.size(0)\n",
    "                # print(num_nodes)\n",
    "                \n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                # data = Data(x=x, edge_index=edge_index, edge_attr=edge_atr ,y = y, batch = batch)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "# temp=extracting_attributes(\"./done/adder6.txt\")\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 164 Verilog files.\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\\ALU14.txt\n",
      "done\\ALU14.txt\n"
     ]
    }
   ],
   "source": [
    "print(verilog_files[0])\n",
    "print(dataset.verilog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data objects are unique.\n"
     ]
    }
   ],
   "source": [
    "def are_all_data_objects_unique(dataset):\n",
    "    data_objects = []\n",
    "    for data in dataset:\n",
    "        if data in data_objects:\n",
    "            return False\n",
    "        data_objects.append(data)\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "is_unique = are_all_data_objects_unique(dataset)\n",
    "if is_unique:\n",
    "    print(\"All data objects are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate data objects found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done\\\\mult18.txt'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = random.randint(0, len(verilog_files))\n",
    "verilog_files[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApQAAAHzCAYAAACe1o1DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAACXGklEQVR4nOzdd1iV9f/H8ec5h42AKCo4caXiVtx6cJUDKMvSysyyadMstV2/bGiWZvnVbNneGzVXlqCYe2MuDsOBkyX7nHP//riDUsHFOec+4/24Li4Tjud+nUp4nc+6dYqiKAghhBBCCHGF9FoHEEIIIYQQrk0KpRBCCCGEqBYplEIIIYQQolqkUAohhBBCiGqRQimEEEIIIapFCqUQQgghhKgWKZRCCCGEEKJapFAKIYQQQohqkUIphBBCCCGqRQqlEEIIIYSoFimUQgghhBCiWqRQCiGEEEKIapFCKYQQQgghqkUKpRBCCCGEqBYplEIIIYQQolqkUAohhBBCiGqRQimEEEIIIapFCqUQQgghhKgWKZRCCCGEEKJapFAKIYQQQohqkUIphBBCCCGqRQqlEEIIIYSoFi+tAwjhiRRFwaKAWVGwWkGvBy+dDoMOdDqd1vGEEEKIyyKFUgg7UxSF0yUWsgrNZBWaOVpoJqvIjNl6/mO99BDu70VEgBfh/3zU8jVIyRRCCOHUdIqiKFqHEMIdZZdY2HqymO2niimxqH/N9EAlPfI8/32cr0FHx9p+dA7zI9TXYKe0QgghxJWTQimEDVkVhYN5pWw+UUxafhk6wBZ/wcqfJzLIm651/Gge7INeRi2FEEI4CSmUQthI5pkyFqfnk1NqtVmRPFf589b00RPbJIhGNbztcBUhhBDi8kihFKKayqwKq48UsOlEsd2K5LnKrxNdx4+Y+oF462W0UgghhHakUApRDZlnyliUnk9eqdUhRfJcOiDYR0+cjFYKIYTQkBRKIa7QphNFrDxU4LBRyaqUX39ww0Ci6/hrmEQIIYSnkkIpxGVSFIXkY0UkHS3UOsp5+kUE0LuevxwzJIQQwqHkTjlCXCZnLZMASUcLST5WpHUMIYQQHkYKpRCXYdNx5y2T5ZKOFrLphJRKIYQQjiOFUohLlHmmjJWHC7SOcUlWHiog80yZ1jGEEEJ4CCmUQlyCMqvCovR8XGVlog5YlJ5PmVWWSAshhLA/KZRCXILVRwo0OxroSihAXqmVxCOuMaIqhBDCtUmhFOIiMs+UselEscuUyXIKsPFEsUx9CyGEsDsplEJcgFVRWOxCU93n0gGL0/OxyulgQggh7EgKpRAXcDCvlBwXmuo+lwLklFpJzZNRSiGEEPYjhVKIC9j8z/25XZkO2CzHCAkhhLAjKZRCVCG7xEJafpnLjk6WUwBTfhnZJRatowghhHBTUiiFqMLWk/YfnSzMzeblQW14qksdZt3Qy27X0QHbThbb7fmFEEJ4NimUQlRCURS2n7L/zu7Fs56jMOeUna+ijlJuO1WMIptzhBBC2IEUSiEqcbrEQonFvuXrwPpEtiR8Q7frx9r1OuVKLArZJVaHXEsIIYRnkUIpRCWyCs12ff6y4iJ+fvUJ6jZrRb/bH7Drtf7L3q9LCCGEZ5JCKUQlsgrNdv3L8ft7Mzl9KI0RT72Owcvbjlf6l14HWUVSKIUQQtieFEohKnG00Iy9JoeP7ttN0ufz6XrtLTTt2ttOVzmfVYEjBXIepRBCCNuTQinEORRFsdtIntVq5ceXJ+FfI4Shj75gl2tcSFaRWTbmCCGEsDkplEKcw6KA2U7Dk+u+fp9Du7YwbOILBNasZZ+LXIDZqo5UCiGEELYkhVKIc5jtNIKXk3WY5fNeo2nX3nS99ha7XONS2Ov1CSGE8FxSKIU4h9VOo5O/vDYFS1kZI56aaZ8LXCKLnBwkhBDCxry0DiCEs9Hb6W3W30nL8QsK4efXJp/1eXNJCaCOYL53z3UAjJvzBb4BNeySwyBvI4UQQtiYFEohzuGls98NF4vzczFtTq70a2XFRRVfs1rsd9/tIVdfTaMG9WnUqFHFR+PGjWnUqBE1a9ZEZ8fXL4QQwj1JoRTiHAYdeOltvzHntS0nKv189pEMXo/rSp3IFkz6cZ1tL3oOxVxG/fB6pKWlkZSUxOHDhzGb/93RHhgYeFbBrKx0BgQE2DWjEEII1yOFUohz6HQ6wv29OFTgfoeANwrx56kvv6z4vcViISsri8zMzIqPjIwMMjMz2bFjB4sXLyYrK+us56hVq1alpbP89w0aNMDb2zGHtQshLk5RFPX0CkXBalWX9XjpdBh0yIyEsBkplEJUIiLAiyMF9jvcXAt6HdQPPLvoGQwGGjRoQIMGDejZs2elf66kpITDhw+fVzgzMzNZs2YNGRkZ5OTkVDxep9MRHh5eZeFs1KgR9erVQ2+vxapCeDBFUThdYiGr0ExWoZmjhWayisyVzrh46SHc34uIAC/C//mo5WuQkimuiBRKISoRHuDlVmUS1PMnw/0v/6+8r68vzZo1o1mzZlU+5syZM5WOcmZmZrJz504yMjIoKiqqeLy3tzcNGzassnA2atSI0NBQ+cEmxCXKLrGw9WQx208VU2JRjwbTwwW/j5mtcKjAfNabZ1+Djo61/egc5keor8HesYUb0Sly2wwhznOq2Mz7e3K0jmFz97YJpZaf439IKIrC6dOnKy2c5f9c1XrOqgpno0aNCAwMdPhrEcJZWBWFg3mlbD5RTFp+GTrAFj/Qy58nMsibrnX8aB7sg17e3ImLkEIpRCUUReGtnacr3um7A1+DjontazntqJ/FYuHYsWNnlc1zS2dV6zmrWtPpNus5LRbYsAEOH4bwcOjYEYKCtE4lNJR5pozF6fnklFptViTPVf68NX30xDYJolENN/i7JOxGCqUQVVh1uICNx4vs8o3a0XRA97r+DGjg2iN6paWlHD58+LzS+d/fZ2dnVzy+fD3nhTYRucR6zl9+gYcfhrw8iIiAZ5+FMWOgtBQ++ggWLIBrr4XRoyEqSuu0wo7KrAqrjxSw6USx3YrkucqvE13Hj5j6gXjrnfNNqdCWrKEUogqdw/zYcLzo4g90AQrQKcxP6xjV5uPjQ9OmTWnatGmVjzl3Ped/C+fOnTvJzMyksLCw4vHe3t40aNDggkclabqeMy8PPv4Yysrgm2+gVy/w+udbt8EAHTqoI5YLF8LBg/Dhh+Drq01WYVeZZ8pYlJ5PXqm64tFRb3bLr7P5RDH7c0uJk9FKUQkZoRTiAr4+kEt6fplLj1LqUNdCjW4RonUUp3Dues5zRzgzMzM5dOjQWes5AwICLjjKaZf1nIoCOh2YTBAfD717w3vvnf+48nNg7r0XFi9WRzOjo//988ItbDpRxMpDBQ4blaxK+fUHNwwkuo6/hkmEs5ERSiEuoGsdP9Lyy7SOUS0K0FW+8VfQ6XTUrl2b2rVr06lTp0ofU76es7LCuWvXLpYsWcKxY8f47/vx0NDQCxbOBg0a4OPjczlB1V/z8+HUKXXtZGXKMwQHg7+/Omop3IaiKCQfKyLpqDqqrvWb2/LrrzxUQIlFoXc9f6ddly0cSwqlEBfQPNiHmj56ckutmn8jvxI6IMRHT7NgmZ66HAaDgfr161O/fn169OhR6WPK13NWVjqTk5PJyMiocj3nwIEDee211y4conzkMTtbnfaOjLzw49PTITQUQmQk2p38t0w6m/JcfcLl7llCCqUQF6TX6YhtEsQX+3O1jnJFFCCuSZAc+WEHl7Kes6CgoNLCeUm3ryzfKPTxx1BUpE5jV6b8v+3x41CzJtSuffbnhcvadNx5y2S5pKOF+Bp0Mv0tpFAKcTGNangTXcePzSeKXWqUUoe6K7OhLJ7XTGBgIK1bt6Z169aX/4evugoKC9Vi+Oab6uabypQXT70eMjL+3bAjXFrmmTJWHi7QOsYlWXmogHr+XrJRx8M5+VkZQjiHmPqBBPvocZUxHx0Q7KPHWN+1jwnyWIoCs2ZBXBwUFECtWhf/M48/rhbQt96CzZv/XVspXE6ZVWFRer5Lfb9ZlJ5PmVX+n/NksstbiEuUeabMpaa+x7QMkREDV5eRATfcoB5svnXrv+sqz2WxqLu7H3wQcnLUsyrXrIH69at86kGDBpGfn1/lRqLw8HDnP5/TTa08dMZlZ0QGNayhdRShEZkbEeISNarhzeCGgaw85PzTUIMbBkqZdAc1a0KjRpCZCWbz+dPZ5UcDpaTApEnQsqV6DmVICNSpc8Gnvuaaa9i3bx+ZmZksXbqUjIyMs87n9PLyqvR8zv/+vlYt573zkqvKPFPGphPFWse4bAqw8UQxV9X0le89HkoKpRCXIbqOPyUWxakXyveLCJAF8u6ioABOnrzwkUE6HezerR4vNGGCWiqt1otuypk6deo5T6WQnZ1d5dmcycnJHD58mLKyf4/RKj+f80L3XK9RQ0asLpVVUVj8z1S3K41OltMBi9PzuTcqVDYCeiAplEJcpt711LLmjKXSGBFAr3pSJt1GYaG6e7tjx8o325SvWEpLgxo1oG7dK76UTqejVq1a1KpVi44dO1b6GKvVWuX5nCkpKSxdupSsrKzzzue8UOFs2LDh5Z3P6cYO5pWS889dcFyRAuSUWknNK6NFiPw39TRSKIW4TDqdjj7hAfgadHLnCmFf3t5qqSyuYgrUYlEPMs/MhICAf6e57TQ6pNfriYiIICIigu7du1f6mNLSUo4cOVLpKOe6dev49ttvOX36dMXjdTod9erVq7JwNm7cmHr16mHwgAPbNzvw/tz2ogM2nyiSQumBpFAKcYWi6/hTz9+r4t66WvwQKN/NLffWdUOKAo0bQ+vW8O236k7vW25Rjw8qL1flI3vr16tl0s6F8lL4+PgQGRlJ5AUOYi8oKODQoUPnFc6MjAyWLl1KZmYmBQX/rlUuX89ZWeEs/5yrr+fMLrG4/F25QC3DpvwysksshPq6/5sA8S/Z5S1ENZVZFVYfKWCTI0cXrFYUHUTqCrixYyTeetf9QSouYvNm+OIL+P136N8fZswAPz/YuRNuvBHOnIGjR2H+fLj7bre49aKiKOTk5FRaOP97v/X/ruf09/e/4Cins6/nXHW4gI3Hi2z+/eNwynb2r/+TQ7u2krlrM3knsvDy8WXaX4dsfKV/6YDudf0Z0ECOLfMkUiiFsJHMM2UsTs8np9Rqt2JZ/rwh3jq+ff4hio6YSE5OluNdPIGiqDu9vf8Zic7JgYQEtVDWrQvx8f+OWHqAc9dzVjbFfvTo0bPWc9asWfOChbNBgwb4+vo6/LUoisJbO09TYrH9d43PJt1Oyp+/nfU5exdKAF+DjontXXvUWFweKZRC2JBVUUjNK2PziSJM+WU2K5blz9M0yJuudfxpFuzN2jVrMBqNvPvuu9x33302uIoQ7uW/6zmrKp2nTp0668/Uq1fvgkclhYeH23w956liM+/vybHpc5Zb/fHblBYV0bBtZxq27cSrV7d1SKEEuLdNKLX8XH/EXFwaKZRC2El2iYVtJ4vZdqq4YuRBsVpAp7/ou3a9DspvOuFr0NGpth+dwvzOW5N011138eOPP/L3339Tr149u7wOIdxZ+XrOqgpnRkZGles5q5pir1279mWNzO0+XUxC+hl7vLzzPNWljsMK5bVNgoiq5fgRX6ENKZRC2JmiKGSXWMkqNPPBdz/jVTuCui2jMFdyOoiXHsL9vagf6E24vxfhAV6E+lZdQE+ePEnr1q0ZNmwYn332mZ1fiRCep3w954UK54XWc1ZVOoOCgioe//s/d8ZxxIFBjiqUep26cXGgrKP0GFIohXCgnj170rp1axYuXIhVAbOiYLGCQQ9eOh16HZe95mjhwoWMHz+eVatWMWDAADslF0JUxWq1cvz48SoL54XWczZq1IhuD/4fvuFNHLLe0JEjlA0Dvbjtqpp2v45wDnJskBAOZDKZGDZsGDqdDoMODOigmkuMxo0bx0cffcSECRPYvn27JpsKhPBker2e8PBwwsPD6datW6WPKSsr48iRI5UWTq9a4W65eSWryIyiKG752sT5pFAK4SAFBQUcP36cpk2b2vR59Xo98+fPp3PnzsycOZNnn33Wps8vhKg+b29vmjRpQpMmTc76vNmq8Mb2U1X8KddmtqprwQ3SJz2CnDUihIOkpaUB2LxQArRr147HH3+cl19+mQMHDtj8+YUQ9mF281Vn7v76xL+kUArhICaTCYBmzZrZ5fmfe+45wsPDeeihh5Cl0UK4Bqvr3rr7kljc/PWJf0mhFMJBUlNT8fX1JSIiwi7PHxgYyDvvvMOyZcv4/vvv7XINIYRtufs9CQxu/vrEv+Q/tRAOYjKZaNKkiV3vahMfH8+IESN49NFHycvLs9t1hBC24eXmG1bc/fWJf0mhFMJBTCaTXdZPnmvOnDnk5eXx3HPP2f1aQojqMejU82fdkZdePY9SeAY3/d9YCOfjqELZuHFj/u///o+5c+eyZcsWu19PCHHldDod4f7ueeBKuL+XHBnkQeRgcyEcQFEUQkJCePbZZ5kyZYrdr1dWVkZ0dDS+vr6sW7fO5vceFkLYjj3vlPN30nJWvT+r4veZuzaj0+lo2LZLxecG3jOJ1v2usel15U45nsc93xYJ4WROnz5Nfn6+Q0YoQT3zbv78+fTp04cFCxbwwAMPOOS6QojLFx7gZbfbLhZknyJz1+azPqcoylmfK8i2/TmYVgW3HXkVlZMRSiEcYNOmTXTr1o1NmzbRtWtXh1333nvv5ZtvvmHv3r2Eh4c77LpCiIsrKSkhMTGRJauTqTfyQa3j2Ny9bUKp5SezI55C1lAK4QCpqamAfQ41v5Dp06fj4+PDpEmTHHpdIUTlTpw4wSeffMKNN95IWFgY11xzDT989hHWkiKto9mUr0FHqK9UDE8i/7WFcACTyURwcDChoaEOvW6tWrV48803+eqrr1ixYoVDry2EUKeXd+7cyauvvkrv3r2pV68ed955J4cOHeLJJ59k27ZtpKel0bNRLdxl+4oO6FTbTzbkeBiZ8hbCAe6//37++usvtm3b5vBrK4rCgAEDOHLkCDt27MDPz8/hGYTwJCUlJfz5558kJCSwaNEi0tPTCQwM5JprriE+Pp7hw4dTr169s/5MdomFBSnZGiW2vfuiQgn1leluTyIrZoVwAEcdGVQZnU7H/Pnz6dixIzNmzOCFF17QJIcQ7uzYsWMsWbKEhIQEli9fTkFBAY0bNyY+Pp74+Hj69++Pr69vlX8+1NdAZJA36flluPIojw6IDPKWMumBpFAK4QAmk4m4uDjNrt+mTRsmT57Mq6++yq233krLli01yyKEO1AUhR07drBo0SISEhLYsGEDAD179uSZZ54hLi6Odu3aXda0b9c6fqTll9krskMoQNc6/lrHEBqQKW8h7MxqteLv788bb7zBww8/rFmOwsJC2rVrR/PmzVm+fLmsbxLiMhUXF/PHH39UTGVnZmZSo0YNhgwZQnx8PMOGDaNu3bpX/PxWReG9lGxyS60uOUqpA0J89NwbFYpevr94HBmhFMLOjhw5QmlpKc2aNdM0R0BAAHPnziU2NpZvvvmGm2++WdM8QriCrKwsFi9eTEJCAitWrKCwsJDIyEhGjBhBfHw8RqPxglPZl0Ov0xHbJIgv9ufa5PkcTQHimgRJmfRQMkIphJ0lJiYSExPD7t27iYqK0joON954I2vXrmXPnj3UrFlT6zhCOBVFUdi2bVvFVPbGjRvR6/X06tWLuLg44uPjiYqKsusI/8p/7pzjSj+cdUB0HT8GNayhdRShERmhFMLOTCYTAJGRkdoG+cdbb71FmzZtePbZZ5k7d67WcYTQXFFREatWrWLRokUsWrSIQ4cOERQUxNChQ3nooYcYPnw4YWFhDssTUz+Q/bml5LnI1LcOCPbRY6wvt1n0ZFIohbAzk8lEvXr1CAgI0DoKAA0bNmTatGlMmjSJcePG0a1bN60jCeFwR48erSiQK1asoKioiGbNmjFy5Eji4+Pp168fPj4+mmTz1uuIc6Gp7/Kpbm+9THV7MpnyFsLOxo0bx759+1i3bp3WUSqYzWa6deuGXq9nw4YNGAxyxIdwb4qisHXrVhISEkhISGDz5s3o9Xr69OlTMZXdunVrp9qstulEESsPFWgd46IGNwwkWnZ2ezwZoRTCzrQ8g7IqXl5evPvuu/Tq1Yt58+ZpuvtcCHspLCzk999/rxiJPHLkCCEhIQwdOpSJEycybNgwateurXXMKkXX8afEopB0tFDrKFXqFxEgZVIAUiiFsDuTyUS/fv20jnGeHj16cN999/HMM88wcuRI6tevr3UkIart8OHDFQVy5cqVFBcX06JFC0aPHk1cXBz9+vXD29tb65iXrHc9taw5Y6k0RgTQq56USaGSKW8h7KikpAR/f3/ef/997rrrLq3jnCc7O5vWrVvTv39/vvnmG63jCHHZrFYrW7ZsqTgbcsuWLRgMBvr06VNxl5qrrrrKqaayr0T59LcONN2oU359meYW55IRSiHsKCMjA0VRnG7Ku1xoaCizZs3itttuY/z48QwZMkTrSEJcVEFBAb///jsJCQksXryYo0ePUrNmTYYNG8bjjz/O0KFDqVWrltYxbSq6jj/1/L1YlJ6v2e7v8t3ccU2CaFTDdUZ5hWPICKUQdrRs2TKGDh1Kamqq05ZKRVEYPHgw6enp7Ny5E39/GXUQziczM7PigPFVq1ZRXFzMVVddRXx8PHFxcfTp08elprKvVJlVYfWRAjadKHbYaGX5dbrV8cNYP1B2c4tKyQilEHZkMpkwGAw0atRI6yhV0ul0zJs3jw4dOvDaa6/x0ksvaR1JCKxWK5s2baqYyt62bRsGg4F+/frxyiuvEBcXx1VXXaV1TIfz1usY3LAGrWr6sjg9n5xSq92KZfnzhvjoiZVRSXERMkIphB1NnTqVb7/9tuJwc2f2/PPPM336dHbs2EHr1q21jiM80JkzZ1i5cmXFVPaxY8cIDQ1l2LBhxMfHM3ToULm7039YFYXUvDI2nyjClF9ms2JZ/jxNg7zpWsefZsHecjtFcVFSKIWwo1GjRnHy5ElWrVqldZSLKioqon379jRu3Jjff//d5TcxCNeQkZFRcZvDP/74g5KSElq3bl0xld27d2+8vGQy7WKySyxsO1nMtlPFlFjUH+t6HVgv4Se8HgV1nBN8DTo61fajU5gfob5yPq24dFIohbCjbt260aFDBz788EOto1yS8jWfn332GbfddpvWcYQbslqtbNiwoaJE7tixAy8vL4xGY0WJbNGihdYxXZaiKGSXWMkqNJNVZOZIQRlZRWbM1vMf66WHcH8v6gd68+YLTxEZGsjc11+VN5PiikihFMKOwsLCeOyxx3jmmWe0jnLJRo8ezZ9//snff/9NaGio1nGEG8jPz2fFihUkJCSwZMkSjh8/Tq1atRg+fDjx8fFcc801MpVtR4qiYFXArChYrGDQg5dOh15HRXl84IEH+P3339m7d6/GaYWr0msdQAh3lZeXx6lTp5x2d3dVZs+eTVFREU8//bTWUYQLS09PZ+7cuQwZMoSwsDBGjhzJhg0buOOOO0hKSuL48eN89tlnjBo1Ssqknel0Ogx6Hb4GPQHeenwNegx63VkjkUajkX379pGVlaVhUuHKZGGKEHZSvhHH1Qpl/fr1eeWVV3j00UcZN24cPXv21DqScAEWi4X169dXTGXv2rULb29vYmJieP3114mLi6N58+ZaxxRVMBqNACQmJjJq1CiN0whXJFPeQtjJzz//zPXXX8/Ro0cJDw/XOs5lsVgs9OjRA7PZzKZNm2RThKhUXl4ey5cvZ9GiRSxevJiTJ08SFhZ21lR2cHCw1jHFJWrZsiVDhgxh7ty5WkcRLkh+SghhJyaTCX9/f+rVq6d1lMtmMBh499136d69O3PnzmXixIlaRxJOwmQyVZwN+eeff1JWVkbbtm25++67iY+Pp0ePHhgMsjvYFcXExJCYmKh1DOGipFAKYScmk4nIyEiX3TEZHR3NAw88wHPPPceNN95Iw4YNtY4kNGCxWPjrr79ISEggISGBlJQUvL29GTBgAG+++SZxcXEut6xDVM5oNPLhhx9y6tQpateurXUc4WJkylsIO4mPj8dqtbJ48WKto1yx3NxcWrduTZ8+ffj++++1jiMcJDc3l2XLlrFo0SKWLFnCqVOnqFOnDrGxscTHx3P11VcTFBSkdUxhY2lpaTRt2pSff/6Z6667Tus4wsXICKUQdmIymejfv7/WMaolJCSE2bNnc8stt7B48WJiY2O1jiTs5ODBgxVT2atXr8ZsNtO+fXvuu+8+4uPj6datm0xlu7nIyEgaN27M6tWrpVCKyyYjlELYgaIo1KhRg2nTpjFp0iSt41SLoigMGTKE/fv3s3v3bgICArSOJGzAbDazbt26ihK5Z88efHx8GDhwIHFxccTFxdGkSROtYwoHGzt2LHv27GHTpk1aRxEuRs6hFMIOjh8/TmFhoVusLdPpdPzvf//j6NGjvPLKK1rHEdWQk5PDN998w2233Ua9evUwGo188skn9OrVix9//JFTp07x22+/8eCDD0qZ9FAxMTFs3bqV3NxcraMIFyNT3kLYgaueQVmVli1b8tRTT/HKK68wZswYoqKitI4kLtH+/fsrzoZMSkrCbDbTsWNHHnjgAeLi4ujWrRt6vYwtCJXRaMRqtZKcnMywYcO0jiNciEx5C2EHX331Fbfeeis5OTmEhIRoHccmiouL6dChAxEREfz5558uu3vd3ZnNZtauXVsxlb137158fX0ZOHBgxb2yGzVqpHVM4aQURaF+/fqMGzeO6dOnax1HuBAZoRTCDkwmE6GhoW5TJgH8/PyYN28eV199NZ9++injxo3TOpL4R3Z2NkuXLiUhIYHffvuNnJwcwsPDiY2NZcaMGQwePJjAwECtYwoXoNPpMBqNch6luGxSKIWwA5PJ5DbT3f81ePBgbr31Vp544gni4+OpVauW1pE81t69eyumstesWYPFYqFz58488sgjxMXF0bVrV5nKFlckJiaGRx99lIKCAnkjIi6ZTHkLYQeDBw+mZs2abnl2Y1ZWFq1bt2bUqFG89957WsfxGGVlZaxZs6aiRO7fvx8/Pz8GDRpEfHw8sbGxcvi8sIldu3bRvn17Vq5cyaBBg7SOI1yEvH0Vwg5MJhPNmjXTOoZdhIeH8+qrr/L++++TnJysdRy3dvr0ab744gtuvvlm6tSpw8CBA/nqq6/o378/v/76K6dOnWLRokXcd999UiaFzURFRVG7dm2Z9haXRUYohbAxs9mMn58f77zzDhMmTNA6jl1YLBZ69epFcXExmzdvxtvbW+tIbkFRFPbu3Vtxm8O1a9ditVrp2rVrxYaazp07y1S2sLvrr7+e7Oxs/vzzT62jCBchayiFsLFDhw5hsVjccg1lOYPBwLvvvku3bt2YM2cOTzzxhNaRXFZpaSlJSUkVU9kHDx7E39+fwYMH8+677xIbG0v9+vW1jik8jNFo5KmnnqKkpARfX1+t4wgXIIVSCBtztzMoq9KlSxceeughXnzxRUaNGkXjxo21juQyTp06xZIlS1i0aBFLly4lLy+PBg0aEBcXx5w5cxg4cCD+/v5axxQeLCYmhpKSEjZs2EC/fv20jiNcgEx5C2FjH330EXfddRdFRUX4+flpHceu8vLyaN26NT169OCnn37SOo7TUhSFPXv2VJwNmZycjNVqpVu3bsTFxREfH0+nTp3kbE/hNCwWC7Vq1WLKlCk888wzWscRLkBGKIWwMZPJRP369d2+TAIEBwczZ84cRo0axa+//sq1116rdSSnUVpaSmJiYkWJTE1NJSAggKuvvpoFCxYQGxtLRESE1jGFqJTBYKBv376sXr1aCqW4JFIohbAxdz2Dsio33ngjQ4cO5eGHH2bQoEEefW7diRMnKqayly1bRn5+Pg0bNiQ+Pp74+Hj69+8vU9nCZRiNRqZNm0ZZWZlsvBMXJVsFhbAxdz4yqDI6nY65c+dy/Phxpk2bpnUch1IUhV27dvHaa6/Rp08f6tWrx5133klmZiZTpkxh27ZtZGRkMG/ePIYNGyZlUriUmJgYCgoK2LJli9ZRhAuQEUohbCw1NZXBgwdrHcOhmjdvzjPPPMP//d//cdttt9GuXTutI9lNSUkJq1evrpjKTktLIzAwkGuuuYYPPviA2NhY6tWrp3VMIaqta9euBAQEkJiYSI8ePbSOI5ycbMoRwoaKiooICAhg4cKF3HHHHVrHcaiSkhI6duxInTp1WL16tVudlXj8+HGWLFlCQkICy5cv58yZMzRu3LjibMj+/ft7xJpZ4Xmuvvpq/Pz8SEhI0DqKcHIyQimEDaWlpQHuf2RQZXx9fZk/fz4DBw7k448/Zvz48VpHumKKorBz586KsyHXr18PQI8ePXjqqaeIi4ujffv2sitbuD2j0cibb76JxWLBYDBoHUc4MRmhFMKGlixZQmxsLOnp6R57LuPtt9/O4sWL2bt3L2FhYVrHuWTFxcX8+eefFVPZGRkZ1KhRg2uuuYb4+HiGDx9O3bp1tY4phEMlJiYSExPD1q1b6dSpk9ZxhBOTEUohbMhkMuHt7U2DBg20jqKZuXPnMnLkSIKCgrSOclHHjh1j8eLFJCQksGLFCgoKCoiMjOS6664jLi6OmJgYuUuI8Gjdu3fH19eX1atXS6EUFyQjlELY0BNPPMHPP//MgQMHtI6iKavV6pRrKBVFYfv27RVT2Rs2bECn09GrV6+K9ZBt27aVqWwh/iMmJoawsDB++OEHraMIJyYjlELYkKcdGVQVZyqTxcXFrFq1ikWLFrFo0SIyMzMJCgpiyJAhPPjggwwbNow6depoHVMIp2U0Gnn33XdRFEXebIkqSaEUwoZMJhPdunXTOobHO3r0aMVU9sqVKyksLKRp06Zcf/31xMfHYzQa8fHx0TqmEC7BaDTy8ssvs2fPHqKiorSOI5yUFEohbCg1NZVRo0ZpHcPjKIrCtm3bSEhIICEhgU2bNqHX6+nduzcvvPACcXFxtGnTRkZXhLgCvXv3xsvLi9WrV0uhFFWSQimEjWRnZ5Obm+uRRwZdNrMZPv4YkpLA3x+mToXL/PdWVFTE77//XjGVffjwYYKDgxk6dCiPPvooQ4cOdald5kI4q8DAQLp27UpiYiITJkzQOo5wUlIohbARk8kEeOYZlJfk+HEICwO9Hg4cgA8/hMBA2LxZ/f2770KLFqAocJGRxJycHOrXr09RURHNmzfnpptuIj4+nr59+8pUthB2EBMTw2effSbrKEWVnGflvBAuTgplJfLz4X//g4gIuPVWSE9XP9+yJaxcqX5MmwYbNsCaNerXLuHgiZo1azJ//nz27NnD/v37mT17NgMHDpQyKYSdGI1Gjh49ysGDB7WOIpyUFEohbMRkMhEYGCjTrP+VmAgvvAADB8IHH0D5Ye8Ggzo6CWrRDAxUSyWoI5iXYNy4cbRu3VpGS4RwgL59+6LT6UhMTNQ6inBSUiiFsJHyI4Ok4PzH4cNw+jTcdx9ERqpF8r+sVqhVC6KiYPt2yMjQJKYQ4sJCQkLo1KkTq1ev1jqKcFJSKIWwEZPJJNPd56pbVy2ROTmVf718envECNixA+bOVX+fng5lZY5IKIS4RDExMTJCKaokhVIIG0lNTZVCea6rrlKnuRMSKl8bWT69feONcMcd8N13EBwM7drJaKUQTsZoNJKWlkaG/N0UlZBCKYQNWK1W0tLSpFCeq3596N1bXUtZVHT+18uXB0REwMyZULMmDBgAf/552ccICSHsq1+/fgAySikqJYVSCBvIysqipKRECuV/KYo62hgaCvv3Q0nJhR//22/q42JjoWtXx2QUQlyysLAw2rZtK+soRaWkUAphA3JkUCV0OigogK1b1ensgIDKH2e1qr/WqgXe3pCb67CIQojLYzQaZYRSVEoKpRA24GmFMj09nb/++guLxVL1g7ZuhZAQWLsWUlLgq6/Uz5cXyHLl6yh79VJL5bZt6mjmJR4fJIRwnJiYGPbt28fRo0e1jiKcjHzHFsIGTCYTYWFh1KhRQ+sodmGxWFi3bh3PPPMMHTp0IDIykgceeADDuccA/VdUFPz6K3z/vXqrxUcegSNHKi+KViv4+ED37rBzpzr1DZd0yLkQwnGMRiMASUlJGicRzkYKpRA2UH4GpTvJz8/nhx9+4M477yQiIoLevXuzYMECOnXqxLfffssff/xx4Sfw9YW4OLjhBvVuOCUl6m7vypQfEdSvH5hMkJmp/l7O9BTCqURERNCyZUtZRynOI/fyFsIG3OXIoLS0NBISEkhISODPP/+krKyMtm3bctdddxEXF0fPnj0vPCpZldat1aOAFi+Gu+7694Dz8sLo66uORu7dq45mtm5tuxclhLApWUcpKiOFUggbMJlM9OrVS+sYl81isbB+/fqKErl79268vb3p378/b775JrGxsbYZea1XD4xG+Owz2LdPnQ7/r7171a+XlMDzz0PDhtW/phDCLmJiYvjwww85deoUtWvX1jqOcBJSKIWoprKyMg4dOuQyI5R5eXksW7aMRYsWsWTJEk6ePElYWBixsbH83//9H1dffTXBwcG2vWhAgHoc0Lx58NBD6giklxdMnQoNGqjnUP70E7Rtq27kEUI4rf+uoxwxYoS2YYTTkEIpRDVlZGRgtVqdulCmpqaSkJDAokWLWL16NWVlZbRv35577rmH+Ph4unfvfmVT2Zdj8GB44QX44gv1OKHbboOgIPVrwcHqAehCCKfXpEkTmjRpQmJiohRKUUEKpRDV5IxHBpXvyi4vkSkpKfj4+DBgwABmz55NbGwskZGRjgukKOp6yalT4emnHXddIYRdGI1G2ZgjziKFUohqMplM6HQ6GjdurGmO3Nxcli1bRkJCAkuWLOH06dPUrVuX2NhYXn75ZQYPHkxQ+Yigo5VvvrmSUVCL5d8/l5cHx49Derp6a0Y321kvhKuIiYnhiy++IDc3lxBZpiKQQilEtZlMJho1aoSPj4/Dr33gwIGKUcjExETMZjMdOnRgwoQJxMfH061bN/SuekD4b7/BsGFqmSwrg1274NNPYeFCyM+HPn3gl1/UWzsKIRzKaDRitVpZu3Ytw4cP1zqOcAJSKIWoJkceGWQ2m0lOTq4okX///Te+vr4MHDiQOXPmEBcXp/lIqU1kZalrLMeOhaFDITUV/vc/OHgQ7rtP3QX+4Ydw//3wzTdapxXC47Ro0YLw8HASExOlUApACqUQ1WYymYg69xgcG8rOzq6Yyv7tt9/Izs6mXr16xMXFMX36dAYNGuQ+d+gpX2sZHq4WyfffVz+KiiAmBubOhQED1MfGxqrHEOXmys5wIRxMp9MRExMj6yhFBSmUQlSTyWQiNjbWps+5b98+Fi1aREJCAklJSVgsFjp16sRDDz1EfHw8Xbt2dd2p7AvR6dQ1k9u3w7p1MGYMjBql3poxJkY9AL1cmzbw0kvg7a1dXiE8mNFo5NFHH6WgoIDAwECt4wiNSaEUohrOnDnDiRMnqj3lXVZWxtq1aytK5L59+/Dz82PQoEHMnTuXuLg4GnrCYd/Hj8OqVTBrlrrxZsAA9bihyuh0apk0m9VRSjlgWQiHiomJwWw2s27dOgZX9fdUeAwplEJUQ1paGnBlRwZlZ2fz22+/sWjRIn777TdycnKIiIggLi6OmTNnMmjQIM94118+zX3wIMyYAZ98oh58vnixOu19MceOqSOZCxZAq1b2zyuEAKBNmzbUrl2bxMREKZRCCqUQ1XE5Z1AqisK+ffsqbnO4du1aLBYLXbp04dFHHyUuLo4uXbq451T2heh06i7uyZPh11/Vw8/vuUddR3kxVivUrKnesvGpp+DHH+0eVwih0uv1cl9vUUEKpRDVYDKZ8PX1JSIiotKvl5WVsWbNmooSeeDAAfz8/Bg8eDDz5s0jNjaWBg0aODi1E0pNheXL4ZFH4MknL74uUlHUMmkwQGCguut7/351d/ilFFEhhE0YjUaefPJJiouL8fPz0zqO0JAUSiGqITU1lcjIyLNGFU+dOlUxlb106VJyc3OpX78+cXFxzJ49m4EDBxIQEKBhaifUqpW6yebwYbVMlk+DV8ZqBb1eLZNHjsDzz8MPP8CUKVImhXCwmJgYSkpK2LhxI/369dM6jtCQFEohqsFkMtG0aVP27NlTMQqZnJyM1WolOjqaSZMmERcXR+fOndFVVZCE6pVXYOZMOHkSwsLO/3p5ySwv7++9p665LC2Fhx6CO+90bF4hBB06dCAkJITVq1dLofRwOkVRFK1DCOFqSktLSUpKYvTo0ZSVlZGXl4e/vz9XX3018fHxxMbGVjkNLqqgKGAyVX47xf/efhHUXeAvvwzduqmHn48cCf7+jssqhKgQFxdHaWkpy5cv1zqK0JCMUApxiU6ePMlvv/1GQkICy5YtIy8vD51OR48ePXjuuecYMGAA/lJqrpxO92+ZLCtTp75LStSzJw0G9XDzhARo1AjWr4fGjeGdd+Cqq9Q/Uz4VLoRwKKPRyEsvvURZWRneci6sx5LvvkJUQVEUdu/ezfTp0+nbty/16tXj9ttvJz09ncmTJ7Nq1SoUReHxxx9n+PDhUiZtJS8P5sxRz5YsP8j822+hRw/1VotGIyxbBuPGqWXSbFZHN6VMCqEJo9FIQUEBW7Zs0TqK0JB8BxbiP0pLS1mxYgWPPPIIzZs3p127dkybNo06derw/vvvc/ToUdavX8+zzz5bcbtDR93H22P4+cGXX8Lw4bB0KVx/vVokg4PVDTi//gp9+8Jrr6mP9/KqegOPEMLuunbtSkBAgNyG0cPJlLfweCdOnGDJkiUkJCSwfPly8vPzadSoEfHx8cTHx9O/f/9Kj8MoP4OyWWVr/sSV8/GBb75RS+Pw4RAZCXfdBbfdBh07qo9p3x5Wr/53SlwIoRlvb2969+5NYmIiU6ZM0TqO0IgUSuFxFEVh165dFbc5/OuvvwDo0aMHU6dOJT4+nvbt2190V7bJZCIkJITQ0FBHxPYsLVvCn3+qO769vNTp7vIpbatVPXdyzBhNIwoh/hUTE8Mbb7yBxWLB8N8NdMJjSKEUHqGkpIQ///yThIQEFi1aRHp6OjVq1OCaa67hww8/ZPjw4dSrV++ynjM1NVWmu+2pTZuzf1++6UbWSgrhdIxGI8899xw7duygc+fOWscRGpBCKdzWsWPHzprKLigooEmTJhVT2TExMfhWY7q0/AxK4SBSJIVwWt27d8fX15fExEQplB5KCqVwG4qisGPHjoqp7A0bNgDQq1cvnnnmGeLj42nbtq3NDhg3mUxce+21NnkuIYRwZX5+fvTo0YPVq1fz6KOPah1HaEAKpXBpxcXF/PHHHyxatIhFixaRkZFBUFAQQ4YMYcKECQwfPpw6derY/LoWi4X09HQZoRRCiH/ExMQwb948FEWRO4N5ICmUwuVkZWWxePFiFi1axIoVKygoKKBp06aMGDGCuLg4YmJi8PHxsWuGI0eOUFZWJoXSicgPMSG0ZTQamTZtGikpKbRt21brOMLBpFAKp6coCtu3b6+4V/bGjRvR6/X06tWL5557jvj4eNq0aePQMiFHBjmP8iJ56tQpwiq7B7gQwiF69eqFl5cXiYmJUig9kKxyF06pqKiIJUuWMGHCBBo3bkznzp154403iIyM5NNPP+XYsWOsWbOGqVOnEhUV5fCRqfJCGRkZ6dDrivPpdDrmz59P+/btycnJ0TqOEB4rMDCQ6OhoOeDcQ0mhFE7j6NGjvP/++1x33XWEhYURGxvLihUruPHGG1m5ciUnTpzg22+/ZezYsZqPRKWmphIeHi63W3QS1157LWfOnOGZZ57ROooQHs1oNJKYmIiiKFpHEQ6mU+S/utCIoihs3bq1Yip78+bN6PV6+vTpQ3x8PHFxcbRu3dop18XdfvvtHDhwgOTkZK2jiH/MmTOHxx57jL/++ovu3btrHUcIj7RkyRJiY2PZt28fLVu21DqOcCAplMKhCgsLWbVqVcUB40eOHCEkJIShQ4cSHx/PsGHDqFWrltYxL6pfv340btyYL774Quso4h9ms7miSG7YsAEvL1kiLoSj5ebmUqtWLd577z3uuusureMIB/LoKW9FUTBbFYotVgrLrBRbrJitigzV29jhw4d57733iI+PJywsjPj4eFatWsXo0aNZtWoVJ06c4Ouvv2bMmDEuUSZBDjV3Rl5eXrz77rts27aNefPmaR1HCI8UEhJCp06dSExM1DqKcDCPeQuvKAqnSyxkFZrJKjRztNBMVpEZs/X8x3rpIdzfi4gAL8L/+ajla3DKqVdnZLVa2bJlS8Uo5JYtWzAYDPTt25dp06YRFxdHq1attI55xUpKSjhy5IgUSifUvXt37r//fp599llGjhxJgwYNtI4khMcxGo389NNPWscQDub2U97ZJRa2nixm+6liSizqS9UDlfTI8/z3cb4GHR1r+9E5zI9QX7nx/bkKCwtZuXIlCQkJLF68mKNHj1KzZk2GDRtGfHw8Q4YMcZnRx4vZt28frVq14vfff2fgwIFaxxHnyMnJoVWrVsTExPDtt99qHUcIj/Pzzz9z/fXXk5aWRpMmTbSOIxzELUcorYrCwbxSNp8oJi2/DB3w39Z8KWXy3MeVWBQ2Hi9iw/EiIoO86VrHj+bBPug9eNQyMzOTxYsXk5CQwKpVqyguLqZVq1bceuutxMfH07t3b7y9vbWOaXNyBqVzq1mzJrNnz2bMmDEsXbqUoUOHah1JCI/St29fABITExk7dqzGaYSjuN0IZeaZMhan55NTaj2vSNpK+fPW9NET2ySIRjXcrzRVxmq1smnTpop7ZW/btg2DwYDRaCQuLo74+HiP2NU3f/58Hn74YYqLi2Xjh5NSFIWrr74ak8nErl275HgnIRysffv29OzZk/fff1/rKMJB3OanYZlVYfWRAjadKKZ8zNBeTbn8eXNLrXyxP5foOn7E1A/EW+9+o5UFBQWsWLGiYir72LFjhIaGMnz4cJ588kmGDBlCzZo1tY7pUCaTicaNG0uZdGI6nY7//e9/dOjQgVdffZVp06ZpHUkIj2I0GlmxYoXWMYQDucUIZeaZMhal55NXarVbibwQHRDsoyfOTUYrMzIyKkYh//jjD0pKSmjdujXx8fHEx8dX3F7LU910002cPn2a33//Xeso4iJeeOEFXnvtNXbs2EHr1q21jiOEx/j2228ZPXo0R44cISIiQus4wgFcvlBuOlHEykMFdpvevlTl1x/cMJDoOq41vWa1WtmwYUNFidyxYwdeXl7ExMQQFxdHXFwcLVq00Dqm04iOjqZTp0588MEHWkcRF1FcXEy7du1o1KgRq1atkpMahHCQo0ePUr9+fb7++mtGjx6tdRzhAC57DqWiKKzNKmTloQL191rn+efXlYcKWJtV6PRnWZ45c4Yff/yR8ePHExERQa9evXj33Xfp2LEj33zzDSdPnmTlypVMnDhRyuQ55AxK1+Hn58e8efP4888/5RB6IRwoIiKCli1bynmUHsRl5y2TjxWRdLRQ6xiVKs/VJzxA4yRnS09Przgb8o8//qC0tJSoqCjuvPNO4uPj6dmzJwaDHIl0IXl5eZw+fVoKpQu55pprGD16NJMmTSI2NpbQ0FCtIwnhEWJiYli9erXWMYSDuOQI5abjzlsmyyUdLWTTiSJNM1gsFtatW8fTTz9N+/btiYyMZNKkSVgsFmbOnMnBgwfZvXs306dPp0+fPlImL4EcGeSaZs2aRUlJCU899ZTWUYTwGEajkd27d3Py5EmtowgHcLlCmXmmjJWHC7SOcUlWHiog80yZQ6+Zl5fHDz/8wB133EFERAS9e/fm/fffp2vXrnz33XecPHmSFStW8Mgjj0gpugKpqakAMkLpYurXr88rr7zCggULWLdundZxhPAIMTExAKxZs0bjJMIRXGpTTplV4YM92Zrt5r5c5bu/724TatcjhUwmU8VU9p9//klZWRnt2rUjPj6euLg4evToIaOPNjJr1iyee+45zpw5Ixs8XIzFYqFHjx6YzWY2bdrk0ScVCOEokZGRXH/99cyePVvrKMLOXOo76uojBS5TJkHdqJNXaiXxSAGDGtaw2fNaLBb++usvEhISSEhIICUlBR8fH/r378+sWbOIi4sjMjLSZtcT/zKZTERGRkqZdEEGg4F3332X7t2788477/DYY49pHUkItxcTEyMbczyEy4xQZp4p44v9uVrHuGJjWoZU64zK3Nxcli9fTkJCAkuWLOHUqVPUqVOH2NhY4uPjufrqqwkKCrJhYlGZuLg4ABYtWqRxEnGlHn74YRYuXMiePXto1KiR1nGEcGsffvgh9957L6dPnyYkJETrOMKOXGINpVVRWJyej6uOCemAxen5WC+zux88eJA5c+YwePBgwsLCGDVqFNu3b+f+++9n3bp1ZGVlsXDhQm644QYpkw4iRwa5vpdffpmgoCAmTpyodRQh3F5MTAxWq5W1a9dqHUXYmUtMeR/MKyWn1Kp1jCumADmlVlLzymgR4lPl48xmM+vWratYD7lnzx58fHwYOHAgc+bMITY2liZNmjguuDiLoiikpaVJoXRxISEhvPXWW9x8880sXryY2NhYrSMJ4baaN29OREQEq1evZvjw4VrHEXbkElPeXx/IJT2/zGXWTlZGB0QGeTO6xdlD/rm5uSxdupSEhAR+++03Tp8+Tb169SqmsgcPHkyNGrZbfymu3LFjxwgPD+enn35ixIgRWscR1aAoCkOHDmXfvn3s3r2bgADnOjNWCHdyyy23kJaWJicsuDmnH6HMLrGQlu/Yo3fsQQFM+WVkl1g4mZFacZvDpKQkzGYznTp14oEHHiA+Pp7o6Gj0epdYjeBR5Mgg96HT6fjf//5Hu3btePnll3n11Ve1jiSE2zIajXz//fcUFBQQGBiodRxhJ05fKLeeLLbLfbqTPp9P+tb1ZB1I4Uz2ScwlJQTVrkvT6N7EjHuYes1b2/iKoFitPDRjLl++MBFfX18GDRrEO++8Q2xsrGwOcAHlh5pLoXQPLVq04Omnn2batGmMGTOGtm3bah1JCLdkNBorlnQNHjxY6zjCTpx6yltRFN7aeZoSi+0jThvYitKiQsJbRhFSJwKAY6l/czL9IAZvH8bO+oRWfWz/P76lpIh2x7cyeNAgeafmYl555RVmzZrFqVOntI4ibKSkpIQOHTpQr149Vq9eLcdBCWEHiqJQt25d7r//fqZNm6Z1HGEnTj1CebrEYpcyCTB21qc0aNMRb1+/sz7/13cL+eW1Kfz40mNMXbINvY0PBDf4+tPvmlgC/eSgcVcjO7zdj6+vL/PmzWPw4MF88skn3HHHHVpHEsLt6HQ6+vXrJ+dRujmnXqiXVWi223NHdupxXpkE6HnTndRu1JS8E1mcSD9gl2vb83UJ+5FC6Z4GDRrErbfeyuTJk2X0WQg7iYmJYf369RQXF2sdRdiJ0xdKLQKWj0oavK78IPIqn1sHWUVSKF2RFEr39eabb1JWVsaTTz6pdRQh3JLRaKSkpIQNGzZoHUXYiVMXyqOFZhx9+uSWRd9wIu0AYU2aU6thpM2f36rAkQLX37XuacxmMxkZGTRr1kzrKMIOwsPDee211/jggw/kAGYh7KBDhw6EhITItLcbc9pNOYqi8OaOU5jt3CgTP5nLsdS/KS0q5IRpP8cO/k1wnXBun/05DaI62uWaXnp4vENt2QDgQsoPNF+6dClDhgzROo6wA4vFQu/evSksLGTLli14e9t+hkIITxYXF0dJSQkrVqzQOoqwA6cdobQo2L1MAuxb9wdbEr5h18oEjh38m5DwBox+9V27lUlQX5fVKWu8qIqcQen+DAYD7777LikpKcyZM0frOEK4nZiYGJKTkykrk1k6d+S0hdLsoIHTu9/9gde2nOD51Qe494NfqdOkBe/fM4I/Pphl1+s66vUJ2zCZTOh0Orn1pZvr3LkzDz/8MC+88AIZGRlaxxHCrRiNRgoLC9m8ebPWUYQdOG2htDp48aR/UAhNu/Tijre/okGbjqyYP53M3Vvtdj2L696a3COZTCbq16+Pr6+v1lGEnb300kvUrFmTRx55ROsoQriVLl26EBgYKOso3ZTTFkqt7jxo8PamwzUjUBSFvxOX2e86TvtvXlRGdnh7juDgYObMmcMvv/zCL7/8onUcIdyGt7c3vXv3ZvXq1VpHEXbgtLXGS8MNKwE1awFQkG2/M+m0fH3i8kmh9CwjR45k2LBhPPLIIxQUFGgdRwi3YTQaWbNmDRaLResowsactlAadOpuaC2YtiQD2OXYIFBfl176pEuRQulZdDodc+fO5fjx47z00ktaxxHCbcTExJCXl8eOHTu0jiJszGkLpU6nI9zfPneGNG1Zx45lP2Exn33AuKWsjOSv32fr4u/w9vOnwzUj7HL9cH8vOTLIhRQWFpKVlSVnUHqYZs2a8eyzzzJr1ix27typdRwh3EK3bt3w9fWVaW835NT38o4I8OJIge0PNz99KI3vX3yEwJq1qd+mAwE1a1GYfYqsA3vIP3kML18/bnzxbWqGN7DxlUGnWKnjY/OnFXaUlpYGyJFBnuiJJ57g888/Z8KECSQmJqLXanG3EG7Cz8+Pnj17kpiYyMSJE7WOI2zIqb87hgd42eVOOU279qb/+ImERTYna38Ku1b8Svr2DQSEhNLr5rt59JvVdhudVHR6nnrwHnr06MHkyZNJSEjg9OnTdrmWsA2TyQRIofREvr6+zJ8/n7Vr17Jw4UKt4wjhFoxGI4mJiVgdfZyLsCunvVMOwKliM+/vydE6hs0FbV/K2uVLSExM5PDhwwC0b98eo9GI0WikX79+REREaJxSlJs7dy6TJk2iqKgIwz/3eReeZdy4cSxatIi9e/cSFhamdRwhXNrvv//O4MGD2bVrF23bttU6jrARpx6hrOVrwNfgXmsNfQ06Hhg3hi+//JLMzExSU1P5+OOP6d69O8uXL2f06NHUr1+fli1bctddd/Hxxx+TmpqKE/d+t2cymWjSpImUSQ82c+ZMFEVhypQpWkcRwuX17NkTLy8vWUfpZpx6hBJg1eECNh4vwqlDXiId0L2uPwMaBFb5mKNHj7JmzRoSExNJTExk586dKIpCgwYN6NevX8UoZps2bWQ9l4PccMMNnDlzhuXLl2sdRWjovffe47777mP16tUYjUat4wjh0nr37k3jxo35+uuvtY4ibMTpC2V2iYUFKdlax7CZ+6JCCfW99JGu7Oxs1q5dW1EwN2/ejNlsplatWmcVzE6dOuHl5dR7rFxW586d6d69OwsWLNA6itCQ1WqlT58+5OXlsXXrVnx8ZHedEFfqySef5JNPPuHIkSNy6ombcPohrlBfA5FB3rj6/246oGmQ92WVSYDQ0FDi4uJ4/fXX+euvv8jJyWHlypU89NBD5OXl8cwzz9CtWzdCQ0MZMmQIr7zyComJiRQXF9vnhXgYRVFITU2VI4MEer2ed999l7179zJr1iyt4wjh0mJiYsjKyuLAgQNaRxE24vQjlAD7c0v4ITVf6xjVdmOzYFqE2HZUo7S0lE2bNpGUlERiYiJr1qwhLy8PHx8fevToUTGK2bt3b4KCgmx6bU9w+vRpateuzTfffMOoUaO0jiOcwOOPP878+fNJSUkhMjJS6zhCuKTc3Fxq1arFggULuPvuu7WOI2zAJQqlVVF4LyWb3FKrS66l1AEhPnrujQpFb+ehfYvFwo4dOyoKZmJiIidOnECv19O5c+eKKfK+ffvKbtVLsHnzZqKjo9mwYQPdunXTOo5wAvn5+URFRdGxY0cSEhJkuk6IK9S1a1fatm3Lp59+qnUUYQMuUSgBMs+U8cX+XK1jXLHbWobQsIa3w6+rKAr79u2rKJeJiYlkZGQAEBUVddZRRQ0bNnR4Pmf3/fffc9NNN3HixAkp4KLCTz/9xA033MCPP/7I9ddfr3UcIVzSpEmT+PHHHytuHiFcm8sUSoCVh86w+USxS41S6oDoOn4MalhD6ygV0tPTK0Ywk5KS+PvvvwH14O7ycmk0GmnRooXHj77MnDmTl156iby8PI//dyH+pSgK1157Ldu2bWPPnj3UqOE8f7+FcBU///wz119/PWlpaTRp0kTrOKKaXKpQllkVPtiTTZ6LTH3rgGAfPXe3CcVb77xl5Pjx4yQlJVWUzG3btqEoCuHh4WcVzHbt2nncUUVz5sxh6dKl/Pbbb1pHEU4mLS2NqKgoHnjgAd544w2t4wjhck6dOkVYWBiffvopY8eO1TqOqCaXKpTgelPfY1qG0EiDqe7qyM3NJTk5uWKKfOPGjZSVlVGzZk369u1bMU3epUsXvL1d67UJYUvTp0/n2WefZfPmzXTs2FHrOEK4nPbt29OjRw8++OADraOIanK5Qgmw6UQRKw8VaB3jogY3DCS6jr/WMaqtsLCQDRs2VBTMdevWUVhYSEBAAL169aoYxezRowcBAQFaxxXCYUpLS+ncuTPBwcGsXbvW40bwhaiuhx56iOXLl7Nv3z6to4hqcslCCbA2q5Cko4Vax6hSv4gA+oS7Z7kqKytjy5YtFWswk5KSyMnJwdvbm27dulVMkffp04eQkBCt4wphV4mJicTExLBgwQLuvfdereMI4VK+/fZbRo8ezZEjR4iIiNA6jqgGly2UiqKQfKzIKUulMSKAXvX8PWYTh9VqZdeuXWcdVZSVlYVOp6Njx45n7SSvW7eu1nGFsLnx48fz008/sXfvXvl/XIjLkJWVRUREBF9//TWjR4/WOo6oBpctlOXKp791oOlGnfLru8s0d3UoisKBAwfOKpgmkwmAVq1anVUwZWefcAcnT56kVatWxMXF8cknn2gdRwiX0qpVKwYNGsS8efO0jiKqweULJagbdRal52u2+7t8N3dckyCX24DjKIcOHTrrqKLdu3cD0Lhx47MKZqtWrTxmZFe4lw8//JC7776bVatWMWDAAK3jCOEy7r33XpKTk9m1a5fWUUQ1uEWhBPVIodVHCth0othho5Xl1+lWxw9j/UCnPhrI2Zw8eZI1a9ZUlMwtW7ZgtVqpU6fOWUcVdejQAYPh8u5/LoQWrFYrRqORkydPsn37dnx9fbWOJIRL+Pzzzxk7dqzcQMLFuU2hLJd5pozF6fnklFrtViytFgt6g4GaPnpiZVTSJvLz80lOTq4omOvXr6e0tJTg4GD69OlTMYoZHR2Nj49t74cuhK3s3LmTLl268OKLL/LMM89oHUcIl5CRkUGTJk3kzlMuzu0KJaj3/k7NK2PziSJM+WU2K5Y6/rmV4bo/uKlHW4Z0jbL7vbk9VXFxMRs3bqxYg5mcnMyZM2fw8/OjZ8+eFQWzZ8+eBAYGah1XiApTp07l7bffZvfu3TRr1kzrOEK4hKZNm3LHHXfwwgsvaB1FXCG3LJT/lV1iYdvJYradKqbEor5UvQ6sl/Cq//s4X4OOTrX9aB/qTc8OUXTu3JnvvvvOjsnFf5nNZrZt23bWUUWnTp3Cy8uLrl27VkyT9+3bl9DQUK3jCg9WUFBAVFQUUVFRLFmyRNYEC3EJTp8+Ta1atbSOIarB7QtlOUVRyC6xklVoJqvIzJGCMrKKzJit5z/WSw/h/l7UD/Qm3N+L8AAvQn31FT8YFixYwIQJE9i7dy8tW7Z08CsRoK5X27NnT0XBTExM5PDhw+h0Otq3b1+xBrNfv35ytplwuF9//ZXrrruO7777jhtvvFHrOEIIYXceUygroygKVgXMioLFCgY9eOl06HVccFShuLiYyMhIrrvuOhYsWODAxKIqiqJgMpnOOqrowIEDALRo0YK1a9fK+YDCoUaMGMHGjRvZs2cPwcHBWscRQgi78uhCWR2vvfYaL774ImlpaTIC5qSOHj1aMT0+a9asi9933PrPcLVOB4oCej3s3QuNG4P/ZZwtqijqc6SlwYoV6vMOGABXXXXFr0W4noyMDNq0acM999zDW2+9pXUcIYSwKymUVygnJ4fGjRvzwAMPMH36dK3jCHsJDIQlS6Bfv38/p9OpHxdy5gzcdBMkJ6tltHlzePtt6NoVSkvhyBGIjLRrdKG9mTNn8uSTT7Jp0yY6d+6sdRwhhLAbvdYBXFXNmjW57777mD9/Prm5uVrHEdVRWgq//QY//wyJibBnD2RmwhdfQFERnDypjlaWf1zKJovVq2HZMnjkEdiwAX75Bdq3V792+jTccgv4+sJjj0F2tl1fntDOxIkTiYqK4r777sNisWgdRwgh7EZGKKvhyJEjNG3alGnTpjFlyhSt44grkZEBDz+slj9/fwgJgeBgKCyEU6fU6e4TJ2DSJGjRAsLD//3w86v8OUtL4bXX4M03Ydcu9Tn+S1EgPR0+/RRefx2++QZiY+3/WoUm1q5dS9++fZk3bx4TJkzQOo4QrqN8+ZDVqr6Zr+xrwmlIoaymu+++m8WLF2MymfCrqmAI5/XFFzBlCjz4oDot/fff6trHggIYMQK6d4fbb1c/7+urls5atWD4cHj8cbWAnuvUKRg3Do4dgz/+gBo1Kr92Who0awavvAJPPWXHFym0ds899/Ddd9/x999/Ex4ernUcIZzfxo0wejSkpp79+ZIS9XuxcDpeWgdwdZMnT+ajjz7is88+45577tE6jrhca9ZA06bw5JPnvwMu9+OP6jT4gQNgMqlT2C+/rL5rfvnl8x+/ZYv6mNtvr7xMWixgMMCOHWohrVdP/by843Zb06dP5+eff+aJJ57g888/1zqOEK4hPR3mzoWYGCguhn37YPt2OHRIXUJ0xx0gm2KdhoxQ2sANN9zArl272LNnj9x32tVs2qQWw+7d1V+t1n833VRVMAEGDlRHFz/44OzPv/ACJCSo3/Q2b4ZOnc7/s+WF8rHH1Md+8gn06VP5tI5wGx9//DF33nknK1euZNCgQVrHEcI5nPtGOj9fneV56SV1XXtwsLru3NtbXW9ep476z2fOqEuRnn9es+jibFIobWD9+vX07NmT77//npEjR2odR1yuSxkZtFrVx5nN6nTL8OHQoQP8d4d/cTG0bAkBAeqoZtu2F77egAHq77/7DsLCbPNahNNSFIX+/ftz9OhRduzYIUtkhPivrVvVN+GbN8Nff6lvyp95Bvr3V5cmtW4N0dHq99WSEpg1S/3eefiw1snFP6RQ2siAAQMoKChg/fr1cqs1T7B3rzpdfe56uK+/hrFj4cUX1W+GVZXV4mIYPFh9552S4pDIQnspKSl07NiR5557judlZEUIdRPjrFkwcybk5qrfU6OjYeRIuPVWdTanMqmp8Oyz8OGHl3dOsLAbmV+zkalTp7Jx40b++OMPraOIS2WxwHPPqbu4L1erVueXSVBHLWvXVqdtQC2UlfHzUzf1FBWpa4Ty8i4/g3A5UVFRTJ48mVdffbXiTk5CeLT8fJgxA7p1g99/V0cqf/hBfWNeXibLbzrxX02awPz5UiadiIxQ2oiiKHTu3Jl69eqxbNkyreOIS7Fvn/pNbOfO84/2uRBFUYtgQMD5X1u+HOLj1bWVY8ee//XydZJ//aWW2R071EJ7223w0UfgVfk+OUVRZOTbTRQWFtK2bVuuuuoqli5dKv9dhWcrLVVL4fffw/XXa51GVIOMUNqITqdj6tSpLF++nK1bt2odR1yKU6fUX3/+WV2TYzaf/5jK3m9lZqoHlhcUnP15qxV271b/TMeOlV+zvDy8+aZaJt95R92xOGtWlWXSYrGwfPlyunTpwsSJE/nxxx85fvz4pb1G4XQCAgKYO3cuy5cv59tvv9U6jhDaMhhg6FB1xgiqntWpTF4erF9vn1zisskIpQ2ZzWauuuoqevTowVdffaV1HHExBw+qI4Pr16sjlfXrq2dRdu8Ow4ZBUJD6uKIitQj6+qq/vvEGzJmjruH57/3BS0rU8yz/+EPdPV6zpvr5c0egCgvVKfPBg2HhwovGVBSFzZs3884775CUlITJZAKgdevWGI1G+vXrh9FopPHljLIKzY0cOZLk5GT+/vtvQio7z1QIT7Frl7omvVGjS3u8oqhv4FNS1PWWJ06ou8GFpqRQ2tj//vc/HnnkEfbv30+zZs20jiMu5uBBdSPNli3qbsGsLPXuOY88ou4qfPVV9R10/frqMUE6HSxaBNdeCx9/fP7z3XILrFun3sLx3IJXVqYW0KQkuO46dcr7sccu+7igzMxMkpKSSEpKIjExkZR/NvU0adKkolwajUauuuoqmU51YocOHaJNmzbccccdvPPOO1rHEcK5lZfI/27S2bFDPS3jxx/VsyqFpqRQ2lhhYSFNmjThpptuYt68eVrHEZfCbFYXhhcWqlMo332nLvYuKoIuXdQimZWlFs7cXHUU88UX1bMjzzV/vrrZpkYN+L//Uw/ePXfR+LPPqmV04UK4+upqnz958uRJ1qxZQ2JiIomJiWzduhWr1UrdunXPKpjt27eXc1KdzOzZs3niiSdYv3490dHRWscRQjunTqmjjN7e/x7TVtV5wHv3qrNAc+aoh59/9ZV6NrDQlBRKO5g2bRqvvvoq6enp1K1bV+s44kpERqqjkM8/r5bDM2fUwmkwQIMGF/6zmZnquWm+vuoUuI+PugFo3jx1Z/isWdC1q3rcRf36Nr9DTl5eHuvWrasomBs2bKC0tJTg4GD69u1bMU0eHR2Nj4+Pza4rLp/ZbCY6OhovLy/Wr18vhV94JosFHnpIXXo0fvz5X7da1SVFL7+szhCVnz0ZEQETJsADDzg2r6iUFEo7OH36NI0bN2bixIm8XNmt+YTzKi93110H992nHmBe1WMux8GD8PDD6q8BAerdcTp0sE3miyguLmbDhg0kJiaSlJTE2rVrKSgowN/fn549e1aMYvbs2ZPAwECHZBL/+uuvv+jduzdvv/02Dz30kNZxhNDGXXep9+9evlxdE/nHH7B2LRw/rs4GNW8Od9+trj0vP+C8Y0f1ZhLyxtgpSKG0k0mTJrFw4UIyMjIIKt/cIYQTMJvNbN26tWINZlJSEqdPn8bLy4uuXbtWTJH36dOH0NBQreN6hPvvv58vv/ySv//+m/r161/Wn1UUBYsCZkWpWD3hpdNh0CFraIXr2L4dOndWp7x9fCAwUJ3RCQxUN+34+MCYMepSItnE5pSkUNpJZmYmzZo1Y/r06Tz++ONaxxGiSlarlZSUlIqCmZiYyJEjR9DpdLRv376iYPbr14/wyg5zF9WWnZ1Nq1atGDhwIF9//XWVj1MUhdMlFrIKzWQVmjlaaCaryIy5knOfvfQQ7u9FRIAX4f981PI1SMkUzmv1akhOVpcWtWypnobRurW6abJnT3j7bXV622JRZ4nKP4RTkEJpR3fccQcrV64kNTVV1qoJl6EoCiaTqaJcJiUlVdzVpWXLlmcVzMjISCkoNvLZZ59x++23s2zZMq655pqzvpZdYmHryWK2nyqmxKJ+y9YDlfTI8/z3cb4GHR1r+9E5zI9QX1mvKZxI+VKiypYUWSzqyGTbtjB6tDb5xEVJobSjlJQU2rZty0cffcSdd96pdRxxCQYOHMjo0aO57777tI7iVI4cOXLWUUU7d+4EoGHDhmedhdmmTRspmFdIURQGDhxIZmYmO3fuxNfPj4N5pWw+UUxafhk6wBbfrMufJzLIm651/Gge7INe/psJZ2O1nj0CaTZXefMH4RykUNrZddddx759+9i9ezf6ahwNI+zPYrHg7+/P7NmzefDBB7WO49ROnz7NmjVrKgrm5s2bsVgshIWFVewkNxqNdOzYES/5IXDJ/v77bzp06MAzM9+m3uBR5JRabVYkz1X+vDV99MQ2CaJRDe+L/REhhKiSFEo7S05Opk+fPvz8889cd911WscRF5CRkUGTJk1YvHgxwyvb3S2qdObMGf7666+KafL169dTXFxMjRo16NOnT0XB7NatG76+vlrHdVplVoXp36/Eq0XHf0Z67T9yWF4so+v4EVM/EG+9jFYKIS6fFEoH6NevH2azmeTkZJkOdGKrV6+mf//+pKSk0KZNG63juLSSkhI2bdpUsQZzzZo15Ofn4+vrS48ePSqmyXv16iWnIPwj80wZi9LzySu12mVE8mJ0QLCPnjgZrRRCXAEplA6wePFi4uLiWL16NUajUes4ogoff/wxd955J4WFhfife3cbUS0Wi4Xt27eftZP85MmTGAwGunTpUrEGs2/fvtSuXVvruA636UQRKw8V2G16+1KVX39ww0Ci68jfASHEpZNC6QCKotChQwcaN27M4sWLtY4jqvDCCy/w/vvvc+TIEa2juD1FUfj777/PKpiZmZkAtG3b9qyd5A0udmciF6YoCsnHikg6Wqh1lPP0iwigdz1/mVURQlwSKZQOUn4kyPbt2+ngoDukiMszduxYUlNTWbt2rdZRPFJ6evpZRxXt3bsXgGbNmp1VMJs3b+42JWdtVqFTlsly/SIC6BMeoHUMIYQLkELpIGVlZbRo0YJ+/frx+eefax1HVKJv375ERkbKfx8ncezYsbOOKtq+fTuKohAREXHWUUVt27Z1yRMUNh0vYuXhAq1jXJRMfwtnUF5V3OXNpDuSQulAc+bM4fHHH+fAgQNERkZqHUeco0GDBowfP55p06ZpHUVUIicnh7Vr11YUzI0bN2I2mwkNDT3rqKLOnTvj7e3cm0oyz5Txxf5crWNcsjEtQ2SjjtCMoihs3LiRgIAA2rVrp3UcUQUplA5UUFBA48aNGTNmDG+//bbWccR/FBcX4+/vz4cffsj48eO1jiMuQWFhIevXr6+YJl+3bh1FRUUEBgbSq1evioLZvXt3p9pkVWZV+GBPtma7uS9X+e7vu9uEypFCQjMdOnSge/fufPDBB1pHEVWQQulgL774Iq+//joZGRmEhYVpHUf8Y+/evbRu3ZpVq1YxYMAAreOIK1BaWsqWLVsqCuaaNWvIzc3Fx8eHbt26VRTM3r17ExwcrFnOlYfOsPlEsUuUyXI61HMqBzWsoXUU4aEeeughli1bxv79+7WOIqoghdLBTp48SePGjZk8eTL/93//p3Uc8Y+lS5cybNgwTCaTLEdwExaLhV27dlVs8klMTOTYsWPo9Xo6depUsQ6zX79+1KlTxyGZXG2q+1wy9S208t133zFq1CgOHz5M/fr1tY4jKiGFUgOPPPIIX3zxBRkZGQQGBmodRwDz58/nkUceoaioSG4V6KYURWH//v1nHVWUlpYGQJs2bSo2+RiNRho1amTz61sVhfdSssl1kanuc+mAEB8990aFyr2/hcNlZWURERHBV199xc0336x1HFEJ19sa6QYef/xxcnNzZS2IE0lNTaVx48ZSJt2YTqfjqquu4q677uKTTz7BZDKRkZHBF198gdFoJCkpidtuu43GjRsTGRnJ7bffzgcffMC+ffuwxfvug3ml5LhomQT1wPOcUiupeWVaRxEeKDw8nFatWpGYmKh1FFEFGaHUyNixY0lMTOTAgQNOvyPVE9x4443k5OSwcuVKraMIDZ04cYI1a9ZUTJNv3boVq9VK3bp1zzoLs3379hgMhst67q8P5JKeX+ayhRLUUcrIIG9GtwjROorwQPfeey9r165l9+7dWkcRlZBCqZGdO3fSoUMHPv30U8aOHat1HI/XtWtXunTpwvvvv691FOFE8vLySE5Orpgm37BhA6WlpYSEhNC3b9+KafKuXbvi4+NT5fNkl1hYkJLtwOT2dV9UKKG+l1eohaiuL774gttuu43jx487bN2zuHRSKDUUGxtLeno6O3bscMmDmd1JrVq1eOKJJ3j66ae1jiKcWFFRERs2bKgomMnJyRQUFODv70/Pnj0rRjF79uxJQMC/d5hZdbiAjceL7DI6WVZSzJ8fzWH7sh/JzTqMf3BNruo9kKsnPElIPdtvXtAB3ev6M6CBrP8WjpWZmUnjxo354YcfuOGGG7SOI84hhVJDSUlJGI1GEhISiIuL0zqOx8rNzaVmzZp8+eWX3HLLLVrHES6krKyMbdu2nXXLyOzsbLy8vIiOjlanyI1G9jXsQanVDtcvKebD+28gfftGgsLqEdm5J9lHMzm0awuBoWFM+HgJtRs1tfl1fQ06JravJXctEQ7XrFkzrr32Wt566y2to4hzSKHUkKIo9OnTB4PBQFJSktZxPNa2bdvo3Lkz69ato2fPnlrHES7MarWSkpJSUTATExMp8wng8Z/+ssv1Vsyfzqr336Rxh26Mn/ctvgHqOZFJn89nyaznadqlF/d+8Ktdrn1vm1Bq+cm0t3CsO+64gx07drBlyxato4hzyDyrhnQ6HVOnTmXNmjWsXbtW6zgey2QyAeo7XyGqQ6/X065dOx544AG+/vprDh8+zLdLV9nlWpayMpK/Vk+KuO7J6RVlEqDfbRMIb9kW05Z1HE7ZbpfrZxWa7fK8QlxITEwM27ZtIycnR+so4hxSKDUWHx9PmzZtmDFjhtZRPJbJZCIgIEAWeQub0+l0WGvUsss32rRt6ynOz6VWw0jqt+5w3tfbDY4HYE/iMptfW6+DrCIplMLxjEYjiqLIIIwTkkKpMb1ez+TJk0lISJCjEDSSmppK06ZNZT2YsIujhWbssHySo/t2AdCgzfllEqBB6/bq4/bb/vuKVYEjBXIepXC8Zs2aUb9+fVavXq11FHEOKZROYMyYMTRo0ICZM2dqHcUjmUwmmja1/cYFIRRFsdtIXk7WYQCC61a+k7t8h3fO0UN2uX5WkdkmB74LcTl0Oh0xMTFywLkTkkLpBHx8fJg0aRJffPEFmZmZWsfxOFIohb1YFDDbY3gSKC0sAMDHz7/Sr3v7qccWlRYV2OX6Zqs6UimEoxmNRjZt2sSZM2e0jiL+Qwqlk7jnnnuoUaMGs2fP1jqKR1EUhbS0NCmUwi7Mdh3B++e5q1yqYf+2Z9/XJ0TlYmJisFgsrFu3Tuso4j+kUDqJoKAgHnzwQd577z1Onz6tdRyPcezYMYqKiqRQCruw2ml0EsDnn13dpUWFlX69rLhIfZy//Q4gt9jx9QlRldatW1OnTh1ZR+lkpFA6kUceeQSLxcL//vc/raN4jPIjg6RQCnuw5w2waoY3ACDv+JFKv557TP18zYiGdstgkJ8gQgM6nQ6j0SjrKJ2MfDtwInXr1mX8+PG8/fbbFBZWPuogbEsKpbAnLzueHBBxVTsADu/ZUenXD/+9E4DwFlF2y2DP1yfEhRiNRtavX09RUZHWUcQ/pFA6mccff5zTp0+zcOFCraN4hNTUVGrXrk1wcLDWUYQbMujAy07fZZt06o5fjWBOH0rjyN/nl8pdKxMAaG28xi7X99Kr51EKoYWYmBhKS0vZsGGD1lHEP6RQOplmzZoxatQo3njjDcxmOTjY3mSHt7AnnU5HuL+XXZ7by9uHXqPvAuDXGU+etZs76fP5ZO3fTWSnHjRq29ku1w/395KzW4Vm2rVrR82aNWXa24lIoXRCU6ZMIS0tjW+//VbrKG5PCqWwt4gAL7t9ox1w9yQatetK+vaNvHFdD76cejfzbh/KklnPE1CzFiNffNsu19XroH6gt12eW4hLYTAY6Nevn2zMcSJSKJ1Q586dGTJkCK+//rocHGxnUiiFvYUHeNnlTjkA3r5+3PPeTwy853G8/fxJ+fM3so9m0CV+NA9/uYqwxva5P71VwW4jr0JcKqPRSHJyMqWlpVpHEYB8R3BSU6dOZeDAgSxbtoyhQ4dqHcctmc1mMjMzpVAKuwoPsO+3WW8/f66e8CRXT3jSrtc5l71flxAXYzQaKSoqYvPmzfTq1UvrOB5PRiidVP/+/enWrRszZszQOorbyszMxGKxSKEUdlXL14Cvwb3WGvoadIT6yo8Poa0uXboQGBgo6yidhHxHcFI6nY6pU6fy559/sn79eq3juKXyI4OaNbPPtKAQoP5d7ljbD3eplDqgU20/2ZAjNOfl5UWfPn1kHaWTkELpxEaMGMFVV10lo5R2kpqaik6no3HjxlpHEW6uc5ifA26E6BgK0CnMT+sYQgDq8UFr1qzBYrFoHcXjSaF0YgaDgcmTJ/Pzzz+zd+9ereO4HZPJRIMGDfD19dU6inBzob4GIoO8XX6UUgc0DfIm1NegdRQhAHUdZX5+Ptu2bdM6iseTQunkxo4dS3h4ODNnztQ6ituRHd7CkbrWcf1RSgXoWsdf6xhCVOjWrRt+fn6yjtIJSKF0cr6+vkycOJFPP/2Uw4cPax3HrUihFI7UPNiHmj56lx2l1AE1ffQ0C5bzJ4Xz8PX1pWfPnlIonYAUShdw//33ExAQwFtvvaV1FLcihVI4kl6nI7ZJkMuOUloVK8EZW9HLZhzhZGJiYkhMTMRqtdeJr+JSSKF0AcHBwUyYMIEFCxaQk5OjdRy3UFhYyLFjx6RQCodqVMOb6DquuONb4ci6FYwZPpgbb7yRo0ePah1IiApGo5HTp0+TkpKidRSPJoXSRTz66KOUlpYyf/58raO4hbS0NECODBKOF1M/kGAXmvrWASE+Bt68/xa++eYbkpKSiIqK4sMPP5Q7eQmn0LNnT7y9veX4II1JoXQR4eHhjBs3jjlz5lBUVKR1HJeXmpoKICOUwuG89TriXGjqWwHimgThY9AzatQo9uzZw4gRI7j77rsZNGgQBw4c0Dqi8HABAQF069ZN1lFqTAqlC3niiSc4fvw4n3zyidZRXJ7JZMLHx4f69etrHUV4oEY1vBncMFDrGJdkcMNAGtX4dyNOrVq1WLhwIStWrCAtLY327dszY8YMzGazhimFpzMajaxevVpGzTUkhdKFtGzZkhtvvJE33nhDDnGtJpPJRJMmTdDr5a+A0EZ0HX/6RQRoHeOC+kUEEF3FMUGDBw9m586dPPjggzz99NN0796dLVu2ODihEKqYmBiOHTvG/v37tY7iseSnqYuZOnUqBw8e5IcfftA6ikuTHd7CGfSu57yl0hgRQO96Fz5zMjAwkDfeeIP169djsVjo3r07U6dOpbCw0EEphVD17t0bvV4v6yg1JIXSxXTt2pVBgwYxY8YMGdqvBimUwhnodDr6hAdUTH9rvVGn/PqDGwbSOzzgku/XHR0dzaZNm5g2bRpz5syhQ4cOrFq1yn5BhThHcHAwXbp0kXWUGpJC6YKmTp3Kli1bWLlypdZRXJKiKFIohVOJruPPmJYhmu7+1gHBPnrGtAypcpr7Qry9vXnqqafYsWMHDRo0YNCgQdx1111kZ2fbPqwQlZB1lNqSQumCBg8eTJcuXZgxY4bWUVxSdnY2eXl5cmSQcCqNanhzd5tQutbxAxw3Wll+neg6ftzdJvSsDThX4qqrruKPP/5gwYIFfP/997Rp04bvv/9efsgLu4uJiSEzM5P09HSto3gkKZQuSKfTMXXqVH7//Xc2b96sdRyXYzKZADkySDgfb72OwQ1rMKZlCCE+6rdnexXL8ucN+WdUclDDGnjrbXM1vV7Pvffey549e+jVqxc33XQT119/vdw+VthV37590el0Mu2tESmULmrkyJE0b95cRimvgJxBKZxdoxre3BsVyo3NgokMUkcMbVUsy58nMsibG5sFc29U9Uclq1K/fn1++uknvv/+e9avX09UVBQLFiyQW+QJu6hVqxbt27eXjTkakULpogwGA0888QTbtm0jNzdX6zguxWQyERQURK1atbSOIkSV9DodLUJ8GN0ihPuiQule1x9fg+4/X7/U5/n3n30NOrrX9ee+qFBGtwihRYiPQ+7NPXLkSFJSUrjpppu4//77GTBgAHv37rX7dYXnMRqNMkKpEZ0iC1tclsViQafToSgKBoNB6zguY8KECSQnJ7N9+3atowhxWRRFIbvESlahmawiM0cKysgqMmOuZMDPSw/h/l7UD/Qm3N+L8AAvQn31l7xz215WrVrFvffey6FDh3j++eeZPHky3t72GSEVnuf777/npptu4vDhw3LjCgeTQik8ztChQ/Hz8+Pnn3/WOooQ1aYoClYFzIqCxQoGPXjpdOh1aF4eq1JUVMT//d//8cYbb9C2bVs++OADunXrpnUs4QaOHTtGeHg4X331FTfffLPWcTyKTHkLjyNHBgl3otPpMOh1+Br0BHjr8TXoMeh1TlsmAfz9/Zk+fTobNmzAYDDQs2dPHn/8cQoKCrSOJlxcvXr1aNWqlayj1IAUSuFRrFYraWlpcmSQEE6gS5cubNiwgenTpzNv3jzatWvH8uXLtY4lXFxMTIyso9SAFEpP4+ErHI4ePUppaamMUArhJLy8vJg8eTI7d+6kadOmDBkyhDvuuINTp05pHU24KKPRSEpKCidOnNA6ikeRQulpnHgazBHkyCAhnFOLFi34/fff+fDDD/nll1+Iiorim2++kQPRxWUzGo0AJCUlaZzEs0ihdEdbtsB/b8tYWgr79sE338CTT8IHH0BZmXb5NFR+qHlkZKS2QYQQ59HpdIwfP56UlBSMRiM333wz1157LZmZmVpHEy6kUaNGNG3aVNZROpiX1gGEHSxcCEuWwLffgtkMycnw+eewfbs65R0aCqdOwdSpWid1OJPJRN26dQkMDNQ6ihCiChEREXz33Xf8/PPPPPjgg7Rt25bp06dz//33o9fLOIi4OFlH6XjyN9MdnDsl1KgRpKXBtddCv34weTIEB8P338PmzXDvvfDGG5pE1Zrs8BbCdYwYMYKUlBRuvfVWHnzwQfr168eePXu0jiVcgNFoZPv27eTk5GgdxWNIoXQH5esiT5+GP/6Ajz+GIUNg/nx46CFYtQoWLYIRI6BTJ3jmGXjsMfhn+teTSKEUwrWEhITw7rvvsnr1ak6ePEmnTp146aWXKC0t1TqacGIxMTEoisKaNWu0juIxpFC6svKRyfx8WL5cXR85ciQcOQJjx6ojlLNmgdEI/53iDQxUH+uBxUoKpRCuqXzE6YknnmDatGl06dKFv/76S+tYwkk1bdqUBg0ayLS3A0mhdGXlI5OvvQY33gg//gi33w4bN8Itt6hfU5TKjwrS69X1lUVFjsursdLSUg4dOiRnUArhovz8/HjllVfYtGkT/v7+9O7dm4kTJ3LmzBmtowkno9PpiImJkY05DiSF0tV98glMnw433AAbNqgjki1b/vt1na7qo4JSUmDCBMfkdALp6ekoiiIjlEK4uI4dO7Ju3TreeOMN3nvvPdq2bcvSpUu1jiWcjNFoZPPmzfKGw0GkULq68HD116eegmbN1JHHS2GxgJcXfPopLFtmv3xOpPzIICmUQrg+Ly8vJk2axK5du2jVqhXDhg3jtttu4+TJk1pHE07CaDRisVhITk7WOopHkELp6nr1grCwfzfYXOgQYEVRiySAwQC+vhAZCR5y+KvJZEKv19OoUSOtowghbKRZs2YsW7aMjz/+mCVLltCmTRu++OILORBd0Lp1a+rUqSPrKB1ECqWrCw6GBQvA21v9/YXuhKPTqUWyrEw93HzYMMjJUXeEewCTyUSjRo3wLv93JYRwCzqdjnHjxrFnzx4GDRrEbbfdxvDhw0lPT9c6mtCQTqfDaDTKOkoHkULpDq6/HgYNOv/zVqv6AeroZEkJzJ2rHh305JNQr566/rJbN4fG1Yrs8BbCvdWrV4+vv/6aX3/9lV27dtG2bVvefvttLOUzM8LjxMTEsGHDBoo8aAOqVqRQugurFbKz1X8uK1MLpF6vfuTlqaOTixfDlClQvz489xx89JF6yLmfn7bZHUQKpRCeIT4+nt27d3PHHXfw6KOP0rdvX3bv3q11LKEBo9FIaWkp69ev1zqK25NC6S7OnIH774f169Xpb51O3cX90EMweDDExqpT4127qr8++ui/u8E9ZK2RyWSSI4OE8BDBwcHMnTuXNWvWkJOTQ+fOnXnhhRcoKSnROppwoPbt21OzZk1ZR+kAUijdRXAwFBTAXXep9+0ePRr691fvkBMaqq6VTE5WDzlv1kw9g7J8OvxC6y7dRH5+PidPnpQRSiE8TJ8+fdi2bRtPPfUUr732Gp07d5Zdvx5Er9fTr18/KZQOIIXSnbz3Hpw6BePHw9at6trKd95Rp7rXroU77lD/GdQjgy71iCE3IEcGCeG5fH19+b//+z+2bNlCcHAwffv25aGHHiIvL0/raMIBYmJiSE5Oltt12plOkbMV3EtaGpw8qU5jd+6sFkdQf5+fD8eOnX3wuYf45ZdfGDFiBEeOHCEiIkLrOEIIjVgsFv73v//x9NNPExoayvz584mLi9M6lrCjjRs30r17d5KTk+nVq5fWcdyW5wxReYrISIiOVndue3mp09qKok5rBwd7ZJkEdYTSz8+P8PKD4IUQHslgMPDII49U7AKPj4/nlltu4fjx41pHE3bSuXNnatSoIccH2ZkUSnen13vEGsmLMZlMREZGopN/F0IIIDIykt9++43PP/+cFStW0KZNGz799FM5EN0NeXl50adPH1lHaWdSKIVHkCODhBDn0ul0jBkzhj179jBs2DDGjRvHkCFDKtZcC/dhNBpZs2YNZrNZ6yhuSwql8AhyZJAQoip16tTh888/Z8mSJezdu5d27doxa9YsORDdjcTExJCfn8/27du1juK2pFAKt6coCqmpqTJCKYS4oGHDhrF7927uvvtunnjiCXr16sWOHTu0jiVsIDo6Gj8/P1lHaUdSKIXbO3HiBIWFhVIohRAXVaNGDebMmUNycjKFhYV07dqVZ555huLiYq2jiWrw9fWlV69eso7SjqRQeiBPm8aRMyiFEJerZ8+ebNmyheeee46ZM2fSsWNHKSMuzmg0kpSUhLX8ph7CpqRQeqCcnBzGjh1Lenq61lEcQgqlEOJK+Pj48Pzzz7Nt2zbCwsKIiYlhwoQJ5Obmah1NXIGYmBhOnz4t93W3EymUHsjPz48lS5Ywa9YsraM4hMlkombNmtSsWVPrKEIIFxQVFUVSUhJz587l888/Jyoqil9++UXrWOIy9ejRA29vbxlpthMplB4oMDCQhx9+mPfff5+TJ09qHcfu5MggIUR16fV6HnzwQVJSUujcuTMjRozgpptuIisrS+to4hIFBATQvXt32ZhjJ1IoPdRDDz0EwNy5czVOYn9yZJAQwlYaNWpEQkICX331FatXr6ZNmzZ89NFHciC6izAajSQmJsp/LzuQQumhwsLCuPvuu3nnnXcoKCjQOo5dyQilEMKWdDodN998M3v27OHaa6/lrrvuYvDgwRw4cEDraOIiYmJiOHbsGPv27dM6ituRQunBJk2aRG5uLh9++KHWUezGYrGQnp4uhVIIYXO1a9fmk08+YdmyZaSmptK+fXtmz559eaNfMlLmUL1790av18s6SjuQQunBIiMjueWWW3jzzTcpKyvTOo5dHDp0CLPZLIVSCGE311xzDbt27eKBBx7AYrFc3rE0Oh3k5sLatVBUZL+QAoCgoCC6dOki6yjtQAqlh5syZQoZGRl8/fXXWkexCzkySAjhCIGBgbz55ps8/vjjGAyGS/+Du3bBsGFwww3QrBksXGi/kAJQp71Xr14t6yhtTAqlh2vfvj3Dhw/n9ddfd8u/XOWFMjIyUtsgQgiPoNPpLv3BmZnwwguwcyfcdReMHw+LF4Os77Mro9HIoUOHSEtL0zqKW/HSOoDQ3tSpU4mJiWHJkiXExsZqHcemTCYTERER+Pn5aR1FCCH+VVICv/wCP/0E8+fDffepn8/OhtDQ8/9Z2Ey/fv3Q6XQkJibK7JUNyQiloF+/fvTs2ZPp06drHcXm5MggIYTTKC6GDRvUf16yBGbNgmuugZtv/vcx5QWyrAzef1+dBv/7b8dndWOhoaG0b99eNubYmBRKgU6n48knn2TNmjUkJydrHcem5MggIYTT+Pln6NkToqPhttvg5El4800ICTn/sd7e6rrKxo3VwilHEtlU+TpKYTtSKAUA8fHxtGnThhkzZmgdxaZSU1OlUAohnMPo0fDii5CaCh98oO7sbtu28sdaLNCiBYwcqa6pdPPzgh3NaDRy8OBBDh8+rHUUtyGFUgDqbcUmT57Mr7/+SkpKitZxbKKoqIijR49KoRRCOAedDpo0gZwcCAqC9u0rf5yiQPlO8R9+UKe9hU0ZjUYAmfa2ISmUosKYMWNo0KABM2fO1DqKTaSnpwNyZJAQwkmcOgXr14OfH3TsWPXjyk/cmD1b3QE+atSFHy8uW926dWndurUUShuSQikq+Pj4MGnSJD7//HMyMzO1jlNtcgalEMJpWK3wxx/wySfw0EPQqFHlj1MU0OvBZIK33oLeveH22/99DmEzso7StqRQirPcc8891KhRg9mzZ2sdpdpMJhNeXl40bNhQ6yhCCE+Xnq6OODZrBo8+WvXjys+x/PJL9dig226D8nN09fIj25aMRiN79uzh+PHjWkdxC/J/pzhLUFAQDz74IO+99x6nT5/WOk61mEwmGjdufHl3rRBCCFsqn77etg3WrYMnn4QGDSp/7H9HIKOiwN8f/lnrh9ls15ieqHwdZVJSksZJ3IMUSnGeRx55BIvFwrx587SOUi1yBqUQQnPlI44HDqj/XKeO+vtz70xmsagjkNnZMGUKvPQSnDkDTz+tft1L7kNiaw0bNqRZs2ayjtJGpFCK89StW5fx48czZ84cCgsLtY5zxeTIICGE0xg/Xj2DcswYdaPNubdoLJ9JmTwZPvpIHcWcOhVWr1aPFxJ2YTQaZR2ljUihFJV6/PHHOX36NAsXLtQ6yhWTQ82FEE6jdm1YuRImTYJdu87+WlmZ+utPP6kfN90Ev/4Kzz8P27dDr16Oz+shYmJi2LFjB9nZ2VpHcXlSKEWlmjVrxqhRo3jjjTcwu+DanZycHHJycqRQCiGch78/PPWUesA5qOdRbtum3hUnLw+eew6uugruukud/lYU9bxK2YxjN0ajEUVRWCujwNUm/5eKKk2ZMoW0tDS+++47raNcNjkySAjhtMoL4vHjMHAgdOigbr7JylLLZHS0+vVzp8XPoZy7DlNctqZNm9KwYUOZ9rYBKZSiSp07d+aaa65hxowZLveNSwqlEMLpXXUVJCVBt27g4wP/+x/ccov6tUs4c1Kn0/H9999TWlpq56DuS6fTYTQaZWOODUihFBf05JNPsn37dpYtW6Z1lMtiMpkICAigTvmOSiGEcEZt28KHH0JiojoVHhiofv4i09yKorBnzx5uvvlmoqOj2bBhgwPCuqeYmBg2b95Mfn6+1lFcmhRKcUH9+/enW7duzJgxQ+sol6X8yCDdRaaMhBDCKfj5XdbDdTodbdq0YdOmTfj4+NCzZ08ee+wxzpw5Y6eA7stoNGKxWFi3bp3WUVyaFEpxQTqdjqlTp/Lnn3+61DtgOTJICOEJOnXqxF9//cXrr7/OggULaNeuncvNKGmtVatW1K1bV9ZRVpMUSnFRI0aMoGXLli41SilHBgkhPIWXlxdPPPEEO3fupEWLFgwdOpTbb7+dkydPah3NJcg6StuQQikuymAwMGXKFH766Sf27t2rdZyLUhSFtLQ0KZRCCI/SvHlzVqxYwUcffcSiRYto06YNX375pcttqtSC0Whkw4YNFBUVaR3FZUmhFJdk7NixhIeHM3PmTK2jXFRWVhbFxcVSKIUQHken03HnnXeSkpLCwIEDGTNmDHFxcWRkZGgdzanFxMRQWlrK+vXrtY7isqRQikvi6+vLxIkT+eyzzzhy5IjWcS5IjgwSQni68PBwvvnmG3755Re2b99O27ZtmTt3LhaLRetoTqldu3aEhobKOspqkEIpLtl9992Hn58fb731ltZRLkgKpRBCqK699lp2797N2LFjefjhh+nXrx+7d+/WOpbT0ev19OvXT9ZRVoMUSnHJQkJCeOCBB3j33XfJycnROk6VTCYTYWFhBAUFaR1FCCE0FxISwrx580hKSuL06dN07tyZF198kZKSEq2jORWj0ci6devkoPgrJIVSXJZHH32U0tJS5s+fr3WUKsmRQUIIcb6+ffuybds2pk6dyiuvvEKXLl3k7MX/iImJoaioiE2bNmkdxSVJoRSXJTw8nHHjxjFnzhyKi4u1jlMpOTJICCEq5+fnx7Rp09iyZQs1atSgT58+PPzww3KXGNQzPYOCgmTa+wpJoRSX7YknnuD48eN88sknWkeplBRKIYS4sPbt25OcnMysWbP46KOPaNu2LUuWLNE6lqa8vLzo06ePbMy5QlIoxWVr2bIlI0eOZObMmU63Y7CsrIzMzEwplEIIcREGg4GJEyeye/du2rRpQ2xsLLfeeisnTpzQOppmjEYja9euxWw2ax3F5UihFFdk6tSpHDx4kB9++EHrKGfJzMzEarVKoRRCiEsUGRnJ0qVL+fTTT1m2bBlt2rThs88+88gD0WNiYsjPz2fbtm1aR3E5UijFFYmOjmbQoEHMmDHDqb7pyJFBQghx+XQ6HWPHjmXPnj1cc8013H777QwdOpS0tDStozlUdHQ0/v7+so7yCkihFFds6tSpbNmyhd9//13rKBVMJhM6nY4mTZpoHUUIIVxO3bp1+fLLL1m0aBF79uyhbdu2vPXWW063vMlefHx86NWrl6yjvAJSKMUVGzx4MJ07d2bGjBlaR6lgMplo2LAhPj4+WkcRQgiXFRsby+7duxk/fjyTJk2iV69e7NixQ+tYDmE0GklKSsJqtWodxaVIoRRXTKfT8eSTT7Jy5Uo2b96sdRxAzqAUQghbCQoK4p133mHNmjWcOXOGrl278txzzzntkXG2YjQayc7OljsKXSYplKJaRo4cSfPmzZ1mlFKODBJCCNvq3bs3W7du5dlnn2XGjBl07tyZNWvWaB3Lbnr27Im3t7dMe18mKZSiWgwGA0888QQ//PADBw4c0DqOFEohhLADX19fXnjhBbZu3UrNmjXp168fDzzwAHl5eVpHszl/f3+6d+8uG3MukxRKUW3jxo0jLCyMN954Q9McBQUFHD9+XAqlEELYSdu2bVmzZg1vv/02n376KVFRUfz6669ax7K5mJgYVq9e7VSnmDg7KZSi2vz9/Zk4cSIff/wxWVlZmuUoP95CCqUQQtiPwWDg4YcfJiUlhQ4dOnDdddcxevRojh07pnU0mzEajRw/fpx9+/ZpHcVlSKEUNjFhwgR8fHyYM2eOZhnKz6Bs1qyZZhmEEMJTNG7cmMWLF/Pll1+yatUq2rRpw8KFC91iVK93794YDAZZR3kZpFAKm6hZsyb33Xcf8+fP12xNjclkwtfXl4iICE2uL4QQnkan03HLLbewZ88e4uLiGD9+PFdffTWpqalaR6uWoKAgunTpIusoL4MUSmEzEydOpLCwkAULFmhy/dTUVJo0aYJeL/9bCyGEI4WFhfHpp5+ydOlSDhw4QLt27XjzzTdd+p7Yso7y8shPXmEzDRo0YOzYscyePZuSkhKHX192eAshhLaGDBnCrl27uO+++5g8eTI9e/Z02ftiG41GDh065HG3n7xSUiiFTU2ePJmsrCw+++wzh19bCqUQQmivRo0azJ49m3Xr1lFSUkJ0dDRPPfUURUVFWke7LH379kWn08k6ykskhVLYVOvWrRkxYgQzZ8506L1fFUWRQimEEE6kR48ebN68mRdffJFZs2bRsWNHlypnoaGhdOjQQdZRXiIplMLmpk6dyr59+/jll18cds3Tp0+Tn58vhVIIIZyIj48Pzz77LNu3b6devXr079+fe++9l5ycHK2jXRKj0SiF8hJJoRQ216NHD2JiYpgxY4bDFjOXHxkkhVIIIZxP69atWb16NfPmzePrr78mKiqKn376SetYFxUTE8PBgwc5fPiw1lGcnhRKYRdPPvkkGzZscNj0hpxBKYQQzk2v1zNhwgRSUlKIjo7mhhtuYOTIkRw9elTraFXq168fgIxSXgIplMIuhgwZQseOHZk+fbpDrpeamkpwcDChoaEOuZ4QQogr07BhQ3755Re++eYb1qxZQ5s2bfjggw+c8nieunXr0qZNG5da+6kVKZTCLnQ6HVOmTGHZsmUOOTKifEOOTqez+7WEEEJUj06nY9SoUezZs4frr7+ee+65h4EDB7J//36to51H1lFeGimUwm5GjRpFZGQkr7/+ut2vJTu8hRDC9dSqVYuFCxeyYsUK0tPT6dChAzNmzKCsrEzraBViYmLYs2cPx48f1zqKU5NCKezGy8uLxx9/nG+++cbut+GSQimEEK5r8ODB7Ny5k4ceeoinn36a7t27s3nzZq1jAeoIJUBSUpLGSZybFEphV+PHj6dWrVq8+eabdruG1WolPT1dCqUQQriwwMBAZs6cyfr161EUhe7duzNlyhQKCws1zdWgQQOaN29+1jpKRVEwWxWKLVYKy6wUW6yYrYpTrgN1FC+tAwj3FhAQwCOPPMKrr77KCy+8QN26dW1+jSNHjlBaWiqFUggh3EB0dDQbN27kzTff5MUXX+THH3/kvffeY+DAgZrkURSFQdeOJL2wjN8PneFooZmsIjNm6/mP9dJDuL8XEQFehP/zUcvX4BHr+3WKJ9dp4RCnT5+mcePGPPbYY0ybNs3mz5+UlITRaGT37t1ERUXZ/PmFEEJoY9++fdxzzz0kJiYyfvx43njjDYed5pFdYmHryWK2nyqmxKJWJT1QSY88z38f52vQ0bG2H53D/Aj1NdgprfakUAqHeOyxx/jkk0/IyMigRo0aNn3uTz75hDvuuIOCggICAgJs+txCCCG0ZbVa+eCDD5g8eTL+/v6888473HjjjXYZ9bMqCgfzStl8opi0/DJ0gC1KUvnzRAZ507WOH82DfdC72ailrKEUDjFp0iTy8/N5//33bf7cJpOJevXqSZkUQgg3pNfruffee9mzZw+9evVi1KhRXH/99Ta/e03mmTLeS8nmh9R80vPVXea2GnErf570/DJ+SM3nvZRsMs84z052W5BCKRyiUaNGjBkzhjfffJPS0lKbPrfs8BZCCPdXv359fvrpJ3744QfWr19PVFQU7777LlbrpUxCV63MqrDy0Bm+2J9Lbqn6XPaaui1/3txSK1/sz2XloTOUWd1jolgKpXCYKVOmcPjwYb788kubPq8USiGE8Bw33HADKSkpjBo1igkTJtC/f3/27t17Rc+VeaaMD/Zks/lEMWC/Inmu8utsPlHMB3vcY7RSCqVwmKioKOLj43n99der/Y7yv6RQCiGEZwkNDeX9999n1apVHD16lI4dO/LKK69c1oHom04U8cX+XPJKrQ4rkudSgLx/Ris3nSjSKIVtSKEUDvXkk0+yZ88eFi1aZJPnKykp4fDhw1IohRDCAw0YMIAdO3YwceJEXnjhBbp27crGjRsv+GcURWFtViErDxWov3dE0Avl+efXlYcKWJtV6LJnWUqhFA7Vu3dv+vbty/Tp023ylyYjIwNFUWjWrJkN0gkhhHA1/v7+TJ8+nQ0bNuDl5UXPnj2ZNGkSBQUFlT4++VgRSUe1PSy9KklHC0k+5pojlVIohcNNnTqVdevWsWbNmmo/V/ktHWWEUgghPFuXLl3YsGED06dPZ/78+bRr147ly5ef9ZhNx523TJZLOlroktPfUiiFww0fPpy2bdsyY8aMaj+XyWTCYDDQqFEjGyQTQgjhyry8vJg8eTI7d+6kWbNmDBkyhHHjxnHq1Ckyz5Sx8nDlo5bOZuWhApfbqCOFUjicXq9nypQpLF68mF27dlXruUwmE40aNcLLS+4iKoQQQtWiRQtWrlzJhx9+yK+//kr7Tp35ZncWrnKUuA5YlJ7vUkcKyZ1yhCbKyspo3rw5/fv359NPP73i5xk1ahQnT55k1apVNkwnhBDCXWRlZTHjl9XU6dofvcF1bn2oA6Lr+DGooW3vLmcvMkIpNOHt7c3jjz/Ol19+SXp6+kUfrygKZqtCscVKYZmVYosVs1WRI4OEEEJcUFmN2tTrPsilyiSou783nih2malvGaEUmikoKKBx48bcdtttzJkzp+LziqJwusRCVqGZrEIzRwvNZBWZMVdydGVZcRFehTl0b9WU8AAvwgO8qOVrsMs9XoUQQrgWq6LwXko2uRqeNVkdOiDER8+9UaFOf+9vKZRCUy+88AJvvPEGGRkZ6GvUZOvJYrafKqbEov5vqQcuegS6oqDX6Soe52vQ0bG2H53D/Aj1da13pEIIIWxnf24JP6Tmax2j2m5sFkyLEB+tY1yQFEqhqeMnTnLNreMZ9cSL6Os2RodtDpktf57IIG+61vGjebCP07+7E0IIYVtfH8glPb/MJUcny+lQf5aNbhGidZQLkkIpNJN5pozF6fnklFqxWszoDbbfqV1eLGv66IltEkSjGt42v4YQQgjnk11iYUFKttYxbOa+qFCnnnWTQikcrsyqsPpIAZtOFNtsRPJiyq8TXcePmPqBeOtltFIIIdzZqsMFbDxeZPOfMe/dcx2mzclVfv2Od76mVZ9BNr2mDuhe158BDQJt+ry2JIf3CYfKPFPGovR88krVFY+OejdTfp3NJ4rZn1tKnIxWCiGE21IUhe2niu36M6bdoDh8As4veCF1I2x+LQXYdqqY/vUDnHbTqRRK4TCbThSx8lCBw0YlK6MAeaVWvtify+CGgUTX8dcoiRBCCHs5XWKp2NxpL8Mf+z9C6ze26zX+q8SikF1ipZafc057yzmUwu4URWFtViErD6m3vNJ6jUX59VceKmBtViGy6kMIIdxLVqFZ6wh24cyvSwqlsLvkY0UkHS3UOkalko4WknysSOsYQgghbCir0Ox2BUevg6wi5y2UMuUt7GrTcectk+WSjhbia9DJ9LcQQriJo4Xmi59hXE0bf/6CwtxsdDodYU2a07b/cGpGNLTb9awKHClw3rvmSKEUdpN5poyVhwu0jnFJVh4qoJ6/l2zUEUIIF6coikNG8v74YNZZv/9t9osMuOdxBt3zuN2umVVkRlEUp9yYI4VS2EWZVWFRer6mG3Auhw5YlJ7P3W1C5UghIYRwYRaFSm/VaytNu/Si24jbaNKxG0Fh9cg5doRdK3/ljw9ns3L+dPwCa9Dn1vvscm2zVR2pNDjhjyk5h1LYxcpDZ9h8wr5HNtiaDvWcykENa2gdRQghxCVSFIWSkhKKioooKioir7CIn/Mdf1eZfev+YOGDo/CrEczTy3fh7WefZVSPdaiFr8H5VojKCKWwucwzZWw6Uax1jMumABtPFHNVTV+Z+hbuRVEgNxeOHwerFRo1gsBAKCuDxETYtw8aNIA+faB2ba3TChdnNpsryp0tP4qLi6v8/H/HxgJr1ubZVX87/HVf1WsADaI6cThlGxk7N9O8W1+7XMdiBZzw5CAplMKmrIrCYhea6j6XDlicns+9UaFy72/h+hQFdDr4/Xd4+mnYvBl69YI5c6BrVygshB9+UD+sVrjnHnj+efDz0zq5sBFFUaosYpda2C73w2y+vPWL/v7+F/wICAigdu3aF31c+Ye3fyBb7PTv82LCGjfjcMo28k8es9s1nHBwEpBCKWzsYF4pOaX23ltnPwqQU2olNa+MFiE+WscRonp0OrVUvv8+/P03fPUVDBmijk4ChITA7Nnwyitw//3w2Wdw/fXQrdu/ZVTYlL1G7y5UEi+Ht7f3RQvb5ZS78g8/P79KP+/r62vzDSZmq8KW7ads+pyXqigvB6DSO+jYipeT/r2UQilsarMD789tLzpg84kiKZTCPZw4AVlZEBMDI0eC4Zy5Mm9vCA2FwYPhzz/h1D8/iD2gUF7O6J2tyt3ljN7pdLqLFrXAwEDCwsKqLGyX+2E49/8PF2TQgZfevhtzKnMm+yRpW/8CoEHrDna5hpdePY/SGUmhFDaTXWIhLd95z8i6VApgyi8ju8RCqK/rf3MVAoMBSkrUae1zC0P52jMvL7VAXuaIli2VlZXZZY2dI0fvwsLCbFLs/P3/v717jZGqvOM4/jtnrju7s8uyO+ylSxdBCCDGxcWoCGgFklY2pjESrFKD8UI10mpfYI2a1jRpa0P0rbVeahNToa2pUYxNCbZC2kZRwHukARdw2bLAstfZ2bmcvjgdkdteOOfMzNn9fpJNNrvDOc/AYeY3//M8/6dM4XC4JNvDlDrDMFRfFtThAfdbBx38YJfSqSHNXHTNaf823R0HtfnRezWcHNS8a7+tqrpG188tSfVlwZK9JgiUcM3uY+5XJ4eTg9r377/rs7f/qsMf71H3kYPKZXOqmX6RFixv05K1P1Ak5v6qbEPSnmND+tY3vLttARREICBFIvZ8yXPJvzmVl9uBM/3/D4WZjBQeuUq/bds2HT161LUKXjabHfPTGk/1zo1wF41GJ0T1brJoiAXVMeB+c/OuL/bpTz/7oeK1daptnqV4zTT1HO3Ql59+oExqSHWz5uqmx54c/UAXwDSkxvLSXTBKoIQrLMvS3uPutwna++af9crPfyxJqps1V3MWX6+h/j4d/OBdbXv6Ce198xXd8+yrqpiacPW8lqQ9x4d0XWOsZD8NYnI5s3oXi8WUSCRGvz6Hh6XeXrsCGRrhzSiRkJJJaedOafXqUcOkJG3cuFG7d++WJIXD4VFDWSKRGPccO6p3uBD1saAnO+VMX9CqK1ffoUMfvqej+z9X+953FI7G1DBngS5deaOuunmdZ+2CcpZdoSxVpTsy+MqJVFaprPszJwOhsK5cfYeW3LZetd+c9dXPe7s69eKPblXHZx/q9U2P6pZf/Mb1c6eylrpTOU2NUpXA6XK5nKtz78ZyrDOrdxs2bNBTTz01ctUsmZS2bpX27rUX3diDl8yvLRPNf9/aKm3YIG3ZIh08KC1cKD30kF3dPI/t27crFApRvUPJqY95E2+mzZyj7z78a0+OPRZePS83lO7I4Cudg95sc3V52xpd3rbmrJ9XJup140NP6Ok7btDH27cqkx5WMOT+IprOwQyBssRZljXq3Du3F16kUqlxjdFp9e5cXzNmzBg5xHV3n+opuXKlHQ6l08Pk11VWSnPn2n/u4EFp/vxRn9eUKVPG9fcAFMrUSECRgOFJoaNYIgFD1ZES7RkkAiVc0jmYkSl5covhfBrmXCJJygynNHjyhCoT9a4e3zTsfVPn6/wVGpzN7erdWL5yubFfeaZpjhrW4vG4pk2bNua5dSU5966qSnr/femtt6RHHpHuv1/avPnsCmXegQN2+6CWFum11+xelOcLn0CJMwxDl9VE9e7RpK+7juQZklpqoiU9zYNACVccGXR/8vNoTnzZLkkKBEOKVVW7fvycJXUM+HvV+liqd27fsh1v9S4SiYwayMYa7sbyFQqFSvpF2TWmaYfD2bPthuY7dtgruM9sWp5vDzQ0ZLcYuv12KRY7tfob8KmFtVG9czRZ7GG4wpLUUlvaGw4QKOGYZVnqTHpzy3sk//zDM5KkOYuvVzDsTRWxM5mRZVmuBZBcLufJ/LpCVu8qKytVV1fn2spZkyqYt4JBqanJnk/Z03N2oMxf2+GwHULz18sk6EOJia06EtCMeEjtfWlfVykNSTPioZJvY0eghGNZq/ANZD/b+Tft+stLCgRDWnnfTzw7TyYnbd7yRw0lB12p5A0PD4/r/KNV7/IBz2mom3TVu8kkELBDZDotnastTz44VlTY3+evUa4DTACtiajv+yNbkloT3qwcdxOBEo5lCnxr7Oj+z7Xl0ftkWZa+88BP1TBngafnW3fnnUoN9CsQCIwazqqqqlRfXz+uOXZU7+Ap05TicfuW9okTUuMZDZfzwTGVkvr7T7UWIlBiAphVGdaUsKme4Zwvq5SGpKqwqZmVpdt/Mo9ACcfGcUfVsZ7/duiF+9co2XtSS9beq2tuXe/5OdsPHdbUiphCI/XwA0qVaUqXXmoHxieflO6+2w6Vzc327w8fthfrvP22/Z/5uuuKOlzATaZhaFVzXC/t6yn2UC6IJamtOS7TBx/wKH/AsUIV0Qa6j+u5e2/Wyc7Dar3xe7rhwccLct4plXHCJPxt+XJp3TrpjTekVaukF1889bsDB6TnnrNvdW/aJC1dWrRhAl6YXhHSokRUpR/JTmdIuiIRVVOFP95/qFDCsWABPjmlBvr1woZb1PXFPl1y/Srd9NhTBZvrV4jnB3gqEJCef97+vr//9F1wliyRPvqIFkGY0K5tLNe+nmH1+uTWtyGpMmxqWaN/tv/lFQSOBQwp6OGVlBlO6fcPfl9ffrJHs6/+lm755TMyC9TXL2ja/SiBCaOi4vRAaRiESUx4IdNQW3PcF2FSOnWrO+SjNyBeReCYYRie7S+ay2b18sPrtX/XTs1YeJXWbvqdJzvinE99WZBVzwAwAUyvCGlFkz8qfiuayjXdJ7e687jlDVc0xILqGHC/ufm/Nj+rj9/aKkkqn1KjV3+18ZyPu+GBx1VeXePquU1Daiz3139oAMD5LUqUKZW1tOPIYLGHcl5LG2Ja5IM2QWciUMIV9bGgJzvlJHtPrczLB8tzWbF+o+uBMmfJs8orAKA4FtfZYa0UQ+WyhpiurvNfmJQkw7LYXwvOHR/K6Lefniz2MFx3z7xqTY2W9u4EAIDx29WV1LbDAzKkos6tzJ9/RVO5LyuTeZRf4IqpkYAiAUOp7MT5fBIJGKqOMM0YACaiRYky1ZUF9Xp7X9FWf+dXc7c1x303Z/JMvFvCFYZh6LIa//X5Oh9DUktNlAU5ADCBTa8I6a551WpN2HvcF+oVP3+eRYmo7ppX7fswKREo4aKFtVHftGQYjSWppTZa7GEAADwWMg2taKrQbbOrVBW2Y5FXwTJ/3KqwqdtmV2l5U4WvWgONhDmUcNXL/+lRe1/a18HSkDQjHtKai6uKPRQAQAHlLEv7e9N6ryupA31p1+ZX5o9zUTyk1kSZZlaGfLGd4ngwhxKuak1E9UVfutjDcMSS1OrjidEAgAtjGoYurgrr4qqwulNZ7Tk2pD3Hh75aH2AadgeQ0Y9z6nGRgKGWmqhaaqOqjkzcRZ5UKOGqnGXpmU+61eOT7a3OZMi+FXHP/OoJ9+kRADB+lmWpO5VT52BGncmMOgbS6kxmlDlHr7ygabebaywPqb4sqPpYUNURc1LMxydQwnWH+tN6aV/P6A8sUWtnV6lpAkyQBgB4w7Is5SwpY1nK5qSAKQUNQ6ahSREez4VFOXDd9IqQFiX8t+LbkHRFIkqYBACMyDAMBUxDkYCpWMhUJGAqYBqTNkxKBEp45NrGclWGTd+EynwvsGWN/tjnFQCAUkKghCdCpqG25rhv5lFaktqa4xOmfQMAAIVEoIRnpleEtKLJHxW/FU3lE6KxLAAAxUCghKcWJcq0tCFW7GGMaGlDzNf7pwIAUGz0oYTnFtfZYW3HkcEij+RsyxpiurqOMAkAgBO0DULB7OpKatvhAdd2HrhQ+fOvaCqnMgkAgAsIlCioQ/1pvd7ep94iNT7Pr+Zua44zZxIAAJcQKFFw6Zylf3QMaFfXUMGqlfnzXJGIalljOau5AQBwEYESRXOoP62t7X06OZzzLFjmjzslbGoVVUkAADxBoERR5SxL+3vTeq8rqQN9adeCZf44F8VDak2UaWZliL25AQDwCIESJaM7ldWeY0Pac3xIqax9WZqGlBvDFfr1x0UChlpqomqpjao6EvBwxAAAQCJQogRZlqXuVE6dgxl1JjPqGEirM5lRJnf2Y4OmVF8WVGN5SPVlQdXHgqqOmJN6P1UAAAqNQAlfsCxLOUvKWJayOSlgSkHDkGmI8AgAQJERKAEAAOAIWy8CAADAEQIlAAAAHCFQAgAAwBECJQAAABwhUAIAAMARAiUAAAAcIVACAADAEQIlAAAAHCFQAgAAwBECJQAAABwhUAIAAMARAiUAAAAcIVACAADAEQIlAAAAHCFQAgAAwBECJQAAABwhUAIAAMARAiUAAAAcIVACAADAEQIlAAAAHCFQAgAAwBECJQAAABz5H0fB6lki7XhKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset[num]\n",
    "data\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(range(data.num_nodes))\n",
    "edges = data.edge_index.t().tolist()\n",
    "# edge_attrs = {tuple(edge): attr.item() for edge, attr in zip(edges, data.edge_attr)}\n",
    "g.add_edges_from(edges)\n",
    "print(len(edges))\n",
    "\n",
    "# Draw the graph with edge attributes\n",
    "pos = nx.spring_layout(g)  # positions for all nodes\n",
    "nx.draw(g, pos, with_labels=True, node_color='skyblue', node_size=1500, edge_color='k', linewidths=1, font_size=15)\n",
    "nx.draw_networkx_edge_labels(g, pos, font_color='red', font_size=12)  # Add edge labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_num_nodes: 4\n",
      "max_num_nodes: 165\n",
      "mean_num_nodes: 21.76829268292683\n",
      "min_num_edges: 3\n",
      "max_num_edges: 208\n",
      "mean_num_edges: 28.341463414634145\n",
      "mean nodes degree: 1.3019607843137253\n"
     ]
    }
   ],
   "source": [
    "def graph_stat(dataset):\n",
    "    \"\"\"\n",
    "    TODO: calculate the statistics of the ENZYMES dataset.\n",
    "    \n",
    "    Outputs:\n",
    "        min_num_nodes: min number of nodes\n",
    "        max_num_nodes: max number of nodes\n",
    "        mean_num_nodes: average number of nodes\n",
    "        min_num_edges: min number of edges\n",
    "        max_num_edges: max number of edges\n",
    "        mean_num_edges: average number of edges\n",
    "    \"\"\"\n",
    "    # for ind,data in enumerate(dataset):\n",
    "        # print(verilog_files[ind])\n",
    "        # print(data)\n",
    "        # print(len(data.x[1]))\n",
    "        \n",
    "    nodes_edges = [(data.num_nodes, data.num_edges) for data in dataset]\n",
    "    num_nodes, num_edges = list(list(zip(*nodes_edges))[0]), list(list(zip(*nodes_edges))[1])\n",
    "    min_num_nodes = min(num_nodes)\n",
    "    max_num_nodes = max(num_nodes)\n",
    "    mean_num_nodes = np.mean(num_nodes)\n",
    "    min_num_edges = min(num_edges)\n",
    "    max_num_edges = max(num_edges)\n",
    "    mean_num_edges = np.mean(num_edges)\n",
    "    mean_degree = (mean_num_edges)/mean_num_nodes\n",
    "    \n",
    "    print(f\"min_num_nodes: {min_num_nodes}\")\n",
    "    print(f\"max_num_nodes: {max_num_nodes}\")\n",
    "    print(f\"mean_num_nodes: {mean_num_nodes}\")\n",
    "    print(f\"min_num_edges: {min_num_edges}\")\n",
    "    print(f\"max_num_edges: {max_num_edges}\")\n",
    "    print(f\"mean_num_edges: {mean_num_edges}\")\n",
    "    print(f\"mean nodes degree: {mean_degree}\")\n",
    "\n",
    "graph_stat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "# # Define the sizes of training, validation, and test sets\n",
    "# train_size = int(0.7 * len(dataset))  # 70% of the data for training\n",
    "# val_size = int(0.15 * len(dataset))   # 15% of the data for validation\n",
    "# test_size = len(dataset) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create DataLoader for each set\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# Define the size of the training set (e.g., 70% of the data)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "\n",
    "# Calculate the size of the testing set\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[8, 17], edge_index=[2, 7], y=[1, 9], batch=[8])\n"
     ]
    }
   ],
   "source": [
    "# len(train_loader.dataset)\n",
    "print(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "batch = next(loader_iter)\n",
    "# print(batch)\n",
    "# print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(in_channels, out_channels))\n",
    "        # Initialize the parameters.\n",
    "        stdv = 1. / math.sqrt(out_channels)\n",
    "        self.theta.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Generate the adjacency matrix with self-loop \\hat{A} using edge_index.\n",
    "            2. Calculate the diagonal degree matrix \\hat{D}.\n",
    "            3. Calculate the output X' with torch.mm using the equation above.\n",
    "        \"\"\"\n",
    "\n",
    "        num_nodes = x.shape[0]\n",
    "        A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (num_nodes, num_nodes))\n",
    "        A = A.to_dense()\n",
    "        A_hat = A + torch.eye(num_nodes)\n",
    "        \n",
    "        A_sum = torch.sum(A_hat, dim=1)\n",
    "        D = torch.pow(A_sum, -0.5)\n",
    "        D[D == float('inf')] = 0.0\n",
    "        D_hat_sqrt = torch.diag(D)\n",
    "        \n",
    "        first = torch.mm(torch.mm(D_hat_sqrt, A_hat), D_hat_sqrt)\n",
    "        second = torch.mm(x, self.theta)\n",
    "        \n",
    "        ret = torch.mm(first, second)\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gcn1): GCNConv()\n",
       "  (a1): ReLU()\n",
       "  (gcn2): GCNConv()\n",
       "  (a2): ReLU()\n",
       "  (gcn3): GCNConv()\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Define the first convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            2. Define the first activation layer using `nn.ReLU()`;\n",
    "            3. Define the second convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            4. Define the second activation layer using `nn.ReLU()`;\n",
    "            5. Define the third convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            6. Define the dropout layer using `nn.Dropout()`;\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        num_node_features = 17\n",
    "        num_output_classes = 9\n",
    "        \n",
    "        # num_channels = 32\n",
    "        \n",
    "        self.gcn1 = GCNConv(in_channels=num_node_features, out_channels=128)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.gcn2 = GCNConv(in_channels= 128, out_channels=128)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.gcn3 = GCNConv(in_channels=128, out_channels=128)\n",
    "        # self.a3 = nn.ReLU()\n",
    "        # self.gcn4 = GCNConv(in_channels=128, out_channels=128)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.linear = nn.Linear(in_features=128, out_features=num_output_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "    \n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.a1(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = self.a2(x)\n",
    "        x = self.gcn3(x, edge_index)\n",
    "        # x = self.a3(x)\n",
    "        # x = self.gcn4(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(x, dim=-1)\n",
    "        \n",
    "        return probs\n",
    "        \n",
    "        \n",
    "        \n",
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.2281, Test Acc: 0.1000\n",
      "Epoch: 002, Train Acc: 0.3246, Test Acc: 0.3200\n",
      "Epoch: 003, Train Acc: 0.3333, Test Acc: 0.3200\n",
      "Epoch: 004, Train Acc: 0.4211, Test Acc: 0.3000\n",
      "Epoch: 005, Train Acc: 0.4123, Test Acc: 0.2600\n",
      "Epoch: 006, Train Acc: 0.4211, Test Acc: 0.2600\n",
      "Epoch: 007, Train Acc: 0.4211, Test Acc: 0.2600\n",
      "Epoch: 008, Train Acc: 0.4298, Test Acc: 0.2600\n",
      "Epoch: 009, Train Acc: 0.4386, Test Acc: 0.2600\n",
      "Epoch: 010, Train Acc: 0.4298, Test Acc: 0.2800\n",
      "Epoch: 011, Train Acc: 0.4825, Test Acc: 0.3800\n",
      "Epoch: 012, Train Acc: 0.5000, Test Acc: 0.3800\n",
      "Epoch: 013, Train Acc: 0.5088, Test Acc: 0.3800\n",
      "Epoch: 014, Train Acc: 0.5088, Test Acc: 0.3600\n",
      "Epoch: 015, Train Acc: 0.5351, Test Acc: 0.4400\n",
      "Epoch: 016, Train Acc: 0.5351, Test Acc: 0.4000\n",
      "Epoch: 017, Train Acc: 0.5263, Test Acc: 0.4000\n",
      "Epoch: 018, Train Acc: 0.5351, Test Acc: 0.4200\n",
      "Epoch: 019, Train Acc: 0.5351, Test Acc: 0.3800\n",
      "Epoch: 020, Train Acc: 0.5526, Test Acc: 0.4400\n",
      "Epoch: 021, Train Acc: 0.5439, Test Acc: 0.4000\n",
      "Epoch: 022, Train Acc: 0.5614, Test Acc: 0.4000\n",
      "Epoch: 023, Train Acc: 0.5351, Test Acc: 0.3800\n",
      "Epoch: 024, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 025, Train Acc: 0.5614, Test Acc: 0.4000\n",
      "Epoch: 026, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 027, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 028, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 029, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 030, Train Acc: 0.5702, Test Acc: 0.4800\n",
      "Epoch: 031, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 032, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 033, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 034, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 035, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 036, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 037, Train Acc: 0.4912, Test Acc: 0.4000\n",
      "Epoch: 038, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 039, Train Acc: 0.5789, Test Acc: 0.3800\n",
      "Epoch: 040, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 041, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 042, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 043, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 044, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 045, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 046, Train Acc: 0.5614, Test Acc: 0.4200\n",
      "Epoch: 047, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 048, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 049, Train Acc: 0.5351, Test Acc: 0.4200\n",
      "Epoch: 050, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 051, Train Acc: 0.5965, Test Acc: 0.4400\n",
      "Epoch: 052, Train Acc: 0.5965, Test Acc: 0.4800\n",
      "Epoch: 053, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 054, Train Acc: 0.5965, Test Acc: 0.4600\n",
      "Epoch: 055, Train Acc: 0.5526, Test Acc: 0.3800\n",
      "Epoch: 056, Train Acc: 0.5877, Test Acc: 0.4400\n",
      "Epoch: 057, Train Acc: 0.5789, Test Acc: 0.3800\n",
      "Epoch: 058, Train Acc: 0.5789, Test Acc: 0.4000\n",
      "Epoch: 059, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 060, Train Acc: 0.5965, Test Acc: 0.4400\n",
      "Epoch: 061, Train Acc: 0.5877, Test Acc: 0.4400\n",
      "Epoch: 062, Train Acc: 0.5702, Test Acc: 0.3800\n",
      "Epoch: 063, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 064, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 065, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 066, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 067, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 068, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 069, Train Acc: 0.5263, Test Acc: 0.4200\n",
      "Epoch: 070, Train Acc: 0.5614, Test Acc: 0.4600\n",
      "Epoch: 071, Train Acc: 0.5789, Test Acc: 0.4400\n",
      "Epoch: 072, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 073, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 074, Train Acc: 0.5965, Test Acc: 0.4600\n",
      "Epoch: 075, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 076, Train Acc: 0.5789, Test Acc: 0.4600\n",
      "Epoch: 077, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 078, Train Acc: 0.6053, Test Acc: 0.4400\n",
      "Epoch: 079, Train Acc: 0.5614, Test Acc: 0.4600\n",
      "Epoch: 080, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 081, Train Acc: 0.5965, Test Acc: 0.4000\n",
      "Epoch: 082, Train Acc: 0.5614, Test Acc: 0.4000\n",
      "Epoch: 083, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 084, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 085, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 086, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 087, Train Acc: 0.6140, Test Acc: 0.4600\n",
      "Epoch: 088, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 089, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 090, Train Acc: 0.6140, Test Acc: 0.4600\n",
      "Epoch: 091, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 092, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 093, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 094, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 095, Train Acc: 0.6140, Test Acc: 0.4800\n",
      "Epoch: 096, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 097, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 098, Train Acc: 0.5965, Test Acc: 0.4600\n",
      "Epoch: 099, Train Acc: 0.6140, Test Acc: 0.4800\n",
      "Epoch: 100, Train Acc: 0.5702, Test Acc: 0.4400\n",
      "Epoch: 101, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 102, Train Acc: 0.5877, Test Acc: 0.4000\n",
      "Epoch: 103, Train Acc: 0.5877, Test Acc: 0.4600\n",
      "Epoch: 104, Train Acc: 0.5439, Test Acc: 0.4400\n",
      "Epoch: 105, Train Acc: 0.5789, Test Acc: 0.4600\n",
      "Epoch: 106, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 107, Train Acc: 0.5965, Test Acc: 0.4800\n",
      "Epoch: 108, Train Acc: 0.6053, Test Acc: 0.4200\n",
      "Epoch: 109, Train Acc: 0.5789, Test Acc: 0.4400\n",
      "Epoch: 110, Train Acc: 0.6140, Test Acc: 0.4600\n",
      "Epoch: 111, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 112, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 113, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 114, Train Acc: 0.5614, Test Acc: 0.4200\n",
      "Epoch: 115, Train Acc: 0.6053, Test Acc: 0.4200\n",
      "Epoch: 116, Train Acc: 0.6053, Test Acc: 0.4400\n",
      "Epoch: 117, Train Acc: 0.6053, Test Acc: 0.4200\n",
      "Epoch: 118, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 119, Train Acc: 0.5702, Test Acc: 0.4400\n",
      "Epoch: 120, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 121, Train Acc: 0.6140, Test Acc: 0.4200\n",
      "Epoch: 122, Train Acc: 0.6140, Test Acc: 0.4800\n",
      "Epoch: 123, Train Acc: 0.6228, Test Acc: 0.4800\n",
      "Epoch: 124, Train Acc: 0.5877, Test Acc: 0.4600\n",
      "Epoch: 125, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 126, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 127, Train Acc: 0.5702, Test Acc: 0.5000\n",
      "Epoch: 128, Train Acc: 0.5965, Test Acc: 0.4800\n",
      "Epoch: 129, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 130, Train Acc: 0.6228, Test Acc: 0.4800\n",
      "Epoch: 131, Train Acc: 0.6228, Test Acc: 0.4600\n",
      "Epoch: 132, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 133, Train Acc: 0.6228, Test Acc: 0.4600\n",
      "Epoch: 134, Train Acc: 0.6316, Test Acc: 0.4800\n",
      "Epoch: 135, Train Acc: 0.6404, Test Acc: 0.4600\n",
      "Epoch: 136, Train Acc: 0.6491, Test Acc: 0.4800\n",
      "Epoch: 137, Train Acc: 0.5614, Test Acc: 0.5000\n",
      "Epoch: 138, Train Acc: 0.5702, Test Acc: 0.5000\n",
      "Epoch: 139, Train Acc: 0.6228, Test Acc: 0.5200\n",
      "Epoch: 140, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 141, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 142, Train Acc: 0.6491, Test Acc: 0.4800\n",
      "Epoch: 143, Train Acc: 0.6579, Test Acc: 0.5000\n",
      "Epoch: 144, Train Acc: 0.6579, Test Acc: 0.4800\n",
      "Epoch: 145, Train Acc: 0.6579, Test Acc: 0.4800\n",
      "Epoch: 146, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 147, Train Acc: 0.6053, Test Acc: 0.5000\n",
      "Epoch: 148, Train Acc: 0.5965, Test Acc: 0.5000\n",
      "Epoch: 149, Train Acc: 0.6316, Test Acc: 0.4600\n",
      "Epoch: 150, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 151, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 152, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 153, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 154, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 155, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 156, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 157, Train Acc: 0.6491, Test Acc: 0.5000\n",
      "Epoch: 158, Train Acc: 0.5877, Test Acc: 0.4400\n",
      "Epoch: 159, Train Acc: 0.6316, Test Acc: 0.4600\n",
      "Epoch: 160, Train Acc: 0.6228, Test Acc: 0.4600\n",
      "Epoch: 161, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 162, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 163, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 164, Train Acc: 0.5702, Test Acc: 0.5000\n",
      "Epoch: 165, Train Acc: 0.5439, Test Acc: 0.4600\n",
      "Epoch: 166, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 167, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 168, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 169, Train Acc: 0.6491, Test Acc: 0.5200\n",
      "Epoch: 170, Train Acc: 0.6667, Test Acc: 0.5400\n",
      "Epoch: 171, Train Acc: 0.6579, Test Acc: 0.4600\n",
      "Epoch: 172, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 173, Train Acc: 0.6404, Test Acc: 0.4800\n",
      "Epoch: 174, Train Acc: 0.6491, Test Acc: 0.5000\n",
      "Epoch: 175, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 176, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 177, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 178, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 179, Train Acc: 0.6667, Test Acc: 0.5000\n",
      "Epoch: 180, Train Acc: 0.6667, Test Acc: 0.5000\n",
      "Epoch: 181, Train Acc: 0.6579, Test Acc: 0.4800\n",
      "Epoch: 182, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 183, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 184, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 185, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 186, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 187, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 188, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 189, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 190, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 191, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 192, Train Acc: 0.6491, Test Acc: 0.5400\n",
      "Epoch: 193, Train Acc: 0.6316, Test Acc: 0.4800\n",
      "Epoch: 194, Train Acc: 0.6491, Test Acc: 0.5400\n",
      "Epoch: 195, Train Acc: 0.6404, Test Acc: 0.5400\n",
      "Epoch: 196, Train Acc: 0.6404, Test Acc: 0.5000\n",
      "Epoch: 197, Train Acc: 0.6404, Test Acc: 0.5000\n",
      "Epoch: 198, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 199, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 200, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Training duration: 256.8498830795288 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gcn = GCN()\n",
    "gcn = gcn.to(device)\n",
    "# print(gcn.parameters())\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.001)\n",
    "# loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "out_labels = []\n",
    "\n",
    "training_running_loss = 0.0\n",
    "def train(train_loader):\n",
    "    \n",
    "    gcn.train()\n",
    "    # print(gcn.parameters())\n",
    "    for batch_data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        for data in batch_data:\n",
    "            data = data.to(device)\n",
    "            #forward pass\n",
    "            out = gcn(data.x, data.edge_index, data.batch)\n",
    "            # calculate the loss\n",
    "            loss = criterion(out, data.y)\n",
    "            # zero the gradients of the weights so that the gradients are not accumulated\n",
    "            optimizer.zero_grad()\n",
    "            # calculate the gradients using backpropagation\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss\n",
    "            training_running_loss += loss.detach().item()\n",
    "            \n",
    "            out_labels.append((out, data.y))\n",
    "        \n",
    "        \n",
    "\n",
    "testing_labels = []\n",
    "def test(loader):\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    for batch_data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        for data in batch_data:\n",
    "            out = gcn(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            testing_labels.append(pred)\n",
    "            y_label = (data.y.tolist())\n",
    "            y_label = y_label[0].index(1.0)\n",
    "            pred_label = (pred.tolist())[0]\n",
    "            # print(pred_label)\n",
    "            # print(y_label)\n",
    "            if y_label == pred_label:\n",
    "                correct += 1            \n",
    "            # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 200\n",
    "# Your training code here\n",
    "for epoch in range(num_epochs):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch + 1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(\"Training duration:\", duration, \"seconds\")\n",
    "# with open(\"out_labels.txt\", \"w\") as output:\n",
    "#         output.write(str(out_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn.state_dict(), 'gcn_model8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\\comparator16.txt\n",
      "tensor([2])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "num = random.randint(0, len(verilog_files))\n",
    "print(verilog_files[num])\n",
    "data_trial = dataset[num]\n",
    "\n",
    "out = gcn(data_trial.x, data_trial.edge_index, data_trial.batch)\n",
    "pred = out.argmax(dim=1)\n",
    "print(pred)\n",
    "print((data_trial.y.tolist())[0].index(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[0.1118, 0.1097, 0.1018, 0.1003, 0.1178, 0.1129, 0.1081, 0.1210, 0.1167]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1059, 0.1114, 0.1021, 0.0979, 0.1187, 0.1096, 0.1097, 0.1217, 0.1228]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1078, 0.1124, 0.1022, 0.1001, 0.1166, 0.1129, 0.1110, 0.1191, 0.1179]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1095, 0.1032, 0.0960, 0.0981, 0.1257, 0.1091, 0.1091, 0.1260, 0.1233]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1096, 0.1091, 0.1006, 0.1017, 0.1147, 0.1134, 0.1094, 0.1264, 0.1150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1065, 0.1128, 0.0992, 0.1019, 0.1142, 0.1175, 0.1083, 0.1225, 0.1171]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1139, 0.1077, 0.0977, 0.0989, 0.1158, 0.1183, 0.1071, 0.1264, 0.1142]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1071, 0.1127, 0.0988, 0.0998, 0.1145, 0.1159, 0.1104, 0.1251, 0.1158]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1081, 0.1133, 0.1001, 0.0981, 0.1155, 0.1141, 0.1137, 0.1236, 0.1135]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1088, 0.1155, 0.0976, 0.0986, 0.1130, 0.1117, 0.1142, 0.1264, 0.1141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1208, 0.1097, 0.0887, 0.0928, 0.1110, 0.1075, 0.1250, 0.1413, 0.1031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1085, 0.1138, 0.0930, 0.0979, 0.1097, 0.1154, 0.1200, 0.1291, 0.1126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1081, 0.1128, 0.0957, 0.0979, 0.1128, 0.1196, 0.1140, 0.1266, 0.1124]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1090, 0.1068, 0.0943, 0.0958, 0.1132, 0.1194, 0.1234, 0.1300, 0.1082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.1046, 0.0898, 0.0919, 0.1132, 0.1343, 0.1389, 0.1294, 0.0975]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1079, 0.1126, 0.0969, 0.1008, 0.1130, 0.1213, 0.1146, 0.1225, 0.1105]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1040, 0.1074, 0.0939, 0.0977, 0.1184, 0.1195, 0.1201, 0.1306, 0.1084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1021, 0.1103, 0.1021, 0.1021, 0.1165, 0.1199, 0.1178, 0.1211, 0.1080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1002, 0.0999, 0.0859, 0.1045, 0.1209, 0.1353, 0.1212, 0.1336, 0.0984]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.1164, 0.0946, 0.0977, 0.1220, 0.1182, 0.1215, 0.1242, 0.1049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1010, 0.1056, 0.0961, 0.0998, 0.1263, 0.1173, 0.1179, 0.1289, 0.1071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1034, 0.1058, 0.0977, 0.1001, 0.1193, 0.1232, 0.1170, 0.1245, 0.1091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1033, 0.1066, 0.0917, 0.0978, 0.1267, 0.1307, 0.1195, 0.1241, 0.0995]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.1101, 0.1022, 0.1006, 0.1219, 0.1193, 0.1169, 0.1203, 0.1091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0970, 0.0955, 0.0993, 0.1000, 0.1293, 0.1240, 0.1209, 0.1316, 0.1024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0919, 0.1066, 0.1006, 0.1029, 0.1235, 0.1216, 0.1224, 0.1208, 0.1096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0944, 0.1025, 0.0988, 0.1020, 0.1265, 0.1249, 0.1308, 0.1184, 0.1018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0939, 0.1065, 0.1001, 0.1006, 0.1239, 0.1316, 0.1233, 0.1226, 0.0977]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1071, 0.1068, 0.0978, 0.1007, 0.1220, 0.1242, 0.1130, 0.1208, 0.1075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1054, 0.1044, 0.1000, 0.1063, 0.1206, 0.1223, 0.1200, 0.1145, 0.1065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0974, 0.1064, 0.1047, 0.1030, 0.1266, 0.1324, 0.1208, 0.1124, 0.0964]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0944, 0.1041, 0.1022, 0.0993, 0.1326, 0.1246, 0.1262, 0.1162, 0.1004]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0994, 0.1075, 0.1058, 0.1002, 0.1237, 0.1211, 0.1149, 0.1195, 0.1079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0977, 0.0997, 0.1061, 0.0960, 0.1364, 0.1253, 0.1237, 0.1167, 0.0984]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0930, 0.1051, 0.1022, 0.0985, 0.1289, 0.1288, 0.1202, 0.1214, 0.1020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0978, 0.1062, 0.1025, 0.0984, 0.1281, 0.1220, 0.1145, 0.1218, 0.1087]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0997, 0.1020, 0.1038, 0.1034, 0.1270, 0.1244, 0.1114, 0.1260, 0.1022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0759, 0.0706, 0.1188, 0.1135, 0.1482, 0.1511, 0.1388, 0.1186, 0.0643]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0805, 0.0973, 0.1074, 0.0960, 0.1498, 0.1348, 0.1199, 0.1220, 0.0925]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0894, 0.0933, 0.1086, 0.1078, 0.1390, 0.1399, 0.1188, 0.1172, 0.0860]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0858, 0.1033, 0.1062, 0.1049, 0.1399, 0.1306, 0.1281, 0.1101, 0.0910]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0884, 0.0970, 0.1113, 0.1019, 0.1403, 0.1238, 0.1221, 0.1135, 0.1018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0979, 0.1051, 0.1036, 0.1089, 0.1342, 0.1239, 0.1208, 0.1134, 0.0922]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.1024, 0.1066, 0.0980, 0.1387, 0.1230, 0.1176, 0.1191, 0.1021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0866, 0.1092, 0.1093, 0.1527, 0.1318, 0.1412, 0.1135, 0.0829]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0647, 0.0729, 0.1220, 0.1259, 0.1680, 0.1497, 0.1247, 0.1015, 0.0706]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0875, 0.0965, 0.1112, 0.1158, 0.1427, 0.1351, 0.1272, 0.1067, 0.0772]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0951, 0.1059, 0.1139, 0.1059, 0.1449, 0.1223, 0.1218, 0.1043, 0.0860]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0815, 0.0841, 0.1227, 0.1163, 0.1655, 0.1343, 0.1398, 0.0931, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0928, 0.0950, 0.1144, 0.1094, 0.1429, 0.1284, 0.1221, 0.1065, 0.0884]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0934, 0.1097, 0.1119, 0.1585, 0.1274, 0.1259, 0.1127, 0.0774]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0792, 0.0763, 0.1274, 0.1077, 0.1611, 0.1331, 0.1588, 0.0936, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0915, 0.0899, 0.1135, 0.1170, 0.1493, 0.1277, 0.1354, 0.1039, 0.0718]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0563, 0.0535, 0.1198, 0.1389, 0.2035, 0.1588, 0.1424, 0.0878, 0.0389]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0815, 0.0586, 0.1195, 0.1449, 0.1896, 0.1302, 0.1423, 0.0831, 0.0502]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0813, 0.0809, 0.1272, 0.1156, 0.1574, 0.1325, 0.1383, 0.0939, 0.0729]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0705, 0.0663, 0.1214, 0.1423, 0.1719, 0.1237, 0.1695, 0.0827, 0.0517]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0589, 0.0549, 0.1517, 0.1126, 0.2043, 0.1377, 0.1818, 0.0606, 0.0376]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0811, 0.0663, 0.1082, 0.1254, 0.1625, 0.1573, 0.1661, 0.0928, 0.0404]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0783, 0.0715, 0.1137, 0.1247, 0.1709, 0.1421, 0.1617, 0.0913, 0.0457]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0697, 0.0553, 0.1361, 0.1254, 0.1759, 0.1472, 0.1726, 0.0755, 0.0423]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0853, 0.0426, 0.1335, 0.1308, 0.1678, 0.1615, 0.1717, 0.0825, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0519, 0.1104, 0.1750, 0.1683, 0.1925, 0.1534, 0.0631, 0.0240]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1032, 0.0836, 0.1148, 0.1244, 0.1465, 0.1387, 0.1373, 0.0882, 0.0632]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0904, 0.0638, 0.1120, 0.1380, 0.1465, 0.1600, 0.1571, 0.0868, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0745, 0.0717, 0.1204, 0.1380, 0.1804, 0.1574, 0.1557, 0.0687, 0.0331]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1000, 0.0819, 0.1204, 0.1079, 0.1708, 0.1505, 0.1268, 0.0843, 0.0574]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0896, 0.0675, 0.0923, 0.1379, 0.1691, 0.1610, 0.1472, 0.0805, 0.0549]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0941, 0.0767, 0.1136, 0.1291, 0.1560, 0.1427, 0.1536, 0.0851, 0.0491]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1035, 0.0598, 0.1109, 0.1437, 0.1532, 0.1655, 0.1315, 0.0876, 0.0443]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1138, 0.0710, 0.1010, 0.1349, 0.1380, 0.1734, 0.1204, 0.0932, 0.0543]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0723, 0.0677, 0.1047, 0.1543, 0.1820, 0.1725, 0.1473, 0.0662, 0.0329]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0920, 0.0721, 0.1033, 0.1391, 0.1913, 0.1488, 0.1276, 0.0799, 0.0460]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0911, 0.0236, 0.0775, 0.1725, 0.2374, 0.1754, 0.1570, 0.0548, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0948, 0.0616, 0.1025, 0.1620, 0.1551, 0.1587, 0.1203, 0.0937, 0.0513]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1131, 0.0589, 0.0982, 0.1332, 0.1599, 0.1827, 0.1146, 0.0979, 0.0415]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1069, 0.0462, 0.0937, 0.1657, 0.1639, 0.2076, 0.1072, 0.0775, 0.0313]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0497, 0.1074, 0.1371, 0.1863, 0.1817, 0.1217, 0.0840, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0682, 0.0411, 0.1219, 0.1727, 0.2034, 0.1782, 0.1250, 0.0639, 0.0257]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0885, 0.0495, 0.0845, 0.1705, 0.1931, 0.1843, 0.1153, 0.0839, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1038, 0.0746, 0.1087, 0.1283, 0.1511, 0.1504, 0.1203, 0.0989, 0.0639]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0852, 0.0559, 0.0919, 0.1503, 0.1910, 0.1766, 0.1086, 0.0968, 0.0437]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1053, 0.0359, 0.0689, 0.2009, 0.2021, 0.2345, 0.0873, 0.0494, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0795, 0.0441, 0.0872, 0.1911, 0.1805, 0.1851, 0.1112, 0.0889, 0.0324]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0901, 0.0695, 0.0852, 0.1489, 0.1709, 0.1846, 0.0929, 0.1001, 0.0578]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0856, 0.0244, 0.0965, 0.1702, 0.2351, 0.2120, 0.0942, 0.0644, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0761, 0.0217, 0.0599, 0.3122, 0.2130, 0.1706, 0.0799, 0.0586, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0545, 0.0231, 0.0607, 0.2487, 0.2486, 0.2047, 0.0734, 0.0669, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0612, 0.0321, 0.0870, 0.1591, 0.2246, 0.2194, 0.0769, 0.1135, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0670, 0.0212, 0.0717, 0.2029, 0.2131, 0.2993, 0.0538, 0.0570, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0848, 0.0477, 0.0873, 0.1673, 0.1955, 0.1916, 0.0871, 0.0989, 0.0398]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0552, 0.0231, 0.0744, 0.1699, 0.3081, 0.2381, 0.0571, 0.0576, 0.0164]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0667, 0.0203, 0.0638, 0.2149, 0.2519, 0.2103, 0.0959, 0.0605, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0118, 0.0008, 0.0140, 0.2004, 0.5902, 0.1593, 0.0165, 0.0063, 0.0007]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0784, 0.0326, 0.0771, 0.1881, 0.2077, 0.2432, 0.0651, 0.0824, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0925, 0.0382, 0.0890, 0.1561, 0.1929, 0.1952, 0.1025, 0.0890, 0.0445]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0711, 0.0280, 0.0647, 0.1724, 0.2174, 0.2565, 0.0845, 0.0768, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0726, 0.0411, 0.0830, 0.1665, 0.2173, 0.2369, 0.0792, 0.0775, 0.0258]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0625, 0.0222, 0.0515, 0.1902, 0.2606, 0.2716, 0.0626, 0.0612, 0.0177]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0996, 0.0570, 0.0846, 0.1522, 0.1587, 0.1976, 0.0883, 0.1083, 0.0536]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0592, 0.0163, 0.0654, 0.2129, 0.2247, 0.2653, 0.0615, 0.0796, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0157, 0.0510, 0.2885, 0.1464, 0.3217, 0.0478, 0.0639, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0633, 0.0196, 0.0512, 0.2280, 0.1892, 0.3098, 0.0608, 0.0628, 0.0154]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0618, 0.0345, 0.0518, 0.2087, 0.2015, 0.2658, 0.0664, 0.0833, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0723, 0.0394, 0.0723, 0.1980, 0.1643, 0.2491, 0.0818, 0.0826, 0.0402]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0521, 0.0138, 0.0384, 0.2750, 0.1628, 0.3084, 0.0752, 0.0610, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0225, 0.0484, 0.2307, 0.1562, 0.3211, 0.0638, 0.0738, 0.0234]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0773, 0.0448, 0.0561, 0.2191, 0.1630, 0.2269, 0.0956, 0.0860, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0772, 0.0329, 0.0621, 0.2126, 0.1566, 0.2672, 0.0750, 0.0887, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0144, 0.0476, 0.2886, 0.1676, 0.3017, 0.0686, 0.0598, 0.0131]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0658, 0.0307, 0.0504, 0.2885, 0.1422, 0.2786, 0.0624, 0.0619, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0740, 0.0269, 0.0589, 0.2084, 0.1286, 0.2891, 0.0651, 0.1186, 0.0304]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0413, 0.0104, 0.0237, 0.2941, 0.0720, 0.4639, 0.0421, 0.0444, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0376, 0.0122, 0.0444, 0.3157, 0.1440, 0.3306, 0.0552, 0.0511, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0567, 0.0206, 0.0489, 0.2647, 0.1004, 0.3248, 0.0866, 0.0805, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0167, 0.0035, 0.0113, 0.3528, 0.0509, 0.4943, 0.0338, 0.0349, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0424, 0.0180, 0.0320, 0.3218, 0.0776, 0.3445, 0.0602, 0.0920, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0423, 0.0183, 0.0389, 0.2447, 0.0965, 0.4033, 0.0743, 0.0709, 0.0107]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0227, 0.0031, 0.0160, 0.4263, 0.0451, 0.3910, 0.0464, 0.0463, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0633, 0.0197, 0.0352, 0.2251, 0.0817, 0.4386, 0.0404, 0.0780, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.2967e-03, 3.0514e-04, 2.0014e-03, 1.5553e-01, 7.2512e-03, 8.1690e-01,\n",
       "           9.0341e-03, 6.6130e-03, 6.8204e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0160, 0.0048, 0.0108, 0.2517, 0.0270, 0.6313, 0.0183, 0.0374, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0218, 0.0034, 0.0113, 0.2594, 0.0311, 0.6152, 0.0244, 0.0303, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0123, 0.0342, 0.2747, 0.0854, 0.4173, 0.0630, 0.0620, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0105, 0.0009, 0.0044, 0.3423, 0.0098, 0.5994, 0.0120, 0.0200, 0.0006]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0441, 0.0102, 0.0225, 0.2536, 0.0616, 0.4867, 0.0705, 0.0429, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0168, 0.0254, 0.2354, 0.0877, 0.4852, 0.0450, 0.0477, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0320, 0.0451, 0.2063, 0.0737, 0.3835, 0.0722, 0.0753, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0107, 0.0011, 0.0042, 0.1618, 0.0138, 0.7829, 0.0092, 0.0152, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0242, 0.0096, 0.0235, 0.1577, 0.0781, 0.6078, 0.0467, 0.0437, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0210, 0.0051, 0.0193, 0.0960, 0.0370, 0.7492, 0.0304, 0.0382, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0413, 0.0133, 0.0331, 0.1980, 0.0813, 0.5237, 0.0415, 0.0568, 0.0111]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0428, 0.0267, 0.0480, 0.1580, 0.1125, 0.4335, 0.0817, 0.0763, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0555, 0.0151, 0.0317, 0.1643, 0.0679, 0.5386, 0.0625, 0.0497, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0107, 0.0018, 0.0122, 0.1424, 0.0372, 0.7502, 0.0222, 0.0223, 0.0010]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0197, 0.0044, 0.0104, 0.1213, 0.0348, 0.7547, 0.0367, 0.0160, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0420, 0.0072, 0.0246, 0.1530, 0.0494, 0.6289, 0.0403, 0.0496, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0195, 0.0047, 0.0170, 0.1810, 0.0489, 0.6487, 0.0382, 0.0388, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0715, 0.0339, 0.0470, 0.1921, 0.0884, 0.3864, 0.0743, 0.0822, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0259, 0.0101, 0.0224, 0.1371, 0.0530, 0.6529, 0.0502, 0.0430, 0.0054]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0673, 0.0236, 0.0417, 0.1817, 0.0845, 0.4283, 0.0647, 0.0840, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0288, 0.0098, 0.0206, 0.1397, 0.0657, 0.6240, 0.0501, 0.0550, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0168, 0.0033, 0.0104, 0.0998, 0.0304, 0.7608, 0.0308, 0.0446, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0494, 0.0166, 0.0277, 0.1261, 0.0675, 0.6007, 0.0445, 0.0546, 0.0129]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0538, 0.0188, 0.0332, 0.1528, 0.0581, 0.5535, 0.0438, 0.0731, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0126, 0.0072, 0.0149, 0.1236, 0.0663, 0.6905, 0.0314, 0.0505, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0772, 0.0320, 0.0585, 0.1501, 0.1274, 0.3228, 0.0883, 0.1123, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0074, 0.0192, 0.1550, 0.0760, 0.6231, 0.0337, 0.0557, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0584, 0.0169, 0.0469, 0.1486, 0.0997, 0.4695, 0.0648, 0.0787, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0343, 0.0166, 0.0389, 0.1245, 0.0596, 0.5945, 0.0518, 0.0719, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0442, 0.0201, 0.0515, 0.1662, 0.1167, 0.4086, 0.0670, 0.1088, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0284, 0.0092, 0.0192, 0.1370, 0.0848, 0.6219, 0.0372, 0.0553, 0.0069]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0060e-03, 4.8164e-05, 8.4278e-04, 3.1442e-02, 7.7538e-03, 9.4764e-01,\n",
       "           2.2061e-03, 9.0409e-03, 2.1262e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0292, 0.0057, 0.0174, 0.1454, 0.0467, 0.6747, 0.0343, 0.0433, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0292, 0.0074, 0.0287, 0.1566, 0.0963, 0.5007, 0.0693, 0.1013, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.8636e-04, 1.3970e-05, 2.9851e-04, 2.2131e-02, 5.7988e-03, 9.6613e-01,\n",
       "           1.6749e-03, 3.5520e-03, 1.3233e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0585, 0.0106, 0.0396, 0.2032, 0.0987, 0.4234, 0.0688, 0.0832, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0557, 0.0348, 0.0509, 0.1239, 0.1347, 0.3851, 0.0909, 0.0996, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0310, 0.0089, 0.0298, 0.1151, 0.0745, 0.5905, 0.0539, 0.0883, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0495, 0.0136, 0.0358, 0.1523, 0.0870, 0.5029, 0.0676, 0.0800, 0.0114]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0515, 0.0223, 0.0399, 0.1866, 0.1234, 0.3783, 0.0640, 0.1147, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0326, 0.0062, 0.0224, 0.1481, 0.0588, 0.6024, 0.0428, 0.0809, 0.0059]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0081, 0.0215, 0.1295, 0.0799, 0.5898, 0.0733, 0.0671, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.3440e-03, 3.1362e-04, 2.3222e-03, 4.9673e-02, 2.1704e-02, 9.0007e-01,\n",
       "           1.4016e-02, 9.3979e-03, 1.5432e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0295, 0.0074, 0.0268, 0.1803, 0.0807, 0.5572, 0.0526, 0.0592, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0349, 0.0148, 0.0383, 0.1368, 0.1252, 0.4874, 0.0633, 0.0867, 0.0127]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0164, 0.0012, 0.0075, 0.1054, 0.0504, 0.6958, 0.0151, 0.1064, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0215, 0.0070, 0.0232, 0.1430, 0.0970, 0.6010, 0.0468, 0.0557, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0252, 0.0138, 0.0354, 0.1752, 0.1157, 0.4686, 0.0717, 0.0833, 0.0112]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0256, 0.0094, 0.0228, 0.1536, 0.0922, 0.5211, 0.0784, 0.0859, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0114, 0.0046, 0.0171, 0.1237, 0.0899, 0.6408, 0.0432, 0.0641, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0665, 0.0367, 0.0558, 0.1542, 0.0992, 0.3254, 0.0823, 0.1403, 0.0397]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0083, 0.0021, 0.0080, 0.1071, 0.0438, 0.7587, 0.0298, 0.0407, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0395, 0.0159, 0.0461, 0.1422, 0.1251, 0.4356, 0.0660, 0.1136, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0246, 0.0059, 0.0250, 0.0913, 0.0848, 0.6198, 0.0596, 0.0843, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0528, 0.0212, 0.0483, 0.1493, 0.1153, 0.3800, 0.0746, 0.1331, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0244, 0.0054, 0.0199, 0.1647, 0.0671, 0.6103, 0.0545, 0.0493, 0.0044]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0622, 0.0226, 0.0542, 0.1412, 0.1273, 0.3620, 0.0961, 0.1132, 0.0212]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.8658e-03, 6.2723e-04, 3.5370e-03, 9.5234e-02, 3.5559e-02, 8.3107e-01,\n",
       "           1.1991e-02, 1.6639e-02, 4.7539e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0176, 0.0030, 0.0097, 0.1679, 0.0614, 0.6604, 0.0213, 0.0564, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0219, 0.0037, 0.0203, 0.1361, 0.0520, 0.6250, 0.0318, 0.1050, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0197, 0.0033, 0.0191, 0.1333, 0.0829, 0.6528, 0.0316, 0.0531, 0.0042]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0469, 0.0105, 0.0333, 0.1662, 0.0899, 0.4705, 0.0736, 0.1010, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0140, 0.0021, 0.0155, 0.1017, 0.0554, 0.7225, 0.0248, 0.0626, 0.0016]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0150, 0.0022, 0.0115, 0.1441, 0.0762, 0.6216, 0.0376, 0.0897, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0363, 0.0088, 0.0223, 0.1311, 0.1092, 0.5040, 0.0627, 0.1125, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0465, 0.0184, 0.0443, 0.1663, 0.1383, 0.3702, 0.0779, 0.1204, 0.0177]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0198, 0.0035, 0.0188, 0.1095, 0.1197, 0.6210, 0.0520, 0.0519, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0275, 0.0026, 0.0152, 0.1316, 0.0583, 0.6620, 0.0383, 0.0619, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0424, 0.0118, 0.0372, 0.1449, 0.1157, 0.4818, 0.0536, 0.1027, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0551, 0.0252, 0.0532, 0.1472, 0.1241, 0.3636, 0.0815, 0.1267, 0.0233]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0511, 0.0178, 0.0482, 0.1235, 0.1455, 0.3787, 0.0712, 0.1371, 0.0268]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0335, 0.0067, 0.0197, 0.1409, 0.1470, 0.4866, 0.0495, 0.1062, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0636, 0.0287, 0.0618, 0.1507, 0.1459, 0.2998, 0.0880, 0.1295, 0.0320]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0170, 0.0022, 0.0111, 0.1742, 0.0790, 0.5901, 0.0387, 0.0858, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0124, 0.0018, 0.0163, 0.1550, 0.1507, 0.5502, 0.0426, 0.0688, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0055, 0.0217, 0.1537, 0.1099, 0.5226, 0.0548, 0.0968, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0456, 0.0119, 0.0347, 0.1358, 0.0990, 0.5300, 0.0655, 0.0685, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0367, 0.0082, 0.0244, 0.1455, 0.1383, 0.4489, 0.0522, 0.1375, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0348, 0.0111, 0.0306, 0.1464, 0.1314, 0.4605, 0.0559, 0.1187, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0597, 0.0170, 0.0427, 0.1495, 0.1268, 0.3506, 0.0848, 0.1519, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0480, 0.0198, 0.0360, 0.1729, 0.1620, 0.3070, 0.1043, 0.1299, 0.0200]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0328, 0.0186, 0.0388, 0.1644, 0.1581, 0.3993, 0.0652, 0.1079, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0194, 0.0013, 0.0097, 0.1074, 0.0689, 0.6829, 0.0350, 0.0735, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0287, 0.0056, 0.0344, 0.1447, 0.1068, 0.4834, 0.0751, 0.1156, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0539, 0.0133, 0.0376, 0.1903, 0.1276, 0.3420, 0.0782, 0.1395, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0709, 0.0380, 0.0698, 0.1660, 0.1431, 0.2367, 0.1171, 0.1203, 0.0382]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0786, 0.0480, 0.0693, 0.1329, 0.1525, 0.2305, 0.1060, 0.1247, 0.0574]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1453e-04, 3.5857e-06, 1.4002e-04, 2.9182e-02, 1.0359e-02, 9.5206e-01,\n",
       "           1.7240e-03, 6.2075e-03, 4.9101e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0522, 0.0157, 0.0428, 0.2002, 0.1234, 0.3605, 0.0936, 0.0924, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0642, 0.0127, 0.0373, 0.1861, 0.1541, 0.3089, 0.0868, 0.1338, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0422, 0.0078, 0.0226, 0.1987, 0.1281, 0.4440, 0.0717, 0.0790, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0454, 0.0665, 0.1398, 0.1503, 0.2213, 0.1040, 0.1320, 0.0535]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0452, 0.0169, 0.0530, 0.1954, 0.1850, 0.2625, 0.1010, 0.1187, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0648, 0.0239, 0.0601, 0.1556, 0.1555, 0.2735, 0.1081, 0.1325, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0184, 0.0016, 0.0078, 0.2803, 0.1084, 0.4645, 0.0512, 0.0663, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0307, 0.0084, 0.0315, 0.2177, 0.1370, 0.3637, 0.0933, 0.1112, 0.0065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0484, 0.0202, 0.0409, 0.2243, 0.1178, 0.3321, 0.0908, 0.1061, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0749, 0.0443, 0.0776, 0.1501, 0.1372, 0.2306, 0.1131, 0.1237, 0.0485]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0449, 0.0150, 0.0365, 0.2476, 0.1387, 0.3220, 0.0778, 0.1009, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0594, 0.0181, 0.0502, 0.2369, 0.1422, 0.2327, 0.1003, 0.1389, 0.0214]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8595e-03, 2.9371e-04, 3.2481e-03, 4.9506e-01, 7.7968e-02, 3.8345e-01,\n",
       "           1.4730e-02, 2.3208e-02, 1.8038e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0186, 0.0454, 0.2229, 0.1327, 0.2922, 0.1099, 0.1021, 0.0161]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0331, 0.0019, 0.0139, 0.3394, 0.0541, 0.4479, 0.0442, 0.0636, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0696, 0.0310, 0.0660, 0.1993, 0.1384, 0.2412, 0.1159, 0.1047, 0.0340]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0268, 0.0033, 0.0176, 0.2968, 0.0719, 0.4533, 0.0566, 0.0708, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0868, 0.0319, 0.0816, 0.1807, 0.1281, 0.2245, 0.1037, 0.1210, 0.0418]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0319, 0.0671, 0.1893, 0.1471, 0.2572, 0.1081, 0.1043, 0.0299]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0473, 0.0076, 0.0395, 0.2757, 0.0817, 0.3803, 0.0890, 0.0714, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0609, 0.0134, 0.0432, 0.2169, 0.1124, 0.3579, 0.0850, 0.0923, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0824, 0.0327, 0.0672, 0.1671, 0.1294, 0.2695, 0.1142, 0.1042, 0.0334]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0761, 0.0481, 0.0783, 0.1547, 0.1374, 0.2297, 0.1329, 0.0999, 0.0429]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0231, 0.0655, 0.1663, 0.1173, 0.3025, 0.1101, 0.1151, 0.0268]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.8159e-03, 6.0923e-04, 8.4433e-03, 1.9427e-01, 4.0449e-02, 6.8429e-01,\n",
       "           4.1066e-02, 2.1164e-02, 8.9709e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0433, 0.0089, 0.0438, 0.2315, 0.0828, 0.4048, 0.0983, 0.0774, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0800, 0.0199, 0.0583, 0.2072, 0.1138, 0.3215, 0.1040, 0.0750, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0461, 0.0119, 0.0484, 0.2744, 0.1351, 0.3063, 0.0940, 0.0714, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0819, 0.0305, 0.0624, 0.2216, 0.1206, 0.2381, 0.1098, 0.0977, 0.0374]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0342, 0.0040, 0.0319, 0.2765, 0.0608, 0.4518, 0.0702, 0.0668, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0768, 0.0491, 0.0870, 0.1850, 0.1341, 0.2134, 0.1164, 0.0955, 0.0428]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.6946e-03, 3.6741e-05, 3.2339e-03, 3.5913e-01, 2.7354e-02, 5.4899e-01,\n",
       "           4.6323e-02, 1.1186e-02, 4.7200e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0487, 0.0136, 0.0673, 0.2761, 0.1132, 0.2925, 0.0964, 0.0783, 0.0139]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0451, 0.0122, 0.0546, 0.3136, 0.1221, 0.2881, 0.0880, 0.0660, 0.0103]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0710, 0.0205, 0.0566, 0.2243, 0.1320, 0.2992, 0.0890, 0.0832, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0141, 0.0008, 0.0094, 0.3070, 0.0438, 0.5808, 0.0197, 0.0227, 0.0017]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0432, 0.0132, 0.0467, 0.2279, 0.1138, 0.3841, 0.0817, 0.0746, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0402, 0.0096, 0.0451, 0.2318, 0.0735, 0.4340, 0.0905, 0.0643, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0186, 0.0025, 0.0135, 0.2251, 0.0594, 0.6136, 0.0337, 0.0308, 0.0028]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0593, 0.0283, 0.0546, 0.1926, 0.1407, 0.3131, 0.0909, 0.0883, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0506, 0.0145, 0.0570, 0.2065, 0.1396, 0.3212, 0.1200, 0.0748, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0023, 0.0212, 0.2556, 0.0576, 0.5332, 0.0522, 0.0368, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0143, 0.0427, 0.1893, 0.0848, 0.4213, 0.0808, 0.0920, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0554, 0.0174, 0.0620, 0.1878, 0.1382, 0.3451, 0.0982, 0.0784, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0136, 0.0500, 0.1865, 0.1194, 0.3949, 0.0948, 0.0776, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0975, 0.0496, 0.0829, 0.1439, 0.1242, 0.2422, 0.1104, 0.1035, 0.0458]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0830, 0.0409, 0.0779, 0.1494, 0.1242, 0.2694, 0.1024, 0.1057, 0.0470]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0277, 0.0059, 0.0556, 0.1602, 0.1238, 0.4471, 0.0946, 0.0782, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2805e-03, 4.9053e-05, 1.5539e-03, 3.5752e-02, 6.6068e-03, 9.4597e-01,\n",
       "           5.9896e-03, 2.7571e-03, 4.1143e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0423, 0.0145, 0.0526, 0.1376, 0.1227, 0.4660, 0.0754, 0.0700, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0446, 0.0082, 0.0374, 0.1851, 0.1199, 0.4737, 0.0659, 0.0592, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.5401e-05, 1.6089e-08, 1.3805e-05, 3.7343e-03, 2.3489e-04, 9.9582e-01,\n",
       "           5.4741e-05, 1.1343e-04, 3.3377e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0210, 0.0032, 0.0147, 0.1537, 0.0501, 0.6810, 0.0256, 0.0475, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0156, 0.0019, 0.0111, 0.0893, 0.0431, 0.7749, 0.0261, 0.0356, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0562, 0.0268, 0.0656, 0.1456, 0.1446, 0.3571, 0.0876, 0.0858, 0.0309]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0100, 0.0345, 0.1661, 0.1036, 0.5010, 0.0591, 0.0544, 0.0112]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0090, 0.0012, 0.0051, 0.1076, 0.0189, 0.8299, 0.0101, 0.0170, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0194, 0.0026, 0.0179, 0.1235, 0.0532, 0.6945, 0.0415, 0.0444, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0177, 0.0015, 0.0104, 0.1403, 0.0381, 0.7257, 0.0307, 0.0331, 0.0025]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6210e-03, 1.5184e-05, 5.4552e-04, 3.3802e-02, 2.6953e-03, 9.5394e-01,\n",
       "           1.9098e-03, 4.4471e-03, 2.6128e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0175, 0.0008, 0.0059, 0.1360, 0.0221, 0.7778, 0.0181, 0.0208, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0507, 0.0149, 0.0498, 0.1736, 0.1394, 0.4221, 0.0671, 0.0643, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0197, 0.0016, 0.0145, 0.0822, 0.0375, 0.7911, 0.0210, 0.0301, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0077, 0.0013, 0.0109, 0.1134, 0.0667, 0.7431, 0.0247, 0.0311, 0.0012]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.3927e-03, 1.6984e-04, 1.2831e-03, 3.9222e-02, 8.3252e-03, 9.3051e-01,\n",
       "           4.3729e-03, 1.0525e-02, 1.9836e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0190, 0.0011, 0.0119, 0.1156, 0.0352, 0.7718, 0.0158, 0.0283, 0.0014]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0430, 0.0139, 0.0354, 0.1266, 0.0713, 0.5775, 0.0608, 0.0574, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0184, 0.0029, 0.0136, 0.0632, 0.0542, 0.8005, 0.0204, 0.0243, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[9.5568e-03, 8.3943e-04, 5.7512e-03, 9.8759e-02, 2.9747e-02, 8.2081e-01,\n",
       "           1.2525e-02, 2.1329e-02, 6.8718e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0504e-02, 5.4642e-04, 6.0157e-03, 6.3735e-02, 3.3636e-02, 8.4922e-01,\n",
       "           1.7857e-02, 1.7855e-02, 6.3194e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.3072e-02, 6.4665e-04, 6.2002e-03, 6.9584e-02, 2.1103e-02, 8.6498e-01,\n",
       "           1.1038e-02, 1.2744e-02, 6.3274e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0313, 0.0060, 0.0381, 0.1064, 0.0715, 0.6431, 0.0416, 0.0547, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0211, 0.0018, 0.0109, 0.1260, 0.0304, 0.7684, 0.0169, 0.0226, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0191, 0.0037, 0.0148, 0.1202, 0.0791, 0.6488, 0.0467, 0.0620, 0.0056]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[5.0436e-03, 1.5003e-04, 2.6697e-03, 3.4019e-02, 2.0853e-02, 9.2157e-01,\n",
       "           6.9617e-03, 8.5433e-03, 1.8611e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0902e-03, 3.4768e-05, 1.0194e-03, 3.4278e-02, 1.3362e-02, 9.4171e-01,\n",
       "           3.7113e-03, 4.7631e-03, 3.0669e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.1986e-05, 4.3504e-08, 2.3622e-05, 3.4064e-03, 7.6640e-04, 9.9550e-01,\n",
       "           1.7253e-04, 1.1876e-04, 1.3296e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.4232e-03, 3.8399e-04, 3.3108e-03, 6.1325e-02, 1.5853e-02, 8.9078e-01,\n",
       "           9.5876e-03, 1.1919e-02, 4.1320e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.6553e-03, 6.6659e-04, 9.6380e-03, 6.1867e-02, 3.3770e-02, 8.4947e-01,\n",
       "           2.0845e-02, 1.8174e-02, 9.1897e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[4.9640e-03, 4.3088e-04, 1.0027e-02, 6.3524e-02, 5.1438e-02, 8.2166e-01,\n",
       "           2.4959e-02, 2.2176e-02, 8.2189e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0098, 0.0025, 0.0105, 0.0913, 0.0529, 0.7947, 0.0188, 0.0177, 0.0016]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0071, 0.0288, 0.1358, 0.0795, 0.6047, 0.0402, 0.0592, 0.0075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6003e-03, 8.0909e-05, 2.4186e-03, 3.1774e-02, 1.8325e-02, 9.3202e-01,\n",
       "           6.1285e-03, 6.5577e-03, 9.1599e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.7956e-03, 7.0113e-05, 9.3268e-04, 3.4926e-02, 7.8277e-03, 9.4695e-01,\n",
       "           2.0287e-03, 5.3526e-03, 1.1869e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0288, 0.0077, 0.0299, 0.1553, 0.0914, 0.5716, 0.0571, 0.0500, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.5164e-03, 1.0616e-04, 1.1056e-03, 3.1736e-02, 1.6240e-02, 9.3937e-01,\n",
       "           4.2070e-03, 4.6406e-03, 7.8487e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.7266e-04, 2.0282e-05, 1.6437e-03, 3.0768e-02, 1.1347e-02, 9.4873e-01,\n",
       "           4.0269e-03, 2.5603e-03, 3.2302e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1991e-03, 1.1119e-04, 3.1791e-03, 6.0622e-02, 2.5335e-02, 8.8993e-01,\n",
       "           8.4732e-03, 8.8673e-03, 2.7930e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0181, 0.0022, 0.0175, 0.1138, 0.0664, 0.7212, 0.0310, 0.0266, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.8840e-03, 4.0429e-05, 1.2859e-03, 7.3859e-02, 7.9795e-03, 9.0702e-01,\n",
       "           3.5822e-03, 4.3059e-03, 4.0768e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0186, 0.0026, 0.0305, 0.1179, 0.0889, 0.6776, 0.0280, 0.0323, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.8308e-05, 1.3776e-07, 2.4991e-05, 7.2382e-03, 1.3879e-03, 9.9067e-01,\n",
       "           1.4896e-04, 4.4366e-04, 8.7022e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0183, 0.0009, 0.0122, 0.0770, 0.0421, 0.7833, 0.0259, 0.0382, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6675e-03, 8.4220e-05, 7.7132e-04, 3.1897e-02, 1.0839e-02, 9.4757e-01,\n",
       "           4.0726e-03, 2.0509e-03, 4.4120e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[3.7397e-03, 1.2030e-04, 2.2087e-03, 6.9303e-02, 1.7918e-02, 8.9554e-01,\n",
       "           5.5687e-03, 5.4395e-03, 1.5967e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0222, 0.0071, 0.0197, 0.1035, 0.0820, 0.6874, 0.0399, 0.0335, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0205, 0.0015, 0.0095, 0.1430, 0.0651, 0.7158, 0.0189, 0.0239, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.0100e-04, 8.2423e-07, 7.2047e-05, 8.7077e-03, 5.0910e-03, 9.8521e-01,\n",
       "           4.3402e-04, 3.8308e-04, 4.9143e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0244, 0.0012, 0.0144, 0.1215, 0.0627, 0.7194, 0.0287, 0.0258, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0274, 0.0053, 0.0214, 0.1306, 0.0667, 0.6492, 0.0355, 0.0555, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.6988e-03, 2.6830e-04, 1.8813e-03, 6.3423e-02, 1.7775e-02, 9.0330e-01,\n",
       "           5.1145e-03, 5.2169e-03, 3.1926e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0161, 0.0017, 0.0094, 0.0995, 0.0446, 0.7787, 0.0195, 0.0283, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0265, 0.0019, 0.0141, 0.1449, 0.0770, 0.6670, 0.0312, 0.0332, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.7467e-03, 2.7168e-04, 5.9011e-03, 8.7144e-02, 4.3795e-02, 8.3207e-01,\n",
       "           1.0871e-02, 1.3817e-02, 3.8490e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6484e-03, 8.2820e-05, 4.1505e-03, 1.8410e-01, 4.1590e-02, 7.4223e-01,\n",
       "           1.2327e-02, 1.2688e-02, 1.8509e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0300, 0.0058, 0.0203, 0.1219, 0.0947, 0.6365, 0.0440, 0.0401, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.3343e-03, 8.8411e-05, 1.6207e-03, 5.3760e-02, 2.7186e-02, 9.0170e-01,\n",
       "           5.8585e-03, 5.3147e-03, 1.3518e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0313, 0.0057, 0.0287, 0.1937, 0.1252, 0.5054, 0.0616, 0.0418, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.5503e-02, 7.5912e-04, 7.2919e-03, 8.6872e-02, 4.8623e-02, 7.8929e-01,\n",
       "           1.5896e-02, 3.4348e-02, 1.4177e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0366, 0.0098, 0.0433, 0.1356, 0.1517, 0.4391, 0.0708, 0.0992, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[6.9314e-05, 3.2684e-07, 5.6569e-05, 3.6625e-02, 6.2398e-03, 9.5645e-01,\n",
       "           3.6664e-04, 1.9371e-04, 3.7855e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0435, 0.0065, 0.0347, 0.1206, 0.2306, 0.4406, 0.0508, 0.0612, 0.0116]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0393, 0.0039, 0.0235, 0.1901, 0.1304, 0.4956, 0.0543, 0.0556, 0.0073]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[5.2883e-03, 2.8170e-04, 6.7409e-03, 2.0027e-01, 1.0574e-01, 6.4250e-01,\n",
       "           2.5106e-02, 1.3657e-02, 4.2640e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[8.2839e-03, 2.2496e-04, 4.4602e-03, 1.0420e-01, 4.4727e-02, 8.1230e-01,\n",
       "           1.1257e-02, 1.4117e-02, 4.2058e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0733, 0.0190, 0.0479, 0.1520, 0.1618, 0.3595, 0.0867, 0.0724, 0.0275]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0283, 0.0032, 0.0190, 0.1887, 0.1119, 0.5880, 0.0318, 0.0262, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0420, 0.0091, 0.0236, 0.1686, 0.1173, 0.5100, 0.0648, 0.0557, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0252, 0.0026, 0.0259, 0.1727, 0.1024, 0.5590, 0.0745, 0.0346, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0466, 0.0104, 0.0367, 0.2061, 0.1645, 0.3832, 0.0860, 0.0564, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0443, 0.0093, 0.0478, 0.1579, 0.1276, 0.4470, 0.0900, 0.0664, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0425, 0.0031, 0.0261, 0.2306, 0.0980, 0.4857, 0.0614, 0.0488, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0445, 0.0072, 0.0407, 0.2223, 0.1621, 0.3318, 0.0782, 0.1047, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0771, 0.0274, 0.0657, 0.1491, 0.1533, 0.3012, 0.0947, 0.0953, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0776, 0.0279, 0.0787, 0.1858, 0.1413, 0.2464, 0.1086, 0.1023, 0.0315]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0509, 0.0091, 0.0531, 0.1765, 0.1395, 0.3923, 0.0875, 0.0769, 0.0142]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0114, 0.0006, 0.0097, 0.2349, 0.1075, 0.5592, 0.0485, 0.0271, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0436, 0.0071, 0.0377, 0.2308, 0.1444, 0.3979, 0.0956, 0.0367, 0.0062]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0507, 0.0053, 0.0369, 0.1972, 0.1083, 0.4525, 0.0755, 0.0652, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0919, 0.0408, 0.0713, 0.1508, 0.1539, 0.2216, 0.1091, 0.1176, 0.0429]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0665, 0.0271, 0.0587, 0.1986, 0.1371, 0.3116, 0.0977, 0.0811, 0.0216]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0670, 0.0220, 0.0767, 0.1751, 0.1783, 0.2588, 0.1072, 0.0898, 0.0250]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0604, 0.0115, 0.0582, 0.2140, 0.1417, 0.3156, 0.1188, 0.0697, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0659, 0.0157, 0.0457, 0.2366, 0.1362, 0.2948, 0.1155, 0.0731, 0.0166]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0803, 0.0485, 0.1008, 0.1224, 0.1703, 0.1968, 0.1213, 0.1029, 0.0567]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0890, 0.0396, 0.0845, 0.1618, 0.1473, 0.2190, 0.1115, 0.1018, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0746, 0.0104, 0.0593, 0.1876, 0.1449, 0.3171, 0.0949, 0.0966, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0746, 0.0277, 0.0685, 0.2157, 0.1480, 0.2261, 0.1136, 0.0981, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0226, 0.0603, 0.2214, 0.1589, 0.2228, 0.1217, 0.1015, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0786, 0.0245, 0.0732, 0.1795, 0.1320, 0.2721, 0.1119, 0.0997, 0.0285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0650, 0.0133, 0.0720, 0.1894, 0.1435, 0.2973, 0.1290, 0.0761, 0.0145]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0964, 0.0352, 0.0840, 0.1629, 0.1350, 0.2207, 0.1017, 0.1167, 0.0474]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0764, 0.0229, 0.0644, 0.1655, 0.1450, 0.2854, 0.1061, 0.1056, 0.0287]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0034, 0.0349, 0.2296, 0.1143, 0.4141, 0.0997, 0.0611, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0554, 0.0113, 0.0588, 0.1689, 0.1335, 0.3548, 0.1162, 0.0892, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0244, 0.0027, 0.0348, 0.2348, 0.1532, 0.4080, 0.0973, 0.0419, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0459, 0.0108, 0.0497, 0.2229, 0.1487, 0.3053, 0.1107, 0.0903, 0.0156]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0258, 0.0752, 0.1888, 0.1569, 0.2336, 0.1069, 0.1231, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0107, 0.0558, 0.2186, 0.0932, 0.3460, 0.1013, 0.1006, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0822, 0.0457, 0.0946, 0.1396, 0.1650, 0.1629, 0.1216, 0.1237, 0.0646]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0127, 0.0644, 0.1967, 0.1635, 0.2622, 0.1201, 0.1022, 0.0183]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0129, 0.0653, 0.1751, 0.1582, 0.3020, 0.1173, 0.1042, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0125, 0.0007, 0.0146, 0.1895, 0.1341, 0.5538, 0.0505, 0.0435, 0.0008]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0862, 0.0509, 0.0893, 0.1384, 0.1519, 0.2062, 0.1164, 0.1084, 0.0523]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0739, 0.0286, 0.0728, 0.1880, 0.1669, 0.2312, 0.1230, 0.0900, 0.0255]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0635, 0.0287, 0.0832, 0.1634, 0.1728, 0.2328, 0.1411, 0.0849, 0.0296]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0640, 0.0384, 0.0777, 0.1670, 0.1888, 0.2056, 0.1257, 0.0930, 0.0397]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0439, 0.0150, 0.0640, 0.2379, 0.1636, 0.2340, 0.1582, 0.0686, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0719, 0.0320, 0.0793, 0.1705, 0.1449, 0.2370, 0.1216, 0.1045, 0.0382]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0232, 0.0715, 0.2035, 0.1507, 0.2406, 0.1297, 0.0914, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0450, 0.0152, 0.0763, 0.1846, 0.1853, 0.2156, 0.1445, 0.1162, 0.0173]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0616, 0.0178, 0.0596, 0.1941, 0.1677, 0.2767, 0.1165, 0.0906, 0.0154]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0520, 0.0122, 0.0614, 0.2046, 0.1908, 0.2576, 0.1176, 0.0863, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0430, 0.0081, 0.0690, 0.1884, 0.2250, 0.2692, 0.1072, 0.0812, 0.0088]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0849, 0.0473, 0.0910, 0.1595, 0.1687, 0.1652, 0.1311, 0.1040, 0.0484]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0079, 0.0005, 0.0150, 0.2699, 0.1815, 0.3820, 0.0781, 0.0646, 0.0007]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0849, 0.0500, 0.0824, 0.1680, 0.1675, 0.1863, 0.1162, 0.0990, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0262, 0.0023, 0.0258, 0.3760, 0.1281, 0.3271, 0.0690, 0.0427, 0.0028]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0761, 0.0322, 0.0946, 0.1528, 0.1708, 0.2173, 0.1220, 0.0976, 0.0367]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0690, 0.0447, 0.0864, 0.1760, 0.1789, 0.1810, 0.1362, 0.0904, 0.0373]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0368, 0.0069, 0.0427, 0.1944, 0.1866, 0.3589, 0.1088, 0.0553, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.9578e-03, 8.0168e-04, 1.1752e-02, 4.9535e-01, 1.2197e-01, 2.5994e-01,\n",
       "           7.7337e-02, 2.3489e-02, 3.9841e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6647e-04, 6.2516e-07, 3.0290e-04, 3.7969e-01, 1.4865e-02, 5.8800e-01,\n",
       "           1.5902e-02, 9.6785e-04, 2.4811e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0509, 0.0161, 0.0721, 0.2170, 0.1919, 0.2017, 0.1306, 0.0996, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0322, 0.0033, 0.0349, 0.2617, 0.1657, 0.3582, 0.0905, 0.0493, 0.0042]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0504, 0.0094, 0.0541, 0.1946, 0.1593, 0.3265, 0.1301, 0.0647, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0720, 0.0243, 0.0803, 0.1663, 0.1842, 0.2192, 0.1280, 0.0972, 0.0285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0673, 0.0285, 0.0808, 0.1600, 0.1804, 0.2134, 0.1237, 0.1113, 0.0347]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0440, 0.0136, 0.0595, 0.2061, 0.1603, 0.2939, 0.1353, 0.0759, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0104, 0.0004, 0.0191, 0.3209, 0.1864, 0.3894, 0.0483, 0.0245, 0.0005]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0450, 0.0108, 0.0488, 0.1771, 0.1614, 0.3827, 0.1025, 0.0611, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0698, 0.0285, 0.0755, 0.1608, 0.1428, 0.2701, 0.1251, 0.1002, 0.0272]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0414, 0.0083, 0.0554, 0.2321, 0.1420, 0.3078, 0.1094, 0.0915, 0.0122]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[6.7043e-03, 1.7912e-04, 1.3032e-02, 2.0665e-01, 6.4837e-02, 6.4179e-01,\n",
       "           4.0856e-02, 2.5534e-02, 4.1998e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0308, 0.0728, 0.1573, 0.1576, 0.2390, 0.1377, 0.0946, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0661, 0.0251, 0.0676, 0.1628, 0.1868, 0.2164, 0.1375, 0.1013, 0.0363]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0071, 0.0490, 0.1457, 0.2005, 0.3697, 0.1259, 0.0553, 0.0095]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0447, 0.0156, 0.0608, 0.1592, 0.2322, 0.2672, 0.1305, 0.0744, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0242, 0.0027, 0.0296, 0.1502, 0.1555, 0.5047, 0.0962, 0.0335, 0.0035]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0448, 0.0101, 0.0473, 0.1444, 0.1494, 0.4241, 0.1013, 0.0687, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0708, 0.0265, 0.0662, 0.1461, 0.1560, 0.2827, 0.1279, 0.0977, 0.0259]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0400, 0.0098, 0.0446, 0.1249, 0.1936, 0.3700, 0.1153, 0.0893, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0663, 0.0337, 0.0728, 0.1479, 0.1706, 0.2390, 0.1318, 0.0991, 0.0390]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0328, 0.0031, 0.0281, 0.1410, 0.1440, 0.4715, 0.1179, 0.0568, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0410, 0.0081, 0.0381, 0.1244, 0.1686, 0.4012, 0.1119, 0.0964, 0.0103]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0596, 0.0274, 0.0579, 0.1618, 0.1759, 0.2736, 0.1306, 0.0900, 0.0232]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0513, 0.0202, 0.0674, 0.1373, 0.1914, 0.2853, 0.1213, 0.0998, 0.0260]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0359, 0.0118, 0.0591, 0.1351, 0.1721, 0.3249, 0.1422, 0.1031, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0732, 0.0254, 0.0765, 0.1239, 0.1662, 0.2792, 0.1396, 0.0847, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.4173e-03, 4.7859e-05, 2.2589e-03, 5.8806e-02, 7.6475e-02, 8.2252e-01,\n",
       "           2.8194e-02, 9.2211e-03, 5.8832e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0324, 0.0041, 0.0411, 0.1300, 0.1556, 0.4719, 0.1210, 0.0382, 0.0056]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2923e-03, 1.0087e-05, 1.7392e-03, 3.8844e-02, 4.3297e-02, 8.6686e-01,\n",
       "           4.5495e-02, 2.4534e-03, 1.1604e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0496, 0.0064, 0.0430, 0.1363, 0.1574, 0.4018, 0.1289, 0.0680, 0.0088]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0351, 0.0051, 0.0450, 0.1220, 0.1745, 0.4161, 0.1382, 0.0532, 0.0107]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0689, 0.0354, 0.0763, 0.1468, 0.1620, 0.2199, 0.1570, 0.0973, 0.0364]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0486, 0.0082, 0.0501, 0.1208, 0.1654, 0.4033, 0.1221, 0.0707, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0483, 0.0849, 0.1145, 0.1578, 0.1929, 0.1327, 0.1277, 0.0581]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0571, 0.0115, 0.0431, 0.1587, 0.1381, 0.3788, 0.1235, 0.0774, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0499, 0.0136, 0.0545, 0.1545, 0.1386, 0.3715, 0.1246, 0.0787, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0515, 0.0124, 0.0584, 0.1361, 0.1679, 0.3472, 0.1417, 0.0691, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0760, 0.0335, 0.0785, 0.1366, 0.1624, 0.2361, 0.1395, 0.0960, 0.0413]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0478, 0.0084, 0.0410, 0.1700, 0.1682, 0.3381, 0.1517, 0.0632, 0.0116]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0578, 0.0150, 0.0592, 0.1339, 0.1514, 0.3810, 0.1106, 0.0726, 0.0185]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0579, 0.0163, 0.0547, 0.1367, 0.1766, 0.3329, 0.1201, 0.0844, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.7019e-03, 7.3369e-05, 2.8247e-03, 1.8651e-01, 7.0547e-02, 6.5508e-01,\n",
       "           6.9104e-02, 1.2043e-02, 1.1487e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0510, 0.0100, 0.0543, 0.1647, 0.1641, 0.2987, 0.1536, 0.0856, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0762, 0.0327, 0.0823, 0.1284, 0.1551, 0.2403, 0.1378, 0.1102, 0.0369]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0609, 0.0175, 0.0595, 0.1556, 0.1440, 0.3533, 0.1034, 0.0771, 0.0287]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0162, 0.0019, 0.0167, 0.1163, 0.1641, 0.5525, 0.0859, 0.0434, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0204, 0.0036, 0.0229, 0.1429, 0.1519, 0.5497, 0.0699, 0.0339, 0.0048]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0190, 0.0023, 0.0195, 0.1234, 0.0969, 0.6251, 0.0775, 0.0343, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0758, 0.0372, 0.0837, 0.1402, 0.1459, 0.2551, 0.1106, 0.1078, 0.0438]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.0897e-02, 5.6326e-04, 1.8740e-02, 1.3102e-01, 1.1400e-01, 6.6019e-01,\n",
       "           4.3292e-02, 1.9389e-02, 1.9030e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0595, 0.0117, 0.0670, 0.1201, 0.1552, 0.3589, 0.1298, 0.0810, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0188, 0.0614, 0.1353, 0.1318, 0.3362, 0.1176, 0.1122, 0.0258]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0335, 0.0043, 0.0397, 0.0916, 0.0782, 0.6359, 0.0719, 0.0403, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.3242e-03, 1.9575e-04, 9.5126e-03, 5.2268e-02, 4.6091e-02, 8.5048e-01,\n",
       "           1.9341e-02, 1.4496e-02, 2.9088e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.2930e-03, 6.9166e-04, 1.2036e-02, 1.0643e-01, 6.4548e-02, 7.2875e-01,\n",
       "           5.1762e-02, 2.7681e-02, 8.0644e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0576, 0.0144, 0.0659, 0.1609, 0.1333, 0.3467, 0.1140, 0.0876, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0298, 0.0039, 0.0452, 0.1570, 0.1234, 0.4695, 0.0916, 0.0718, 0.0078]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0548, 0.0161, 0.0566, 0.1510, 0.1307, 0.3665, 0.0949, 0.1125, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0555, 0.0146, 0.0670, 0.1592, 0.1261, 0.3364, 0.1024, 0.1163, 0.0225]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0262, 0.0029, 0.0307, 0.1368, 0.1043, 0.5494, 0.0875, 0.0573, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.7947e-03, 2.3045e-04, 8.5734e-03, 1.2327e-01, 4.1779e-02, 7.8245e-01,\n",
       "           2.3946e-02, 1.4582e-02, 3.7029e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0469, 0.0054, 0.0383, 0.1585, 0.1356, 0.4288, 0.1174, 0.0606, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.8013e-03, 3.5813e-05, 3.0361e-03, 7.1114e-02, 2.2653e-02, 8.6818e-01,\n",
       "           1.9187e-02, 1.1917e-02, 7.0728e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0502, 0.0058, 0.0372, 0.1526, 0.1166, 0.4760, 0.0702, 0.0832, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.4857e-06, 5.3940e-10, 1.1012e-05, 6.9655e-03, 2.8741e-04, 9.9229e-01,\n",
       "           1.4475e-04, 2.9511e-04, 1.7880e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0331, 0.0044, 0.0224, 0.1716, 0.0854, 0.5555, 0.0602, 0.0633, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0202, 0.0008, 0.0222, 0.0996, 0.0595, 0.7217, 0.0463, 0.0283, 0.0013]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0262, 0.0054, 0.0393, 0.1438, 0.1309, 0.5098, 0.0700, 0.0645, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0114, 0.0581, 0.1280, 0.1358, 0.4413, 0.0959, 0.0684, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0254, 0.0023, 0.0425, 0.1189, 0.1306, 0.5462, 0.0756, 0.0531, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0391, 0.0033, 0.0356, 0.1350, 0.1200, 0.5457, 0.0658, 0.0496, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0585, 0.0184, 0.0809, 0.1603, 0.1689, 0.2837, 0.1081, 0.0959, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0329, 0.0040, 0.0431, 0.1216, 0.1137, 0.5498, 0.0666, 0.0612, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0350, 0.0061, 0.0535, 0.1564, 0.1489, 0.4309, 0.0952, 0.0658, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0637, 0.0138, 0.0781, 0.1214, 0.1307, 0.3635, 0.1172, 0.0908, 0.0208]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0802, 0.0396, 0.0897, 0.1306, 0.1677, 0.2224, 0.1176, 0.1074, 0.0448]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0548, 0.0048, 0.0483, 0.1683, 0.1280, 0.4301, 0.0909, 0.0666, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0569, 0.0099, 0.0681, 0.1704, 0.1336, 0.3562, 0.0927, 0.0962, 0.0161]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0287, 0.0022, 0.0250, 0.1376, 0.0695, 0.6450, 0.0526, 0.0367, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2015e-02, 4.5406e-04, 1.5500e-02, 1.4146e-01, 1.1208e-01, 6.5504e-01,\n",
       "           4.2723e-02, 1.9327e-02, 1.4021e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0713, 0.0275, 0.0874, 0.1502, 0.1585, 0.2274, 0.1345, 0.1082, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.2820e-05, 1.1329e-08, 1.0387e-04, 2.2055e-02, 3.5955e-03, 9.7025e-01,\n",
       "           3.8165e-03, 9.6099e-05, 5.3351e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0768, 0.0159, 0.0622, 0.1489, 0.1397, 0.3203, 0.1130, 0.0979, 0.0253]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0366, 0.0820, 0.1404, 0.1557, 0.2358, 0.1113, 0.1049, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0551, 0.0038, 0.0483, 0.1364, 0.1232, 0.4956, 0.0674, 0.0631, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0014, 0.0401, 0.1013, 0.1198, 0.6062, 0.0639, 0.0362, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0722, 0.0131, 0.0534, 0.1789, 0.1357, 0.3513, 0.0994, 0.0796, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0928, 0.0349, 0.0927, 0.1365, 0.1434, 0.2305, 0.1181, 0.1054, 0.0458]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0792, 0.0224, 0.0721, 0.1606, 0.1516, 0.2858, 0.1071, 0.0927, 0.0284]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0908, 0.0308, 0.0795, 0.1626, 0.1487, 0.2455, 0.1035, 0.0975, 0.0412]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0786, 0.0235, 0.0785, 0.1535, 0.1396, 0.2746, 0.1312, 0.0927, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0794, 0.0248, 0.0799, 0.1347, 0.1688, 0.2491, 0.1258, 0.1006, 0.0369]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1011, 0.0585, 0.0964, 0.1261, 0.1353, 0.1884, 0.1143, 0.1154, 0.0644]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0739, 0.0284, 0.0833, 0.1446, 0.1619, 0.2403, 0.1266, 0.1011, 0.0399]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0567, 0.0101, 0.0549, 0.1738, 0.1442, 0.3448, 0.1011, 0.0961, 0.0182]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0718, 0.0182, 0.0735, 0.1780, 0.1539, 0.2903, 0.1069, 0.0835, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0945, 0.0273, 0.0798, 0.1458, 0.1533, 0.2334, 0.1235, 0.1061, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0982, 0.0596, 0.0991, 0.1260, 0.1363, 0.1673, 0.1206, 0.1203, 0.0728]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0355, 0.0962, 0.1342, 0.1663, 0.1942, 0.1288, 0.1067, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0963, 0.0588, 0.0989, 0.1282, 0.1377, 0.1780, 0.1241, 0.1088, 0.0692]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0906, 0.0392, 0.0843, 0.1411, 0.1484, 0.2221, 0.1215, 0.1027, 0.0501]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[2.8827e-02, 5.1956e-04, 1.5076e-02, 1.6954e-01, 9.7590e-02, 6.1624e-01,\n",
       "           4.2468e-02, 2.8756e-02, 9.7868e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0990, 0.0626, 0.0989, 0.1215, 0.1382, 0.1681, 0.1203, 0.1199, 0.0716]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0973, 0.0422, 0.0887, 0.1193, 0.1558, 0.2064, 0.1202, 0.1118, 0.0582]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1056, 0.0656, 0.1009, 0.1204, 0.1327, 0.1553, 0.1194, 0.1201, 0.0800]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.9091e-04, 2.5918e-07, 3.6338e-04, 4.5238e-02, 7.7582e-03, 9.3710e-01,\n",
       "           7.9220e-03, 8.2900e-04, 1.9724e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0519, 0.0100, 0.0541, 0.1644, 0.1357, 0.3764, 0.1084, 0.0829, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0805, 0.0209, 0.0768, 0.1664, 0.1434, 0.2556, 0.1134, 0.1101, 0.0329]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0009, 0.0396, 0.1281, 0.1211, 0.5467, 0.0667, 0.0553, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0291, 0.0894, 0.1441, 0.1522, 0.2217, 0.1219, 0.1105, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0883, 0.0338, 0.0885, 0.1315, 0.1625, 0.2205, 0.1176, 0.1085, 0.0488]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.0654, 0.1016, 0.1235, 0.1397, 0.1554, 0.1239, 0.1177, 0.0724]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0925, 0.0364, 0.0936, 0.1440, 0.1509, 0.2156, 0.1199, 0.1020, 0.0452]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0965, 0.0354, 0.0832, 0.1500, 0.1494, 0.1941, 0.1232, 0.1168, 0.0514]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0941, 0.0393, 0.0896, 0.1423, 0.1421, 0.2035, 0.1173, 0.1171, 0.0548]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0592, 0.0969, 0.1320, 0.1367, 0.1599, 0.1203, 0.1231, 0.0712]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0917, 0.0350, 0.0913, 0.1540, 0.1518, 0.1980, 0.1213, 0.1097, 0.0472]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1048, 0.0668, 0.0966, 0.1294, 0.1345, 0.1497, 0.1224, 0.1200, 0.0758]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0430, 0.0890, 0.1346, 0.1670, 0.1963, 0.1263, 0.1074, 0.0438]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0965, 0.0652, 0.0998, 0.1224, 0.1429, 0.1598, 0.1199, 0.1212, 0.0723]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0854, 0.0399, 0.0891, 0.1411, 0.1454, 0.2009, 0.1319, 0.1149, 0.0514]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0972, 0.0701, 0.1017, 0.1220, 0.1427, 0.1473, 0.1213, 0.1177, 0.0800]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1064, 0.0671, 0.1013, 0.1200, 0.1364, 0.1563, 0.1173, 0.1184, 0.0769]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5655e-02, 3.2035e-04, 2.2072e-02, 2.4972e-01, 1.1190e-01, 5.0702e-01,\n",
       "           4.6187e-02, 4.6268e-02, 8.6893e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0946, 0.0415, 0.0927, 0.1472, 0.1540, 0.1819, 0.1307, 0.1104, 0.0471]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0664, 0.0148, 0.0747, 0.1840, 0.1586, 0.2340, 0.1326, 0.1171, 0.0178]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0570, 0.0145, 0.0674, 0.1869, 0.1593, 0.2763, 0.1160, 0.1033, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1000, 0.0541, 0.1000, 0.1302, 0.1487, 0.1525, 0.1286, 0.1228, 0.0632]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0572, 0.0087, 0.0727, 0.2008, 0.1602, 0.2693, 0.1206, 0.0949, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1032, 0.0588, 0.0988, 0.1264, 0.1356, 0.1523, 0.1237, 0.1283, 0.0729]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0335, 0.0005, 0.0285, 0.2217, 0.0972, 0.4954, 0.0775, 0.0449, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1036, 0.0600, 0.0989, 0.1303, 0.1424, 0.1565, 0.1253, 0.1182, 0.0649]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1026, 0.0653, 0.1017, 0.1276, 0.1390, 0.1363, 0.1257, 0.1238, 0.0779]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0965, 0.0558, 0.0907, 0.1376, 0.1429, 0.1544, 0.1295, 0.1250, 0.0677]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0925, 0.0462, 0.0983, 0.1395, 0.1583, 0.1646, 0.1308, 0.1154, 0.0543]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0893, 0.0373, 0.0910, 0.1714, 0.1434, 0.1796, 0.1362, 0.1134, 0.0384]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0950, 0.0507, 0.1033, 0.1387, 0.1462, 0.1564, 0.1285, 0.1175, 0.0637]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0760, 0.0232, 0.0782, 0.2018, 0.1547, 0.2021, 0.1238, 0.1094, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0059, 0.0523, 0.2549, 0.1246, 0.2702, 0.1220, 0.1017, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0074, 0.0539, 0.2532, 0.1362, 0.2557, 0.1171, 0.1116, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0761, 0.0244, 0.0715, 0.1918, 0.1558, 0.2260, 0.1221, 0.1048, 0.0275]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0195, 0.0671, 0.1949, 0.1485, 0.1970, 0.1451, 0.1191, 0.0298]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0881, 0.0298, 0.0855, 0.1645, 0.1387, 0.1819, 0.1356, 0.1365, 0.0394]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0795, 0.0179, 0.0731, 0.1998, 0.1444, 0.2181, 0.1358, 0.1083, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0816, 0.0195, 0.0733, 0.1854, 0.1539, 0.2108, 0.1301, 0.1149, 0.0306]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0703, 0.0175, 0.0750, 0.1973, 0.1480, 0.2084, 0.1375, 0.1230, 0.0230]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0330, 0.0012, 0.0297, 0.3097, 0.1287, 0.3218, 0.1213, 0.0525, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.5077e-02, 2.4557e-04, 1.5038e-02, 3.8496e-01, 9.9823e-02, 3.4351e-01,\n",
       "           9.0183e-02, 4.0070e-02, 1.0899e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0737, 0.0162, 0.0753, 0.1998, 0.1478, 0.2200, 0.1484, 0.0994, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0697, 0.0141, 0.0651, 0.2239, 0.1763, 0.1961, 0.1397, 0.0997, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0892, 0.0323, 0.0880, 0.1826, 0.1406, 0.1777, 0.1347, 0.1166, 0.0383]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0231, 0.0807, 0.1830, 0.1559, 0.2375, 0.1112, 0.1035, 0.0295]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0796, 0.0367, 0.0800, 0.1660, 0.1548, 0.1898, 0.1238, 0.1277, 0.0418]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0549, 0.0077, 0.0560, 0.2657, 0.1400, 0.2457, 0.1217, 0.0981, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0710, 0.0212, 0.0700, 0.1988, 0.1751, 0.2104, 0.1273, 0.1014, 0.0249]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0688, 0.0119, 0.0641, 0.2145, 0.1809, 0.2167, 0.1322, 0.0962, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0433, 0.0036, 0.0499, 0.2571, 0.1281, 0.3426, 0.0994, 0.0689, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0691, 0.0102, 0.0648, 0.2303, 0.1474, 0.2431, 0.1199, 0.1014, 0.0138]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0301, 0.0013, 0.0322, 0.2836, 0.1697, 0.3262, 0.0946, 0.0596, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0669, 0.0059, 0.0522, 0.2034, 0.1638, 0.2870, 0.1267, 0.0855, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0838, 0.0209, 0.0709, 0.2050, 0.1428, 0.2147, 0.1149, 0.1167, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0698, 0.0147, 0.0762, 0.1901, 0.1700, 0.2224, 0.1213, 0.1135, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0750, 0.0193, 0.0795, 0.1833, 0.1567, 0.2144, 0.1336, 0.1115, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0544, 0.0037, 0.0465, 0.2536, 0.1680, 0.2854, 0.1176, 0.0658, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0881, 0.0285, 0.0769, 0.1750, 0.1536, 0.1929, 0.1308, 0.1195, 0.0346]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0888, 0.0254, 0.0796, 0.1715, 0.1429, 0.2209, 0.1136, 0.1213, 0.0358]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0482, 0.0049, 0.0406, 0.2957, 0.1460, 0.3095, 0.0748, 0.0733, 0.0069]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0605, 0.0071, 0.0484, 0.2406, 0.1402, 0.2890, 0.1247, 0.0801, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0055, 0.0584, 0.1786, 0.1452, 0.3711, 0.0948, 0.0762, 0.0087]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0953, 0.0329, 0.0837, 0.1579, 0.1475, 0.1974, 0.1277, 0.1199, 0.0378]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0323, 0.0028, 0.0451, 0.2532, 0.2038, 0.2999, 0.1134, 0.0446, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0247, 0.0013, 0.0298, 0.2518, 0.1156, 0.4448, 0.0865, 0.0428, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.9489e-03, 7.8343e-06, 2.3157e-03, 2.9443e-01, 5.0124e-02, 6.1496e-01,\n",
       "           2.7495e-02, 6.7044e-03, 1.7184e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0690, 0.0103, 0.0576, 0.1627, 0.2089, 0.2771, 0.1173, 0.0797, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0670, 0.0167, 0.0679, 0.1900, 0.1670, 0.2374, 0.1318, 0.0986, 0.0236]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0499, 0.0061, 0.0446, 0.2175, 0.1666, 0.2907, 0.1137, 0.1008, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0294, 0.0792, 0.1641, 0.1806, 0.2213, 0.1223, 0.0886, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0319, 0.0046, 0.0392, 0.2143, 0.1791, 0.3728, 0.0967, 0.0537, 0.0077]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0850, 0.0083, 0.0476, 0.1795, 0.1638, 0.3067, 0.1262, 0.0736, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0262, 0.0880, 0.1538, 0.1648, 0.2380, 0.1096, 0.1090, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0526, 0.0035, 0.0328, 0.2001, 0.1617, 0.4071, 0.0830, 0.0555, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0129, 0.0592, 0.1814, 0.1602, 0.2971, 0.1076, 0.0923, 0.0221]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0388, 0.0006, 0.0263, 0.2817, 0.1812, 0.3852, 0.0569, 0.0275, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.1932e-03, 3.0251e-05, 4.1978e-03, 2.0855e-01, 7.6807e-02, 6.6607e-01,\n",
       "           3.4957e-02, 5.1268e-03, 6.4327e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0449, 0.0013, 0.0252, 0.2201, 0.1512, 0.4467, 0.0573, 0.0494, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0747, 0.0131, 0.0645, 0.1725, 0.1570, 0.3226, 0.0859, 0.0909, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0827, 0.0209, 0.0646, 0.1664, 0.1782, 0.2566, 0.1083, 0.0981, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0807, 0.0126, 0.0763, 0.1681, 0.1522, 0.2814, 0.1052, 0.1045, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0910, 0.0228, 0.0703, 0.1475, 0.1908, 0.2436, 0.1089, 0.0889, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0438, 0.0020, 0.0294, 0.1859, 0.1261, 0.4987, 0.0564, 0.0548, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0192, 0.0687, 0.1279, 0.1931, 0.2781, 0.1291, 0.0869, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0414, 0.0030, 0.0459, 0.1436, 0.1881, 0.4299, 0.0942, 0.0483, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0519, 0.0033, 0.0508, 0.1393, 0.1925, 0.4082, 0.0668, 0.0798, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8290e-03, 5.5726e-06, 3.1003e-03, 1.0511e-01, 8.1381e-02, 7.9835e-01,\n",
       "           6.3338e-03, 3.8572e-03, 2.7878e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0074, 0.0593, 0.1523, 0.1959, 0.3633, 0.0945, 0.0555, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0726, 0.0114, 0.0637, 0.1558, 0.1436, 0.3409, 0.0891, 0.1055, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0873, 0.0243, 0.0857, 0.1418, 0.1802, 0.2543, 0.0916, 0.1006, 0.0343]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.4432e-03, 3.1443e-06, 1.3684e-03, 3.3509e-02, 2.4225e-02, 9.2877e-01,\n",
       "           4.5620e-03, 2.1069e-03, 1.3580e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7651e-02, 4.8778e-04, 2.5753e-02, 1.4609e-01, 1.1653e-01, 6.3369e-01,\n",
       "           4.1731e-02, 1.7033e-02, 1.0313e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0428, 0.0027, 0.0305, 0.1323, 0.1782, 0.4999, 0.0663, 0.0436, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0049, 0.0672, 0.1455, 0.2048, 0.3686, 0.0772, 0.0586, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0716, 0.0075, 0.0725, 0.1613, 0.1747, 0.3259, 0.0804, 0.0916, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0205, 0.0808, 0.1478, 0.1823, 0.2649, 0.1076, 0.0818, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0615, 0.0104, 0.0843, 0.1266, 0.2101, 0.3197, 0.1021, 0.0679, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0896, 0.0287, 0.0818, 0.1375, 0.1690, 0.2618, 0.1067, 0.0910, 0.0339]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.4251e-03, 6.7285e-07, 1.2397e-03, 6.1020e-02, 3.8445e-02, 8.9126e-01,\n",
       "           4.0551e-03, 1.5471e-03, 6.2119e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0462, 0.0034, 0.0555, 0.1592, 0.1651, 0.4412, 0.0699, 0.0523, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7823e-03, 1.2831e-06, 9.3926e-04, 6.5138e-02, 3.5684e-02, 8.9302e-01,\n",
       "           1.7761e-03, 1.6490e-03, 5.0643e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0394, 0.0007, 0.0340, 0.1188, 0.1455, 0.5704, 0.0527, 0.0362, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.3101e-04, 1.2084e-07, 1.4915e-04, 2.3956e-02, 1.6401e-02, 9.5714e-01,\n",
       "           1.7196e-03, 1.9850e-04, 4.6423e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0475, 0.0025, 0.0407, 0.1173, 0.1317, 0.5566, 0.0617, 0.0385, 0.0036]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.1328e-04, 1.4467e-08, 6.4575e-05, 9.2537e-03, 6.8090e-03, 9.8306e-01,\n",
       "           5.1251e-04, 8.3288e-05, 9.5502e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0473, 0.0040, 0.0426, 0.1189, 0.1858, 0.4644, 0.0819, 0.0473, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0819, 0.0062, 0.0463, 0.1274, 0.1874, 0.3756, 0.1071, 0.0564, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0872, 0.0256, 0.0828, 0.1201, 0.1657, 0.2829, 0.1147, 0.0922, 0.0288]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0624, 0.0050, 0.0574, 0.1241, 0.1905, 0.3693, 0.1118, 0.0671, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0270, 0.0936, 0.1188, 0.1702, 0.2624, 0.1178, 0.0842, 0.0334]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0573, 0.0041, 0.0429, 0.1181, 0.2143, 0.4228, 0.0806, 0.0518, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.6853e-02, 5.6298e-04, 1.5784e-02, 9.1545e-02, 8.6926e-02, 7.3007e-01,\n",
       "           4.0239e-02, 1.7176e-02, 8.4273e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0081, 0.0639, 0.1238, 0.1747, 0.3919, 0.0763, 0.0585, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0354, 0.0015, 0.0308, 0.0847, 0.1599, 0.5967, 0.0559, 0.0333, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0966, 0.0293, 0.0932, 0.1156, 0.1698, 0.2276, 0.1187, 0.1082, 0.0410]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0264, 0.0010, 0.0198, 0.1171, 0.1896, 0.5609, 0.0596, 0.0238, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0621, 0.0039, 0.0553, 0.0914, 0.2162, 0.4195, 0.0761, 0.0675, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0844, 0.0139, 0.0769, 0.1191, 0.2203, 0.2753, 0.1077, 0.0797, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0823, 0.0142, 0.0801, 0.1173, 0.2054, 0.3069, 0.0908, 0.0799, 0.0232]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0401, 0.0032, 0.0538, 0.1006, 0.2025, 0.4880, 0.0681, 0.0384, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0616, 0.0052, 0.0500, 0.0940, 0.1745, 0.4974, 0.0670, 0.0456, 0.0045]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0300, 0.0008, 0.0190, 0.0875, 0.2364, 0.5720, 0.0383, 0.0149, 0.0010]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0442, 0.0028, 0.0469, 0.1383, 0.1988, 0.4525, 0.0654, 0.0472, 0.0039]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0577, 0.0072, 0.0481, 0.1170, 0.2191, 0.4102, 0.0842, 0.0487, 0.0078]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0483, 0.0061, 0.0510, 0.1506, 0.1720, 0.4289, 0.0691, 0.0604, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.8162e-06, 2.2930e-12, 2.4271e-06, 7.3854e-04, 6.3580e-03, 9.9284e-01,\n",
       "           4.9737e-05, 7.2994e-06, 8.6266e-11]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0158, 0.0745, 0.1332, 0.1811, 0.2944, 0.0986, 0.0899, 0.0224]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0009, 0.0272, 0.0911, 0.2100, 0.5481, 0.0631, 0.0302, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0653, 0.0072, 0.0520, 0.1201, 0.2212, 0.3826, 0.0820, 0.0555, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0387, 0.0011, 0.0416, 0.1048, 0.2073, 0.5245, 0.0504, 0.0290, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0385, 0.0013, 0.0392, 0.0870, 0.1951, 0.5612, 0.0493, 0.0267, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.9525e-03, 2.3720e-05, 4.3264e-03, 5.3099e-02, 1.7454e-01, 7.4670e-01,\n",
       "           1.1343e-02, 3.9583e-03, 6.0971e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0727, 0.0128, 0.0817, 0.1091, 0.1898, 0.3361, 0.0970, 0.0834, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0224, 0.0007, 0.0299, 0.1060, 0.1797, 0.5910, 0.0426, 0.0264, 0.0012]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0083, 0.0716, 0.1133, 0.1656, 0.3962, 0.0828, 0.0693, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0492, 0.0055, 0.0525, 0.1076, 0.2319, 0.3965, 0.0886, 0.0562, 0.0120]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[1.3127e-02, 1.9658e-04, 1.1524e-02, 1.3729e-01, 9.5579e-02, 7.0825e-01,\n",
       "           1.5768e-02, 1.7914e-02, 3.5064e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0640, 0.0061, 0.0541, 0.1266, 0.2086, 0.3989, 0.0905, 0.0429, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0554, 0.0026, 0.0346, 0.1478, 0.1811, 0.4774, 0.0481, 0.0496, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0683, 0.0113, 0.0666, 0.1368, 0.2092, 0.3226, 0.0850, 0.0800, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0046, 0.0578, 0.0987, 0.2086, 0.4085, 0.0789, 0.0706, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0534, 0.0050, 0.0486, 0.1256, 0.2325, 0.4091, 0.0851, 0.0357, 0.0050]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0465, 0.0007, 0.0284, 0.1410, 0.1566, 0.5308, 0.0583, 0.0360, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0516, 0.0071, 0.0494, 0.1450, 0.2047, 0.4208, 0.0728, 0.0422, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0708, 0.0081, 0.0577, 0.1591, 0.1901, 0.3212, 0.1049, 0.0754, 0.0127]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.9246e-02, 4.0045e-04, 1.6510e-02, 1.3925e-01, 1.5192e-01, 6.2869e-01,\n",
       "           2.9120e-02, 1.3993e-02, 8.6685e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0427, 0.0012, 0.0278, 0.1234, 0.1351, 0.5840, 0.0495, 0.0344, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0596, 0.0059, 0.0555, 0.1270, 0.2159, 0.3922, 0.0878, 0.0470, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0744, 0.0061, 0.0632, 0.1468, 0.2060, 0.3316, 0.0879, 0.0701, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0030, 0.0536, 0.1711, 0.1535, 0.4289, 0.0582, 0.0518, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1003, 0.0225, 0.0981, 0.1131, 0.1903, 0.2365, 0.1083, 0.0979, 0.0332]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0311, 0.0016, 0.0341, 0.1274, 0.1440, 0.5745, 0.0541, 0.0313, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0909, 0.0091, 0.0629, 0.1358, 0.1974, 0.3340, 0.0914, 0.0656, 0.0130]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2209e-02, 7.0802e-05, 1.0742e-02, 1.1461e-01, 1.0202e-01, 7.2496e-01,\n",
       "           2.8516e-02, 6.6424e-03, 2.2536e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0625, 0.0098, 0.0788, 0.1307, 0.1963, 0.3609, 0.0744, 0.0705, 0.0160]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0351, 0.0017, 0.0331, 0.1567, 0.1640, 0.5482, 0.0387, 0.0207, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0960, 0.0295, 0.1042, 0.1177, 0.1644, 0.2488, 0.1003, 0.0975, 0.0416]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0891, 0.0189, 0.0914, 0.1362, 0.1746, 0.2815, 0.0900, 0.0899, 0.0284]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0768, 0.0055, 0.0714, 0.1610, 0.1315, 0.3973, 0.0687, 0.0794, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.9307e-03, 3.2678e-06, 5.2637e-03, 7.3315e-02, 4.7274e-02, 8.5575e-01,\n",
       "           8.6316e-03, 5.7878e-03, 4.3043e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0658, 0.0055, 0.0727, 0.1459, 0.1771, 0.3839, 0.0755, 0.0639, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0791, 0.0131, 0.0765, 0.1558, 0.1660, 0.2916, 0.0997, 0.0957, 0.0225]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1007, 0.0261, 0.0981, 0.1229, 0.1636, 0.2330, 0.1184, 0.1050, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0321, 0.0014, 0.0415, 0.1342, 0.1877, 0.4849, 0.0730, 0.0416, 0.0035]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0909, 0.0283, 0.0978, 0.1340, 0.1710, 0.2381, 0.1125, 0.0966, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1027, 0.0143, 0.0894, 0.1490, 0.1601, 0.2840, 0.0923, 0.0859, 0.0223]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1001, 0.0118, 0.0957, 0.1332, 0.1661, 0.2885, 0.1059, 0.0799, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.2112e-03, 5.9346e-06, 2.4503e-03, 5.8930e-02, 2.6802e-02, 8.9852e-01,\n",
       "           8.7037e-03, 1.3715e-03, 8.1023e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0712, 0.0056, 0.0546, 0.1786, 0.1697, 0.3722, 0.0782, 0.0605, 0.0095]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0885, 0.0193, 0.0886, 0.1283, 0.1931, 0.2631, 0.1177, 0.0772, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0036, 0.0437, 0.1582, 0.1400, 0.4391, 0.0758, 0.0713, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0888, 0.0280, 0.1025, 0.1346, 0.1739, 0.2109, 0.1232, 0.1061, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6727e-05, 2.9860e-10, 1.0854e-04, 4.5540e-02, 1.3337e-02, 9.4032e-01,\n",
       "           6.3379e-04, 3.0102e-05, 6.6746e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1036, 0.0473, 0.1112, 0.1233, 0.1517, 0.1913, 0.1136, 0.1024, 0.0556]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0968, 0.0359, 0.1096, 0.1189, 0.1605, 0.2083, 0.1139, 0.1106, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0912, 0.0229, 0.1013, 0.1318, 0.1819, 0.2390, 0.1172, 0.0905, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0721, 0.0104, 0.0794, 0.1469, 0.1671, 0.3230, 0.0807, 0.0999, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1011, 0.0197, 0.0802, 0.1453, 0.1499, 0.2951, 0.1033, 0.0805, 0.0249]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0901, 0.0145, 0.0810, 0.1389, 0.1589, 0.2918, 0.0933, 0.1037, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0356, 0.1094, 0.1257, 0.1709, 0.1959, 0.1206, 0.1080, 0.0427]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0117, 0.0750, 0.1512, 0.1582, 0.3462, 0.0984, 0.0713, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0655, 0.0139, 0.0806, 0.1282, 0.1662, 0.3072, 0.1277, 0.0907, 0.0201]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1038, 0.0335, 0.0986, 0.1334, 0.1564, 0.2097, 0.1107, 0.1136, 0.0402]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0951, 0.0281, 0.1109, 0.1243, 0.1600, 0.2168, 0.1158, 0.1031, 0.0461]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0553, 0.0044, 0.0591, 0.1814, 0.1348, 0.3988, 0.0996, 0.0586, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0880, 0.0231, 0.0907, 0.1390, 0.1798, 0.2378, 0.1350, 0.0817, 0.0250]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0967, 0.0374, 0.0994, 0.1208, 0.1854, 0.1838, 0.1253, 0.1053, 0.0459]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1101, 0.0517, 0.0996, 0.1235, 0.1525, 0.1713, 0.1191, 0.1157, 0.0564]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0354, 0.0927, 0.1192, 0.2079, 0.1921, 0.1282, 0.0938, 0.0394]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0795, 0.0108, 0.1096, 0.1282, 0.1859, 0.2479, 0.1180, 0.1004, 0.0198]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1143, 0.0238, 0.0914, 0.1548, 0.1670, 0.2165, 0.1091, 0.0935, 0.0297]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0908, 0.0334, 0.0983, 0.1245, 0.1749, 0.2033, 0.1228, 0.1112, 0.0409]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0937, 0.0329, 0.0970, 0.1136, 0.1829, 0.2139, 0.1219, 0.0993, 0.0448]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.4368e-02, 1.8603e-04, 2.8430e-02, 9.5073e-02, 1.9854e-01, 5.9114e-01,\n",
       "           4.7224e-02, 2.4552e-02, 4.8126e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.3616e-02, 2.1173e-04, 2.0069e-02, 9.8120e-02, 1.3305e-01, 6.8159e-01,\n",
       "           2.9999e-02, 2.2339e-02, 1.0126e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0049, 0.0739, 0.1424, 0.1618, 0.3354, 0.0880, 0.0999, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0335, 0.0017, 0.0505, 0.1622, 0.2086, 0.4059, 0.0899, 0.0447, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.1270e-03, 1.4251e-05, 6.2418e-03, 7.6829e-02, 1.4013e-01, 7.4336e-01,\n",
       "           2.0566e-02, 4.6611e-03, 7.9827e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0077, 0.0719, 0.1244, 0.1988, 0.3303, 0.1075, 0.0769, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0858, 0.0190, 0.0893, 0.1184, 0.1784, 0.2515, 0.1311, 0.0982, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0016, 0.0592, 0.1086, 0.2254, 0.4138, 0.0925, 0.0523, 0.0044]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0897, 0.0203, 0.0904, 0.1367, 0.1934, 0.2264, 0.1084, 0.1103, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5221e-03, 1.0784e-06, 2.6836e-03, 3.5026e-02, 2.5537e-02, 9.2305e-01,\n",
       "           9.7508e-03, 2.4187e-03, 8.3190e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0014, 0.0347, 0.1523, 0.1562, 0.5108, 0.0676, 0.0360, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0957, 0.0277, 0.0949, 0.1212, 0.1625, 0.2000, 0.1339, 0.1227, 0.0415]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.6040e-02, 4.0320e-04, 2.5700e-02, 9.1616e-02, 1.6311e-01, 5.9462e-01,\n",
       "           7.7645e-02, 2.9605e-02, 1.2599e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0900, 0.0125, 0.0775, 0.1324, 0.1604, 0.3011, 0.1121, 0.0933, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0282, 0.0010, 0.0385, 0.1336, 0.1744, 0.4848, 0.0900, 0.0471, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0881, 0.0237, 0.0991, 0.1056, 0.1717, 0.2506, 0.1152, 0.1092, 0.0366]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0942, 0.0208, 0.0868, 0.1377, 0.1597, 0.2599, 0.1129, 0.0997, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0061, 0.0636, 0.1073, 0.1848, 0.3957, 0.1132, 0.0687, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0813, 0.0218, 0.0834, 0.1176, 0.1827, 0.2705, 0.1263, 0.0924, 0.0240]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.2920e-03, 7.0075e-07, 1.1610e-03, 3.4530e-02, 2.3427e-02, 9.3137e-01,\n",
       "           5.5792e-03, 1.6311e-03, 1.3043e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0949, 0.0228, 0.0876, 0.1214, 0.1609, 0.2543, 0.1225, 0.1046, 0.0309]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8648e-04, 1.3539e-08, 2.4567e-04, 8.1128e-03, 5.7829e-03, 9.8395e-01,\n",
       "           1.2591e-03, 4.6725e-04, 1.4965e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0497, 0.0077, 0.0768, 0.0965, 0.2390, 0.3268, 0.1116, 0.0773, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0749, 0.0098, 0.0833, 0.0913, 0.2275, 0.2995, 0.1207, 0.0785, 0.0145]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0283, 0.1041, 0.1088, 0.1709, 0.2404, 0.1229, 0.1008, 0.0331]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.8445e-04, 9.0254e-08, 4.5316e-04, 9.0519e-03, 1.2230e-02, 9.7114e-01,\n",
       "           5.1886e-03, 1.0500e-03, 9.6520e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0978, 0.0341, 0.1009, 0.1137, 0.1774, 0.1842, 0.1268, 0.1181, 0.0472]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0716, 0.0060, 0.0622, 0.1219, 0.1873, 0.3514, 0.1098, 0.0789, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.1165e-03, 3.5691e-07, 1.0297e-03, 4.1795e-02, 5.3991e-02, 8.8261e-01,\n",
       "           1.7276e-02, 2.1749e-03, 1.6193e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0924, 0.0259, 0.0911, 0.1146, 0.1910, 0.2289, 0.1306, 0.0970, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0771, 0.0098, 0.0710, 0.1312, 0.1606, 0.3237, 0.1181, 0.0898, 0.0187]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1029, 0.0215, 0.0837, 0.0926, 0.2343, 0.2124, 0.1314, 0.0925, 0.0288]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0860, 0.0093, 0.0734, 0.1401, 0.1603, 0.3204, 0.1033, 0.0952, 0.0121]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.6616e-02, 1.8077e-04, 1.9768e-02, 1.0641e-01, 5.5606e-02, 6.9727e-01,\n",
       "           5.7520e-02, 2.6165e-02, 4.6786e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0319, 0.0011, 0.0308, 0.1040, 0.1343, 0.5680, 0.0968, 0.0310, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0931, 0.0218, 0.0888, 0.1192, 0.1627, 0.2609, 0.1367, 0.0905, 0.0264]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0772, 0.0116, 0.0766, 0.1201, 0.2022, 0.2728, 0.1182, 0.1019, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0651, 0.0070, 0.0716, 0.0984, 0.2632, 0.2868, 0.1346, 0.0631, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2308e-02, 5.8348e-05, 7.9575e-03, 9.3742e-02, 1.3380e-01, 6.8125e-01,\n",
       "           5.3034e-02, 1.7641e-02, 2.1243e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0146, 0.0689, 0.1200, 0.2116, 0.3092, 0.1024, 0.0725, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0826, 0.0184, 0.0968, 0.1009, 0.2198, 0.2225, 0.1277, 0.1041, 0.0273]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0573, 0.0052, 0.0597, 0.1242, 0.1557, 0.4121, 0.0953, 0.0803, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7785e-02, 3.0777e-04, 1.5647e-02, 1.0680e-01, 1.0254e-01, 6.4832e-01,\n",
       "           7.3165e-02, 3.4354e-02, 1.0917e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1065, 0.0250, 0.0906, 0.1138, 0.1608, 0.2316, 0.1201, 0.1148, 0.0366]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1114, 0.0280, 0.0837, 0.1219, 0.1617, 0.2195, 0.1243, 0.1141, 0.0353]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0893, 0.0089, 0.0637, 0.1392, 0.1666, 0.3086, 0.1081, 0.0980, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0007, 0.0249, 0.1260, 0.1733, 0.5370, 0.0656, 0.0428, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0062, 0.0675, 0.1069, 0.1693, 0.3913, 0.0990, 0.0805, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0813, 0.0158, 0.0756, 0.1159, 0.1934, 0.2754, 0.1223, 0.0981, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0833, 0.0204, 0.0826, 0.1153, 0.1840, 0.2727, 0.1225, 0.0910, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0997, 0.0264, 0.0904, 0.1211, 0.1681, 0.2302, 0.1060, 0.1126, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1008, 0.0277, 0.0948, 0.1017, 0.1758, 0.2188, 0.1246, 0.1187, 0.0371]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0946, 0.0106, 0.0807, 0.1145, 0.1624, 0.3020, 0.1246, 0.0970, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[5.9159e-03, 3.1939e-05, 3.2650e-03, 4.4915e-02, 4.8543e-02, 8.6180e-01,\n",
       "           2.5486e-02, 9.9636e-03, 7.7341e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0866, 0.0078, 0.0879, 0.1106, 0.1666, 0.3190, 0.0988, 0.1074, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0966, 0.0297, 0.0966, 0.0951, 0.1782, 0.2379, 0.1281, 0.1068, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[7.6997e-03, 2.2290e-04, 1.2393e-02, 7.0858e-02, 1.3513e-01, 7.0835e-01,\n",
       "           4.6153e-02, 1.8424e-02, 7.6963e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0372, 0.0013, 0.0302, 0.1089, 0.1280, 0.5358, 0.1070, 0.0484, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0531, 0.0035, 0.0573, 0.0906, 0.2166, 0.3889, 0.0934, 0.0873, 0.0092]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0375, 0.0010, 0.0335, 0.0910, 0.1614, 0.5576, 0.0625, 0.0533, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0983, 0.0368, 0.0993, 0.1125, 0.1694, 0.2006, 0.1142, 0.1149, 0.0539]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0876, 0.0091, 0.0595, 0.1375, 0.1562, 0.3253, 0.1110, 0.1012, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0026, 0.0443, 0.1269, 0.1676, 0.4418, 0.0940, 0.0701, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.7353e-07, 5.7695e-15, 1.0336e-07, 2.8528e-04, 7.6066e-05, 9.9959e-01,\n",
       "           4.4816e-05, 1.5243e-06, 6.5039e-13]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0675, 0.0038, 0.0539, 0.1063, 0.1885, 0.3558, 0.1003, 0.1103, 0.0136]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.0250, 0.0956, 0.1101, 0.1707, 0.2182, 0.1256, 0.1175, 0.0377]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0656, 0.0037, 0.0416, 0.1190, 0.1999, 0.3590, 0.1052, 0.0992, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0876, 0.0219, 0.0947, 0.1114, 0.1831, 0.2267, 0.1312, 0.1068, 0.0365]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0543, 0.0045, 0.0533, 0.1147, 0.1583, 0.4423, 0.0998, 0.0630, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.4268e-03, 4.6720e-07, 6.4393e-04, 4.8895e-02, 1.9255e-02, 9.1924e-01,\n",
       "           8.0745e-03, 2.4584e-03, 4.1139e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6752e-03, 6.7114e-06, 3.1123e-03, 4.7285e-02, 5.0628e-02, 8.6070e-01,\n",
       "           2.1109e-02, 1.4370e-02, 1.0898e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0575, 0.0054, 0.0569, 0.1321, 0.1611, 0.3555, 0.1199, 0.0991, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1105, 0.0285, 0.0935, 0.1228, 0.1536, 0.2125, 0.1295, 0.1104, 0.0388]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1043, 0.0389, 0.0991, 0.1172, 0.1491, 0.1947, 0.1284, 0.1178, 0.0504]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0810, 0.0213, 0.0881, 0.1213, 0.1952, 0.2409, 0.1332, 0.0983, 0.0207]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0508, 0.0034, 0.0536, 0.1153, 0.1663, 0.4315, 0.0991, 0.0717, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0593, 0.0042, 0.0640, 0.1269, 0.1498, 0.4084, 0.1177, 0.0620, 0.0077]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0024, 0.0524, 0.1190, 0.1119, 0.4918, 0.0841, 0.0711, 0.0073]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0033, 0.0551, 0.1108, 0.1715, 0.4211, 0.1005, 0.0673, 0.0059]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0897, 0.0062, 0.0582, 0.1438, 0.1668, 0.3545, 0.0973, 0.0721, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0202, 0.0863, 0.1322, 0.1588, 0.2712, 0.1089, 0.1006, 0.0312]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0604, 0.0049, 0.0409, 0.1596, 0.1217, 0.4481, 0.0850, 0.0711, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0872, 0.0059, 0.0705, 0.1339, 0.1288, 0.3612, 0.1120, 0.0884, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0170, 0.0906, 0.1354, 0.1851, 0.2276, 0.1156, 0.1070, 0.0317]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0542, 0.0032, 0.0503, 0.1350, 0.1460, 0.4586, 0.0889, 0.0590, 0.0050]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.0811e-02, 5.4054e-05, 9.3744e-03, 7.5009e-02, 4.6837e-02, 8.2909e-01,\n",
       "           2.0704e-02, 7.9453e-03, 1.7038e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0169, 0.0931, 0.1320, 0.1947, 0.2553, 0.1225, 0.0839, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.1149e-03, 3.7612e-05, 8.5657e-03, 1.0992e-01, 7.6846e-02, 7.6137e-01,\n",
       "           2.7724e-02, 7.2835e-03, 1.4314e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0035, 0.0610, 0.1263, 0.2177, 0.3624, 0.1013, 0.0722, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0458, 0.0020, 0.0538, 0.0997, 0.1549, 0.4961, 0.0810, 0.0591, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0326, 0.0017, 0.0421, 0.1260, 0.1518, 0.5121, 0.0867, 0.0435, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0580, 0.0022, 0.0523, 0.1693, 0.1790, 0.3799, 0.0882, 0.0662, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0868, 0.0212, 0.0971, 0.1320, 0.1741, 0.2455, 0.1187, 0.0947, 0.0299]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0470, 0.0068, 0.0688, 0.1024, 0.2194, 0.3640, 0.1191, 0.0608, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0840, 0.0206, 0.1038, 0.1120, 0.1990, 0.2355, 0.1094, 0.1012, 0.0343]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.3623e-02, 2.0746e-04, 1.5912e-02, 1.2704e-01, 6.0358e-02, 7.0189e-01,\n",
       "           4.1678e-02, 2.8704e-02, 5.8630e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.8285e-02, 4.6788e-04, 2.7925e-02, 1.9006e-01, 1.2382e-01, 5.0009e-01,\n",
       "           7.7816e-02, 3.0545e-02, 9.9719e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0408, 0.0022, 0.0753, 0.1063, 0.2753, 0.3488, 0.0930, 0.0524, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0198, 0.0920, 0.1306, 0.1878, 0.2445, 0.1163, 0.0882, 0.0295]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.4708e-02, 9.7714e-05, 2.5191e-02, 1.0275e-01, 1.7510e-01, 5.9959e-01,\n",
       "           5.9874e-02, 2.2239e-02, 4.4758e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.7086e-02, 1.9330e-04, 2.2845e-02, 9.5299e-02, 1.4435e-01, 6.5512e-01,\n",
       "           4.5925e-02, 1.8714e-02, 4.6421e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0655, 0.0044, 0.0665, 0.1320, 0.1935, 0.3731, 0.0886, 0.0695, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0541, 0.0012, 0.0564, 0.1515, 0.1735, 0.4067, 0.0835, 0.0685, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0870, 0.0172, 0.1112, 0.1045, 0.2208, 0.2164, 0.1062, 0.1059, 0.0310]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0578, 0.0055, 0.0875, 0.1037, 0.2693, 0.2970, 0.1110, 0.0567, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.6911e-02, 2.1569e-04, 2.2580e-02, 1.0265e-01, 1.5388e-01, 6.3877e-01,\n",
       "           4.4006e-02, 2.0296e-02, 6.9228e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0367e-02, 6.4999e-05, 2.0096e-02, 1.0162e-01, 1.6315e-01, 6.4748e-01,\n",
       "           4.4464e-02, 1.2581e-02, 1.7173e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0489, 0.0054, 0.0684, 0.1446, 0.2475, 0.3029, 0.1096, 0.0639, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0445, 0.0025, 0.0707, 0.1124, 0.2406, 0.3667, 0.0835, 0.0739, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0879, 0.0169, 0.0929, 0.1282, 0.2049, 0.2344, 0.1295, 0.0826, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1024, 0.0124, 0.0779, 0.1230, 0.2291, 0.2415, 0.1130, 0.0789, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.4176e-07, 2.2320e-14, 2.1612e-07, 2.2857e-03, 3.9561e-04, 9.9718e-01,\n",
       "           1.3615e-04, 9.0908e-07, 2.8579e-12]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0732, 0.0155, 0.0884, 0.1119, 0.2564, 0.2384, 0.1152, 0.0794, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0525, 0.0035, 0.0526, 0.1157, 0.2092, 0.3967, 0.1084, 0.0554, 0.0061]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0407, 0.0023, 0.0645, 0.1212, 0.2824, 0.3357, 0.0901, 0.0563, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0418, 0.0012, 0.0620, 0.1112, 0.2219, 0.4241, 0.0940, 0.0395, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0545, 0.0055, 0.0708, 0.1190, 0.2465, 0.3078, 0.1246, 0.0631, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0828, 0.0134, 0.0967, 0.1160, 0.2438, 0.2168, 0.0990, 0.1085, 0.0230]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0204, 0.1035, 0.1170, 0.1966, 0.2119, 0.1236, 0.1058, 0.0342]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.7175e-03, 5.0588e-07, 2.7210e-03, 4.2988e-02, 4.5778e-02, 8.9225e-01,\n",
       "           1.1728e-02, 2.8122e-03, 6.6120e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0505, 0.0036, 0.0699, 0.1168, 0.2406, 0.3397, 0.1062, 0.0640, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0294, 0.0023, 0.0603, 0.1146, 0.2229, 0.4158, 0.0976, 0.0507, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0694, 0.0092, 0.0997, 0.1049, 0.2453, 0.2497, 0.1277, 0.0772, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0097, 0.0766, 0.1200, 0.2186, 0.2785, 0.1183, 0.0917, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0737, 0.0147, 0.0915, 0.1195, 0.2124, 0.2506, 0.1245, 0.0893, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[9.3109e-03, 4.9946e-05, 1.4204e-02, 1.0392e-01, 1.2136e-01, 6.8483e-01,\n",
       "           4.8451e-02, 1.7622e-02, 2.4035e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0623, 0.0041, 0.0659, 0.1426, 0.1763, 0.3341, 0.1279, 0.0778, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0041, 0.0729, 0.1470, 0.2122, 0.3149, 0.0930, 0.0814, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.0691e-04, 7.3386e-08, 1.6676e-03, 5.7655e-02, 1.0786e-01, 8.1624e-01,\n",
       "           1.5558e-02, 7.1582e-04, 1.2455e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0895, 0.0204, 0.1057, 0.1211, 0.1971, 0.1972, 0.1341, 0.1077, 0.0272]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0706, 0.0077, 0.0819, 0.1185, 0.2045, 0.2999, 0.1314, 0.0702, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[3.3165e-02, 2.6503e-04, 2.5067e-02, 1.9928e-01, 1.0835e-01, 5.3649e-01,\n",
       "           6.9466e-02, 2.7250e-02, 6.7056e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6869e-02, 3.1362e-04, 3.9930e-02, 1.2672e-01, 1.4428e-01, 5.2683e-01,\n",
       "           1.0357e-01, 3.0718e-02, 7.7290e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5203e-07, 4.8981e-16, 5.2992e-08, 3.4080e-03, 2.9636e-04, 9.9627e-01,\n",
       "           2.4790e-05, 1.6476e-07, 1.5931e-13]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.2410e-04, 1.1237e-06, 2.4186e-03, 7.8492e-02, 1.3182e-01, 7.6124e-01,\n",
       "           2.4108e-02, 1.2977e-03, 4.4133e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0636, 0.0087, 0.0857, 0.1317, 0.2155, 0.2617, 0.1243, 0.0929, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0927, 0.0154, 0.0907, 0.1137, 0.2110, 0.2354, 0.1344, 0.0873, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0550, 0.0027, 0.0760, 0.1145, 0.2386, 0.3446, 0.0991, 0.0644, 0.0051]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0078, 0.0773, 0.1164, 0.2363, 0.2573, 0.1337, 0.0836, 0.0120]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0485, 0.0035, 0.0603, 0.0942, 0.2802, 0.3142, 0.1328, 0.0603, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0515, 0.0037, 0.0771, 0.1538, 0.2065, 0.3104, 0.1042, 0.0837, 0.0090]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0688, 0.0095, 0.0857, 0.1257, 0.2455, 0.2429, 0.1154, 0.0874, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0023, 0.0540, 0.1004, 0.2744, 0.3492, 0.1047, 0.0635, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.5990e-04, 2.3059e-08, 7.0870e-04, 2.7269e-02, 3.5484e-02, 9.2689e-01,\n",
       "           8.7109e-03, 5.7417e-04, 3.1722e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0512, 0.0041, 0.0684, 0.1127, 0.2463, 0.3206, 0.1209, 0.0691, 0.0067]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.7664e-02, 2.4165e-04, 1.8883e-02, 1.5936e-01, 1.8246e-01, 4.8703e-01,\n",
       "           6.8008e-02, 5.5708e-02, 6.3770e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0802, 0.0162, 0.0864, 0.0907, 0.3034, 0.1775, 0.1420, 0.0856, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0730, 0.0091, 0.0953, 0.1072, 0.2929, 0.2063, 0.1137, 0.0899, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0925, 0.0307, 0.1038, 0.1072, 0.2107, 0.1792, 0.1328, 0.1081, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1041, 0.0224, 0.1001, 0.1221, 0.2127, 0.1781, 0.1244, 0.1015, 0.0345]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0674, 0.0051, 0.0873, 0.1222, 0.2626, 0.2407, 0.1150, 0.0898, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0400, 0.0021, 0.0499, 0.1202, 0.2565, 0.3433, 0.1190, 0.0637, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0743, 0.0209, 0.1102, 0.0890, 0.2707, 0.1765, 0.1153, 0.1107, 0.0322]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0525, 0.0051, 0.0873, 0.0740, 0.4145, 0.1572, 0.1152, 0.0845, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.0065, 0.0819, 0.1037, 0.2572, 0.2027, 0.1146, 0.1229, 0.0109]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1044, 0.0443, 0.1151, 0.1025, 0.1870, 0.1543, 0.1227, 0.1174, 0.0524]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1008, 0.0392, 0.1028, 0.1133, 0.1899, 0.1473, 0.1259, 0.1319, 0.0489]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.2144e-02, 9.3604e-05, 3.2464e-02, 1.0779e-01, 2.8476e-01, 3.6789e-01,\n",
       "           9.0771e-02, 9.3553e-02, 5.2131e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0799, 0.0221, 0.1025, 0.1063, 0.2586, 0.1704, 0.1281, 0.1066, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1139, 0.0308, 0.1086, 0.1024, 0.2047, 0.1524, 0.1369, 0.1150, 0.0352]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1013, 0.0247, 0.1003, 0.0941, 0.2439, 0.1566, 0.1228, 0.1185, 0.0378]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1052, 0.0227, 0.1109, 0.1034, 0.2232, 0.1575, 0.1170, 0.1232, 0.0370]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1076, 0.0342, 0.1073, 0.1022, 0.2175, 0.1379, 0.1468, 0.1081, 0.0386]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1300, 0.0199, 0.0981, 0.1081, 0.2442, 0.1508, 0.1144, 0.1129, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0982, 0.0401, 0.1048, 0.0976, 0.2231, 0.1396, 0.1315, 0.1162, 0.0489]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1020, 0.0351, 0.1094, 0.0973, 0.1898, 0.1650, 0.1243, 0.1286, 0.0486]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0715, 0.0044, 0.0771, 0.0924, 0.3027, 0.2382, 0.1270, 0.0796, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1091, 0.0262, 0.0895, 0.1027, 0.2236, 0.1629, 0.1305, 0.1253, 0.0303]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0860, 0.0165, 0.0950, 0.0920, 0.2505, 0.1919, 0.1283, 0.1094, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1473e-02, 3.6097e-04, 2.5544e-02, 6.2497e-02, 3.9972e-01, 3.4227e-01,\n",
       "           9.3792e-02, 4.2964e-02, 1.3820e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1023, 0.0287, 0.0897, 0.1129, 0.2052, 0.1818, 0.1316, 0.1136, 0.0341]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0116, 0.0873, 0.0886, 0.3048, 0.1936, 0.1345, 0.0851, 0.0156]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0820, 0.0104, 0.0971, 0.1084, 0.2319, 0.2197, 0.1354, 0.1010, 0.0139]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.7931e-03, 7.7134e-07, 1.9165e-03, 9.4313e-02, 1.9546e-01, 6.4798e-01,\n",
       "           4.5701e-02, 1.1830e-02, 1.6143e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1074, 0.0388, 0.0999, 0.0953, 0.2073, 0.1503, 0.1299, 0.1249, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.9926e-03, 8.9203e-07, 6.4528e-03, 6.8655e-02, 3.0564e-01, 5.5360e-01,\n",
       "           5.0974e-02, 1.1681e-02, 1.0795e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1199, 0.0207, 0.1091, 0.1056, 0.2126, 0.1614, 0.1268, 0.1176, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0859, 0.0060, 0.0896, 0.0936, 0.3138, 0.1739, 0.1177, 0.1087, 0.0109]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0647, 0.0073, 0.0857, 0.0734, 0.3632, 0.1645, 0.1277, 0.1008, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.1832e-03, 3.8668e-06, 8.0238e-03, 1.0945e-01, 2.0991e-01, 6.0686e-01,\n",
       "           4.2355e-02, 1.8199e-02, 1.0463e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0447, 0.0012, 0.0564, 0.0799, 0.2565, 0.3989, 0.1116, 0.0476, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0910, 0.0345, 0.1097, 0.1133, 0.1781, 0.1864, 0.1197, 0.1198, 0.0475]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1041, 0.0175, 0.1058, 0.0966, 0.2018, 0.1944, 0.1205, 0.1277, 0.0316]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0404, 0.0010, 0.0326, 0.1266, 0.2324, 0.4058, 0.0946, 0.0639, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0859, 0.0058, 0.1056, 0.0881, 0.2488, 0.2574, 0.1185, 0.0781, 0.0119]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0620, 0.0031, 0.0453, 0.1210, 0.2414, 0.3461, 0.1220, 0.0543, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.5385e-03, 3.2538e-06, 2.7868e-03, 8.8315e-02, 8.8349e-02, 7.8382e-01,\n",
       "           2.5610e-02, 7.5539e-03, 2.2208e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1083, 0.0129, 0.1104, 0.0986, 0.2079, 0.2245, 0.1277, 0.0889, 0.0209]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0973, 0.0081, 0.0951, 0.1134, 0.2204, 0.2235, 0.1156, 0.1087, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0529, 0.0042, 0.0698, 0.1448, 0.2025, 0.3266, 0.1106, 0.0811, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1059, 0.0296, 0.1001, 0.1185, 0.1698, 0.2046, 0.1193, 0.1103, 0.0420]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0612, 0.0056, 0.0899, 0.1213, 0.2204, 0.3134, 0.1000, 0.0783, 0.0100]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.5863e-02, 9.5941e-05, 1.2847e-02, 1.1690e-01, 8.6257e-02, 7.0125e-01,\n",
       "           3.7886e-02, 1.8558e-02, 3.5094e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.8516e-02, 4.6675e-04, 3.8391e-02, 1.1118e-01, 2.3426e-01, 4.9378e-01,\n",
       "           6.8653e-02, 3.3754e-02, 9.9404e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0427, 0.0010, 0.0415, 0.1075, 0.2132, 0.4515, 0.0851, 0.0543, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0639, 0.0039, 0.0647, 0.1084, 0.2331, 0.3476, 0.1100, 0.0619, 0.0065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0759, 0.0072, 0.0843, 0.1186, 0.1784, 0.3123, 0.1105, 0.0979, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0669, 0.0050, 0.0819, 0.1149, 0.2157, 0.3069, 0.1226, 0.0788, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[9.8415e-03, 2.4956e-05, 8.3376e-03, 1.1613e-01, 8.3731e-02, 7.1423e-01,\n",
       "           3.8752e-02, 2.8769e-02, 1.7953e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0775, 0.0138, 0.0885, 0.1363, 0.1913, 0.2534, 0.1087, 0.1061, 0.0244]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0258, 0.0007, 0.0360, 0.1198, 0.1083, 0.5989, 0.0657, 0.0434, 0.0013]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.5079e-03, 1.3220e-05, 5.1300e-03, 6.7412e-02, 4.2691e-02, 8.5199e-01,\n",
       "           2.3475e-02, 5.7262e-03, 5.6945e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.0444e-10, 7.1713e-18, 1.0387e-08, 6.2350e-05, 2.5873e-06, 9.9993e-01,\n",
       "           2.2927e-06, 2.7043e-08, 7.5059e-16]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.3887e-03, 2.1214e-06, 1.8871e-03, 5.4368e-02, 1.9224e-02, 9.0862e-01,\n",
       "           9.9172e-03, 3.5748e-03, 1.4472e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.0677e-03, 6.5503e-06, 2.6923e-03, 1.0706e-01, 3.2269e-02, 8.3219e-01,\n",
       "           1.5061e-02, 6.6265e-03, 3.4436e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.0769e-02, 4.5637e-05, 1.3453e-02, 7.0605e-02, 8.9452e-02, 7.6650e-01,\n",
       "           3.0379e-02, 1.8569e-02, 2.3133e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0595, 0.0036, 0.0619, 0.1607, 0.1371, 0.3992, 0.0901, 0.0807, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0597, 0.0058, 0.0884, 0.1346, 0.1748, 0.3503, 0.0958, 0.0815, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0329, 0.0013, 0.0413, 0.1183, 0.1452, 0.5466, 0.0668, 0.0447, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0311e-05, 2.2504e-10, 4.2295e-05, 3.0573e-02, 1.2043e-03, 9.6769e-01,\n",
       "           3.1589e-04, 1.5360e-04, 6.3623e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0998e-03, 3.3322e-07, 9.9497e-04, 3.7305e-02, 8.7372e-03, 9.3760e-01,\n",
       "           8.9852e-03, 4.2697e-03, 4.9726e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.4477e-02, 6.7981e-05, 1.1033e-02, 1.0739e-01, 8.0783e-02, 7.3643e-01,\n",
       "           3.5594e-02, 1.4048e-02, 1.7565e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[2.6062e-03, 8.7317e-06, 3.4537e-03, 1.1920e-01, 1.0175e-01, 7.5274e-01,\n",
       "           1.4403e-02, 5.8138e-03, 2.3115e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0956, 0.0177, 0.0891, 0.1390, 0.1702, 0.2504, 0.1185, 0.0962, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0878, 0.0184, 0.0819, 0.1199, 0.1863, 0.2860, 0.1082, 0.0877, 0.0239]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[1.3884e-02, 9.1714e-05, 1.4851e-02, 9.2978e-02, 8.9825e-02, 7.2477e-01,\n",
       "           3.2364e-02, 3.1019e-02, 2.2142e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0889, 0.0122, 0.0992, 0.1552, 0.1549, 0.2637, 0.1074, 0.0984, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0678, 0.0017, 0.0465, 0.1390, 0.1059, 0.4918, 0.0757, 0.0674, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0880, 0.0172, 0.0978, 0.1167, 0.1898, 0.2734, 0.1090, 0.0866, 0.0214]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0664, 0.0043, 0.0778, 0.1582, 0.1355, 0.3470, 0.1076, 0.0944, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.1102e-03, 3.6902e-06, 5.7054e-03, 8.4879e-02, 1.9997e-02, 8.6807e-01,\n",
       "           1.0963e-02, 6.2410e-03, 2.8610e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0044, 0.0728, 0.1476, 0.2165, 0.3461, 0.0963, 0.0620, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0260, 0.0005, 0.0236, 0.2170, 0.1107, 0.4953, 0.0757, 0.0503, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0760, 0.0071, 0.0692, 0.1618, 0.1886, 0.2763, 0.1071, 0.1006, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1049, 0.0353, 0.1085, 0.1148, 0.1838, 0.1859, 0.1180, 0.1115, 0.0372]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0989, 0.0049, 0.0919, 0.1223, 0.1992, 0.2821, 0.1006, 0.0903, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0418, 0.0011, 0.0343, 0.1372, 0.1247, 0.5458, 0.0694, 0.0424, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.0434e-04, 6.9120e-08, 4.9615e-04, 8.6754e-02, 7.7581e-03, 8.9701e-01,\n",
       "           4.2065e-03, 3.1731e-03, 1.3034e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0980, 0.0108, 0.1059, 0.1130, 0.2006, 0.2390, 0.1200, 0.0931, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1023, 0.0249, 0.1054, 0.1197, 0.1953, 0.1806, 0.1194, 0.1180, 0.0344]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1065, 0.0245, 0.1171, 0.1052, 0.1876, 0.1903, 0.1243, 0.1069, 0.0377]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.2891e-03, 9.2138e-07, 2.2220e-03, 1.7685e-01, 3.5359e-02, 7.6637e-01,\n",
       "           1.3373e-02, 3.5212e-03, 6.4389e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1156, 0.0227, 0.1192, 0.1147, 0.1734, 0.1992, 0.1079, 0.1125, 0.0348]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.9055e-04, 3.5681e-08, 3.5296e-04, 6.6401e-02, 1.7277e-02, 9.1192e-01,\n",
       "           2.5999e-03, 1.1568e-03, 4.5870e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0923, 0.0104, 0.0904, 0.1401, 0.1626, 0.2781, 0.1101, 0.0914, 0.0246]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1119, 0.0166, 0.1344, 0.0987, 0.2165, 0.1741, 0.1091, 0.1057, 0.0330]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0905, 0.0119, 0.1102, 0.1103, 0.2200, 0.2196, 0.1201, 0.0980, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0974, 0.0174, 0.1123, 0.1072, 0.2046, 0.2004, 0.1306, 0.0987, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0977, 0.0253, 0.1059, 0.1207, 0.1875, 0.1938, 0.1097, 0.1193, 0.0401]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.4275e-05, 3.0579e-10, 1.2933e-05, 1.5366e-02, 1.3090e-03, 9.8279e-01,\n",
       "           4.5582e-04, 3.1928e-05, 5.5483e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1012, 0.0064, 0.0806, 0.1239, 0.2296, 0.2703, 0.1006, 0.0756, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.0595e-02, 5.1301e-04, 2.4407e-02, 1.8251e-01, 1.0728e-01, 5.4869e-01,\n",
       "           7.7425e-02, 2.6856e-02, 1.7343e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0947, 0.0112, 0.1060, 0.1270, 0.2148, 0.2550, 0.1059, 0.0691, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0767, 0.0048, 0.0586, 0.1772, 0.1474, 0.3645, 0.0936, 0.0681, 0.0092]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.7302e-03, 1.2483e-05, 6.1699e-03, 1.2581e-01, 1.0991e-01, 7.2828e-01,\n",
       "           2.0612e-02, 5.4066e-03, 7.4213e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0286, 0.1057, 0.1244, 0.1925, 0.2050, 0.1274, 0.0901, 0.0337]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0476, 0.0011, 0.0357, 0.1753, 0.1493, 0.4538, 0.0736, 0.0596, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0615, 0.0030, 0.0590, 0.1332, 0.1977, 0.3816, 0.0933, 0.0634, 0.0075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0745, 0.0038, 0.0582, 0.1569, 0.1946, 0.3484, 0.0922, 0.0655, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0393, 0.0008, 0.0313, 0.2029, 0.1268, 0.4891, 0.0557, 0.0510, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8835e-05, 2.2652e-11, 1.2516e-05, 5.2833e-02, 1.7567e-03, 9.4496e-01,\n",
       "           3.6673e-04, 5.2643e-05, 2.9691e-10]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0285, 0.0005, 0.0215, 0.2284, 0.1090, 0.5140, 0.0655, 0.0308, 0.0017]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.4803e-03, 6.8495e-05, 5.3047e-03, 2.2135e-01, 6.2316e-02, 6.7630e-01,\n",
       "           1.9128e-02, 1.0881e-02, 1.7490e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0823, 0.0138, 0.0877, 0.1367, 0.1824, 0.2584, 0.1185, 0.0972, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0249, 0.1013, 0.1394, 0.1533, 0.2303, 0.1118, 0.1013, 0.0368]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0748, 0.0148, 0.0756, 0.1519, 0.1888, 0.2677, 0.1132, 0.0916, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0144, 0.0894, 0.1526, 0.1733, 0.2665, 0.1187, 0.0905, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.4722e-02, 5.6117e-04, 2.7263e-02, 1.6292e-01, 1.3630e-01, 5.7504e-01,\n",
       "           4.1107e-02, 3.0220e-02, 1.8711e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0100, 0.0847, 0.1648, 0.1829, 0.2798, 0.1026, 0.0837, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.1835e-02, 3.2765e-04, 1.8945e-02, 2.9743e-01, 9.6197e-02, 4.9233e-01,\n",
       "           4.5319e-02, 2.6783e-02, 8.3265e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0731, 0.0104, 0.0799, 0.1666, 0.1622, 0.3226, 0.0876, 0.0784, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[6.1646e-03, 4.6419e-05, 6.8653e-03, 2.6123e-01, 6.9545e-02, 6.2841e-01,\n",
       "           1.9535e-02, 8.0310e-03, 1.7041e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0602, 0.0071, 0.0730, 0.1485, 0.2006, 0.3394, 0.0886, 0.0674, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.8084e-02, 4.4121e-04, 3.1435e-02, 2.0229e-01, 1.5840e-01, 4.9253e-01,\n",
       "           5.1216e-02, 3.4356e-02, 1.2547e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0324, 0.0014, 0.0414, 0.1923, 0.1490, 0.4790, 0.0548, 0.0464, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0323, 0.0014, 0.0271, 0.2367, 0.1668, 0.4393, 0.0581, 0.0350, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.8600e-03, 1.0620e-05, 4.4141e-03, 2.0484e-01, 5.5908e-02, 7.0997e-01,\n",
       "           1.2309e-02, 8.6411e-03, 4.3176e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[6.0688e-03, 6.5275e-05, 1.1643e-02, 1.7902e-01, 8.3932e-02, 6.6886e-01,\n",
       "           3.3561e-02, 1.6514e-02, 3.3803e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0412, 0.0015, 0.0502, 0.1829, 0.2399, 0.3693, 0.0632, 0.0467, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2636e-02, 2.6852e-04, 1.2798e-02, 3.3461e-01, 1.2990e-01, 4.6446e-01,\n",
       "           2.1808e-02, 2.2880e-02, 6.3752e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.9411e-02, 3.5399e-04, 1.1088e-02, 2.2676e-01, 6.7245e-02, 6.0513e-01,\n",
       "           4.8413e-02, 2.0955e-02, 6.4775e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.8343e-03, 4.5441e-06, 3.7663e-03, 2.8216e-01, 2.1070e-02, 6.7757e-01,\n",
       "           9.7356e-03, 3.8480e-03, 9.9684e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0105, 0.0872, 0.1682, 0.1864, 0.2849, 0.0933, 0.0804, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0644, 0.0106, 0.0701, 0.1930, 0.1720, 0.2824, 0.0987, 0.0872, 0.0215]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0478, 0.0026, 0.0502, 0.1787, 0.1921, 0.3732, 0.0896, 0.0601, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0566, 0.0056, 0.0639, 0.1912, 0.1543, 0.3559, 0.0910, 0.0718, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.0984e-02, 3.9326e-04, 2.9128e-02, 1.8745e-01, 1.5587e-01, 5.3281e-01,\n",
       "           5.0279e-02, 2.1983e-02, 1.0968e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0763, 0.0182, 0.0782, 0.1866, 0.1624, 0.2515, 0.1162, 0.0853, 0.0252]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.3975e-08, 8.5826e-17, 1.3355e-08, 3.6971e-02, 6.5444e-06, 9.6302e-01,\n",
       "           1.5335e-06, 7.1980e-08, 1.2645e-14]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0852, 0.0153, 0.0892, 0.1575, 0.1917, 0.2471, 0.1004, 0.0880, 0.0256]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.5030e-03, 6.3248e-05, 1.1026e-02, 1.8980e-01, 7.3889e-02, 6.6412e-01,\n",
       "           2.9785e-02, 2.3600e-02, 2.1140e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.1595e-04, 1.8085e-08, 9.5959e-05, 1.0947e-01, 1.1182e-02, 8.7767e-01,\n",
       "           1.2545e-03, 2.1418e-04, 6.6928e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0770, 0.0061, 0.0607, 0.1781, 0.1608, 0.3532, 0.0801, 0.0736, 0.0104]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.4726e-04, 1.4753e-07, 9.9070e-04, 1.6566e-01, 1.4706e-02, 8.0544e-01,\n",
       "           1.0052e-02, 2.6990e-03, 2.6216e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.3584e-03, 2.0956e-05, 6.4765e-03, 2.1117e-01, 7.9289e-02, 6.5709e-01,\n",
       "           2.8121e-02, 1.1342e-02, 1.3634e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[9.4993e-04, 1.4701e-07, 9.3432e-04, 2.1226e-01, 1.5681e-02, 7.6726e-01,\n",
       "           2.1420e-03, 7.7323e-04, 8.5606e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0800, 0.0063, 0.1000, 0.1383, 0.2353, 0.2372, 0.0949, 0.0929, 0.0151]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1103, 0.0313, 0.1097, 0.1252, 0.1997, 0.1782, 0.1139, 0.0931, 0.0386]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0674, 0.0024, 0.0701, 0.1442, 0.2412, 0.3390, 0.0799, 0.0502, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1084, 0.0321, 0.1118, 0.1134, 0.1867, 0.1789, 0.1122, 0.1115, 0.0450]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.0090e-03, 1.0226e-05, 4.5788e-03, 1.3396e-01, 4.4911e-02, 7.8960e-01,\n",
       "           1.7192e-02, 4.6910e-03, 4.8637e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1116, 0.0213, 0.1091, 0.1105, 0.2228, 0.1866, 0.1214, 0.0901, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.0156e-02, 1.5859e-05, 8.4047e-03, 2.3805e-01, 1.0142e-01, 6.0195e-01,\n",
       "           2.3603e-02, 1.6301e-02, 1.0082e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1142, 0.0407, 0.1081, 0.1129, 0.1899, 0.1655, 0.1123, 0.1082, 0.0481]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0969, 0.0257, 0.0933, 0.1391, 0.1658, 0.2187, 0.1135, 0.1085, 0.0385]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.6885e-02, 8.0813e-05, 1.1198e-02, 2.2169e-01, 1.4521e-01, 5.3291e-01,\n",
       "           5.2568e-02, 1.9218e-02, 2.4215e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1308, 0.0453, 0.1123, 0.1042, 0.1874, 0.1430, 0.1096, 0.1059, 0.0614]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[3.7562e-02, 4.0708e-04, 2.5219e-02, 1.6415e-01, 1.0293e-01, 5.9702e-01,\n",
       "           4.4986e-02, 2.6374e-02, 1.3527e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.6135e-02, 1.1230e-04, 9.9799e-03, 1.9414e-01, 3.9284e-02, 6.9766e-01,\n",
       "           2.8173e-02, 1.4284e-02, 2.2789e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[8.7449e-05, 1.0089e-09, 6.0170e-05, 4.6628e-02, 3.7353e-03, 9.4876e-01,\n",
       "           5.8848e-04, 1.4144e-04, 6.8248e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0990, 0.0210, 0.1035, 0.1240, 0.2146, 0.1947, 0.1051, 0.1018, 0.0364]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0682e-02, 1.5268e-04, 1.6666e-02, 2.2732e-01, 8.4266e-02, 5.7524e-01,\n",
       "           3.9840e-02, 3.5054e-02, 7.7801e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1152, 0.0510, 0.1121, 0.1078, 0.1682, 0.1541, 0.1177, 0.1125, 0.0612]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0787, 0.0067, 0.0843, 0.1309, 0.2153, 0.2919, 0.1072, 0.0717, 0.0135]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1095, 0.0217, 0.0939, 0.1220, 0.1810, 0.2245, 0.1176, 0.0974, 0.0325]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.6393e-08, 2.0252e-17, 1.0312e-08, 4.4779e-03, 6.8078e-06, 9.9551e-01,\n",
       "           4.5188e-06, 8.9195e-08, 7.5604e-15]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[7.9107e-04, 2.9215e-07, 1.0523e-03, 2.1543e-01, 3.6378e-02, 7.3537e-01,\n",
       "           9.8032e-03, 1.1776e-03, 2.0309e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1165, 0.0279, 0.1145, 0.1044, 0.1898, 0.1823, 0.1165, 0.1069, 0.0411]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1025, 0.0170, 0.0985, 0.1038, 0.2261, 0.2123, 0.1038, 0.1042, 0.0318]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0062, 0.0688, 0.1384, 0.1629, 0.3662, 0.1097, 0.0665, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1223, 0.0398, 0.1158, 0.1046, 0.1719, 0.1648, 0.1119, 0.1110, 0.0580]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0720, 0.0063, 0.0712, 0.1394, 0.1626, 0.3484, 0.1112, 0.0758, 0.0131]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0550, 0.0016, 0.0497, 0.1734, 0.1453, 0.4565, 0.0694, 0.0455, 0.0036]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1102, 0.0470, 0.1072, 0.1123, 0.1582, 0.1661, 0.1210, 0.1175, 0.0604]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1042, 0.0129, 0.1061, 0.1161, 0.1834, 0.2613, 0.0972, 0.0955, 0.0234]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0495, 0.0010, 0.0336, 0.1784, 0.1146, 0.5267, 0.0573, 0.0349, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0048, 0.0557, 0.1652, 0.1365, 0.4045, 0.0998, 0.0636, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1153, 0.0128, 0.1055, 0.1125, 0.1849, 0.2492, 0.0963, 0.1032, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " ...]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn = GCN()\n",
    "# gcn.load_state_dict(torch.load('gcn_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 321 verilog files \n",
    "* only 3 features             [type, operation_type, num_of_connections]\n",
    "* no edge attribute\n",
    "* 18 classes \n",
    "* 200 epochs \n",
    "* learning rate = 0.01\n",
    "* Dropoout = 0.4\n",
    "* Adam Optimizer\n",
    "* train 70, test 30 (on whole dataset, not each class)\n",
    "* time of training = seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train acc:  0.2902\n",
    "* Test Acc: 0.1959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Clean dataset (by removing unnecessay, uninformative or wrong code files)\n",
    "2) remove reduntant parsing (different files but same parsing)\n",
    "3) include more informative features\n",
    "4) improve encoding format\n",
    "5) try using less classes (most important ones, so that less classes but more balanced dataset)\n",
    "6) adding more files\n",
    "7) adjusting hyperparameters such as learning rate, dropout, ...etc\n",
    "8) splitting train, val, test\n",
    "9) using equal percentages of each class (adjusting splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "droput 0.4\n",
    "\n",
    "314 files\n",
    "\n",
    "17 features (node_type)\n",
    "\n",
    "16 classes\n",
    "\n",
    "conv relu conv relu conv relu conv linear\n",
    "\n",
    "train = 40, test = 27\n",
    "\n",
    "200 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as 5 but 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = 43, test = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "conv relu conv relu conv dropout linear\n",
    "\n",
    "9 classes\n",
    "\n",
    "164 file\n",
    "\n",
    "train = 34, test = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "9 classes\n",
    "\n",
    "conv relu conv relu conv dropout linear \n",
    "\n",
    "train = 64, test = 52\n",
    "\n",
    "164 \n",
    "\n",
    "17 features (node type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
