{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "!pip install torch_geometric -q\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "from torch_geometric.utils import to_dense_adj, add_self_loops\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DATA_PATH = \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['done\\\\adder11.txt', 'done\\\\adder12.txt', 'done\\\\adder13.txt', 'done\\\\adder14.txt', 'done\\\\adder15.txt', 'done\\\\adder16.txt', 'done\\\\adder17.txt', 'done\\\\adder18.txt', 'done\\\\adder19.txt', 'done\\\\adder2.txt', 'done\\\\adder20.txt', 'done\\\\adder5.txt', 'done\\\\adder6.txt', 'done\\\\adder8.txt', 'done\\\\ALU10.txt', 'done\\\\ALU13.txt', 'done\\\\ALU14.txt', 'done\\\\ALU15.txt', 'done\\\\ALU2.txt', 'done\\\\ALU6.txt', 'done\\\\ALU7.txt', 'done\\\\ALU8.txt', 'done\\\\ALU9.txt', 'done\\\\and1.txt', 'done\\\\and10.txt', 'done\\\\and12.txt', 'done\\\\and13.txt', 'done\\\\and14.txt', 'done\\\\and15.txt', 'done\\\\and16.txt', 'done\\\\and17.txt', 'done\\\\and18.txt', 'done\\\\and19.txt', 'done\\\\and2.txt', 'done\\\\and20.txt', 'done\\\\and21.txt', 'done\\\\and23.txt', 'done\\\\and25.txt', 'done\\\\and26.txt', 'done\\\\and27.txt', 'done\\\\and28.txt', 'done\\\\and29.txt', 'done\\\\and3.txt', 'done\\\\and30.txt', 'done\\\\and6.txt', 'done\\\\and7.txt', 'done\\\\and8.txt', 'done\\\\and9.txt', 'done\\\\comparator1.txt', 'done\\\\comparator13.txt', 'done\\\\comparator14.txt', 'done\\\\comparator15.txt', 'done\\\\comparator16.txt', 'done\\\\comparator17.txt', 'done\\\\comparator18.txt', 'done\\\\comparator19.txt', 'done\\\\comparator2.txt', 'done\\\\comparator20.txt', 'done\\\\comparator21.txt', 'done\\\\comparator22.txt', 'done\\\\comparator23.txt', 'done\\\\comparator6.txt', 'done\\\\comparator7.txt', 'done\\\\comparator8.txt', 'done\\\\comparator9.txt', 'done\\\\decoder1.txt', 'done\\\\decoder11.txt', 'done\\\\decoder12.txt', 'done\\\\decoder13.txt', 'done\\\\decoder14.txt', 'done\\\\decoder15.txt', 'done\\\\decoder16.txt', 'done\\\\decoder17.txt', 'done\\\\decoder18.txt', 'done\\\\decoder19.txt', 'done\\\\decoder2.txt', 'done\\\\decoder20.txt', 'done\\\\decoder21.txt', 'done\\\\decoder22.txt', 'done\\\\decoder23.txt', 'done\\\\decoder24.txt', 'done\\\\decoder25.txt', 'done\\\\decoder26.txt', 'done\\\\decoder27.txt', 'done\\\\decoder28.txt', 'done\\\\decoder29.txt', 'done\\\\decoder3.txt', 'done\\\\decoder30.txt', 'done\\\\decoder31.txt', 'done\\\\decoder32.txt', 'done\\\\decoder4.txt', 'done\\\\encoder1.txt', 'done\\\\encoder10.txt', 'done\\\\encoder11.txt', 'done\\\\encoder12.txt', 'done\\\\encoder13.txt', 'done\\\\encoder14.txt', 'done\\\\encoder15.txt', 'done\\\\encoder16.txt', 'done\\\\encoder17.txt', 'done\\\\encoder18.txt', 'done\\\\encoder19.txt', 'done\\\\encoder2.txt', 'done\\\\encoder20.txt', 'done\\\\encoder21.txt', 'done\\\\encoder24.txt', 'done\\\\encoder25.txt', 'done\\\\encoder3.txt', 'done\\\\encoder4.txt', 'done\\\\encoder5.txt', 'done\\\\encoder6.txt', 'done\\\\encoder7.txt', 'done\\\\encoder8.txt', 'done\\\\encoder9.txt', 'done\\\\mult1.txt', 'done\\\\mult10.txt', 'done\\\\mult11.txt', 'done\\\\mult12.txt', 'done\\\\mult13.txt', 'done\\\\mult14.txt', 'done\\\\mult15.txt', 'done\\\\mult16.txt', 'done\\\\mult17.txt', 'done\\\\mult18.txt', 'done\\\\mult19.txt', 'done\\\\mult2.txt', 'done\\\\mult3.txt', 'done\\\\mult30.txt', 'done\\\\mult31.txt', 'done\\\\mult32.txt', 'done\\\\mult33.txt', 'done\\\\mult34.txt', 'done\\\\mult35.txt', 'done\\\\mult4.txt', 'done\\\\mult5.txt', 'done\\\\mult6.txt', 'done\\\\mult8.txt', 'done\\\\mult9.txt', 'done\\\\mux1.txt', 'done\\\\mux10.txt', 'done\\\\mux11.txt', 'done\\\\mux12.txt', 'done\\\\mux13.txt', 'done\\\\mux14.txt', 'done\\\\mux15.txt', 'done\\\\mux16.txt', 'done\\\\mux17.txt', 'done\\\\mux18.txt', 'done\\\\mux19.txt', 'done\\\\mux2.txt', 'done\\\\mux20.txt', 'done\\\\mux21.txt', 'done\\\\mux22.txt', 'done\\\\mux23.txt', 'done\\\\mux24.txt', 'done\\\\mux25.txt', 'done\\\\mux26.txt', 'done\\\\mux3.txt', 'done\\\\mux4.txt', 'done\\\\mux5.txt', 'done\\\\mux6.txt', 'done\\\\mux7.txt', 'done\\\\mux8.txt', 'done\\\\mux9.txt', 'done\\\\nand1.txt', 'done\\\\nand10.txt', 'done\\\\nand11.txt', 'done\\\\nand12.txt', 'done\\\\nand14.txt', 'done\\\\nand15.txt', 'done\\\\nand16.txt', 'done\\\\nand17.txt', 'done\\\\nand18.txt', 'done\\\\nand19.txt', 'done\\\\nand2.txt', 'done\\\\nand20.txt', 'done\\\\nand21.txt', 'done\\\\nand22.txt', 'done\\\\nand3.txt', 'done\\\\nand4.txt', 'done\\\\nand6.txt', 'done\\\\nand7.txt', 'done\\\\nand8.txt', 'done\\\\nand9.txt', 'done\\\\nor1.txt', 'done\\\\nor10.txt', 'done\\\\nor11.txt', 'done\\\\nor12.txt', 'done\\\\nor13.txt', 'done\\\\nor14.txt', 'done\\\\nor15.txt', 'done\\\\nor16.txt', 'done\\\\nor17.txt', 'done\\\\nor2.txt', 'done\\\\nor3.txt', 'done\\\\nor4.txt', 'done\\\\nor6.txt', 'done\\\\nor7.txt', 'done\\\\nor8.txt', 'done\\\\nor9.txt', 'done\\\\not1.txt', 'done\\\\not10.txt', 'done\\\\not11.txt', 'done\\\\not12.txt', 'done\\\\not13.txt', 'done\\\\not14.txt', 'done\\\\not2.txt', 'done\\\\not3.txt', 'done\\\\not6.txt', 'done\\\\not7.txt', 'done\\\\not9.txt', 'done\\\\or1.txt', 'done\\\\or10.txt', 'done\\\\or11.txt', 'done\\\\or12.txt', 'done\\\\or13.txt', 'done\\\\or14.txt', 'done\\\\or15.txt', 'done\\\\or16.txt', 'done\\\\or17.txt', 'done\\\\or18.txt', 'done\\\\or19.txt', 'done\\\\or2.txt', 'done\\\\or20.txt', 'done\\\\or21.txt', 'done\\\\or22.txt', 'done\\\\or23.txt', 'done\\\\or24.txt', 'done\\\\or25.txt', 'done\\\\or26.txt', 'done\\\\or28.txt', 'done\\\\or4.txt', 'done\\\\or7.txt', 'done\\\\or8.txt', 'done\\\\or9.txt', 'done\\\\pe1.txt', 'done\\\\pe14.txt', 'done\\\\pe15.txt', 'done\\\\pe16.txt', 'done\\\\pe17.txt', 'done\\\\pe18.txt', 'done\\\\pe19.txt', 'done\\\\pe2.txt', 'done\\\\pe20.txt', 'done\\\\pe21.txt', 'done\\\\pe22.txt', 'done\\\\pe3.txt', 'done\\\\pe4.txt', 'done\\\\pe5.txt', 'done\\\\pe6.txt', 'done\\\\pe7.txt', 'done\\\\sub1.txt', 'done\\\\sub10.txt', 'done\\\\sub11.txt', 'done\\\\sub12.txt', 'done\\\\sub2.txt', 'done\\\\sub5.txt', 'done\\\\sub7.txt', 'done\\\\sub8.txt', 'done\\\\sub9.txt', 'done\\\\xnor1.txt', 'done\\\\xnor10.txt', 'done\\\\xnor11.txt', 'done\\\\xnor12.txt', 'done\\\\xnor13.txt', 'done\\\\xnor14.txt', 'done\\\\xnor15.txt', 'done\\\\xnor16.txt', 'done\\\\xnor17.txt', 'done\\\\xnor18.txt', 'done\\\\xnor19.txt', 'done\\\\xnor2.txt', 'done\\\\xnor20.txt', 'done\\\\xnor21.txt', 'done\\\\xnor22.txt', 'done\\\\xnor23.txt', 'done\\\\xnor24.txt', 'done\\\\xnor25.txt', 'done\\\\xnor4.txt', 'done\\\\xnor6.txt', 'done\\\\xnor7.txt', 'done\\\\xnor8.txt', 'done\\\\xnor9.txt', 'done\\\\xor1.txt', 'done\\\\xor10.txt', 'done\\\\xor12.txt', 'done\\\\xor13.txt', 'done\\\\xor14.txt', 'done\\\\xor15.txt', 'done\\\\xor16.txt', 'done\\\\xor17.txt', 'done\\\\xor18.txt', 'done\\\\xor19.txt', 'done\\\\xor2.txt', 'done\\\\xor20.txt', 'done\\\\xor21.txt', 'done\\\\xor22.txt', 'done\\\\xor23.txt', 'done\\\\xor24.txt', 'done\\\\xor25.txt', 'done\\\\xor26.txt', 'done\\\\xor3.txt', 'done\\\\xor6.txt', 'done\\\\xor7.txt', 'done\\\\xor8.txt', 'done\\\\xor9.txt']\n"
     ]
    }
   ],
   "source": [
    "def get_files_in_folder(input_folder):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'done'\n",
    "verilog_files = get_files_in_folder(folder_path)\n",
    "print(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Shuffle the dataset in-place\n",
    "# random.shuffle(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                # edge_atr = loaded_data[2]\n",
    "                label = loaded_data[3]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                # edge_atr = torch.tensor(edge_atr, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                num_nodes = x.size(0)\n",
    "                # print(num_nodes)\n",
    "                \n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                # data = Data(x=x, edge_index=edge_index, edge_attr=edge_atr ,y = y, batch = batch)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "# temp=extracting_attributes(\"./done/adder6.txt\")\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 306 Verilog files.\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[165, 22], edge_index=[2, 208], y=[1, 16], batch=[165])\n",
      "done\\adder11.txt\n",
      "done\\adder11.txt\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(verilog_files[0])\n",
    "print(dataset.verilog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data objects are unique.\n"
     ]
    }
   ],
   "source": [
    "def are_all_data_objects_unique(dataset):\n",
    "    data_objects = []\n",
    "    for data in dataset:\n",
    "        if data in data_objects:\n",
    "            return False\n",
    "        data_objects.append(data)\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "is_unique = are_all_data_objects_unique(dataset)\n",
    "if is_unique:\n",
    "    print(\"All data objects are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate data objects found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done\\\\comparator20.txt'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = random.randint(0, len(verilog_files))\n",
    "verilog_files[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset[num]\n",
    "data\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(range(data.num_nodes))\n",
    "edges = data.edge_index.t().tolist()\n",
    "# edge_attrs = {tuple(edge): attr.item() for edge, attr in zip(edges, data.edge_attr)}\n",
    "g.add_edges_from(edges)\n",
    "print(len(edges))\n",
    "\n",
    "# Draw the graph with edge attributes\n",
    "pos = nx.spring_layout(g)  # positions for all nodes\n",
    "nx.draw(g, pos, with_labels=True, node_color='skyblue', node_size=1500, edge_color='k', linewidths=1, font_size=15)\n",
    "nx.draw_networkx_edge_labels(g, pos, font_color='red', font_size=12)  # Add edge labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_num_nodes: 3\n",
      "max_num_nodes: 165\n",
      "mean_num_nodes: 14.683006535947712\n",
      "min_num_edges: 2\n",
      "max_num_edges: 208\n",
      "mean_num_edges: 17.915032679738562\n",
      "mean nodes degree: 1.2201201869574896\n"
     ]
    }
   ],
   "source": [
    "def graph_stat(dataset):\n",
    "    \"\"\"\n",
    "    TODO: calculate the statistics of the ENZYMES dataset.\n",
    "    \n",
    "    Outputs:\n",
    "        min_num_nodes: min number of nodes\n",
    "        max_num_nodes: max number of nodes\n",
    "        mean_num_nodes: average number of nodes\n",
    "        min_num_edges: min number of edges\n",
    "        max_num_edges: max number of edges\n",
    "        mean_num_edges: average number of edges\n",
    "    \"\"\"\n",
    "    # for ind,data in enumerate(dataset):\n",
    "        # print(verilog_files[ind])\n",
    "        # print(data)\n",
    "        # print(len(data.x[1]))\n",
    "        \n",
    "    nodes_edges = [(data.num_nodes, data.num_edges) for data in dataset]\n",
    "    num_nodes, num_edges = list(list(zip(*nodes_edges))[0]), list(list(zip(*nodes_edges))[1])\n",
    "    min_num_nodes = min(num_nodes)\n",
    "    max_num_nodes = max(num_nodes)\n",
    "    mean_num_nodes = np.mean(num_nodes)\n",
    "    min_num_edges = min(num_edges)\n",
    "    max_num_edges = max(num_edges)\n",
    "    mean_num_edges = np.mean(num_edges)\n",
    "    mean_degree = (mean_num_edges)/mean_num_nodes\n",
    "    \n",
    "    print(f\"min_num_nodes: {min_num_nodes}\")\n",
    "    print(f\"max_num_nodes: {max_num_nodes}\")\n",
    "    print(f\"mean_num_nodes: {mean_num_nodes}\")\n",
    "    print(f\"min_num_edges: {min_num_edges}\")\n",
    "    print(f\"max_num_edges: {max_num_edges}\")\n",
    "    print(f\"mean_num_edges: {mean_num_edges}\")\n",
    "    print(f\"mean nodes degree: {mean_degree}\")\n",
    "\n",
    "graph_stat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "# # Define the sizes of training, validation, and test sets\n",
    "# train_size = int(0.7 * len(dataset))  # 70% of the data for training\n",
    "# val_size = int(0.15 * len(dataset))   # 15% of the data for validation\n",
    "# test_size = len(dataset) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create DataLoader for each set\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# Define the size of the training set (e.g., 70% of the data)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "\n",
    "# Calculate the size of the testing set\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[29, 22], edge_index=[2, 50], y=[1, 16], batch=[29])\n"
     ]
    }
   ],
   "source": [
    "# len(train_loader.dataset)\n",
    "print(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "batch = next(loader_iter)\n",
    "# print(batch)\n",
    "# print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCNConv(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(GCNConv, self).__init__()\n",
    "#         self.theta = nn.Parameter(torch.FloatTensor(in_channels, out_channels))\n",
    "#         # Initialize the parameters.\n",
    "#         stdv = 1. / math.sqrt(out_channels)\n",
    "#         self.theta.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "#     def forward(self, x, edge_index):\n",
    "#         \"\"\"\n",
    "#         TODO:\n",
    "#             1. Generate the adjacency matrix with self-loop \\hat{A} using edge_index.\n",
    "#             2. Calculate the diagonal degree matrix \\hat{D}.\n",
    "#             3. Calculate the output X' with torch.mm using the equation above.\n",
    "#         \"\"\"\n",
    "\n",
    "#         num_nodes = x.shape[0]\n",
    "#         A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (num_nodes, num_nodes))\n",
    "#         A = A.to_dense()\n",
    "#         A_hat = A + torch.eye(num_nodes)\n",
    "        \n",
    "#         A_sum = torch.sum(A_hat, dim=1)\n",
    "#         D = torch.pow(A_sum, -0.5)\n",
    "#         D[D == float('inf')] = 0.0\n",
    "#         D_hat_sqrt = torch.diag(D)\n",
    "        \n",
    "#         first = torch.mm(torch.mm(D_hat_sqrt, A_hat), D_hat_sqrt)\n",
    "#         second = torch.mm(x, self.theta)\n",
    "        \n",
    "#         ret = torch.mm(first, second)\n",
    "        \n",
    "#         return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gcn1): GCNConv(22, 64)\n",
       "  (r1): ReLU()\n",
       "  (gcn2): GCNConv(64, 64)\n",
       "  (r2): ReLU()\n",
       "  (gcn3): GCNConv(64, 128)\n",
       "  (r3): ReLU()\n",
       "  (gcn4): GCNConv(128, 128)\n",
       "  (linear): Linear(in_features=128, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Define the first convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            2. Define the first activation layer using `nn.ReLU()`;\n",
    "            3. Define the second convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            4. Define the second activation layer using `nn.ReLU()`;\n",
    "            5. Define the third convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            6. Define the dropout layer using `nn.Dropout()`;\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        num_node_features = 22\n",
    "        num_output_classes = 16\n",
    "        \n",
    "        # num_channels = 32\n",
    "        \n",
    "        self.gcn1 = GCNConv(num_node_features, 64)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.gcn2 = GCNConv(64, 64)\n",
    "        self.r2 = nn.ReLU()\n",
    "        self.gcn3 = GCNConv(64, 128)\n",
    "        self.r3 = nn.ReLU()\n",
    "        self.gcn4 = GCNConv(128, 128)\n",
    "        self.linear = nn.Linear(in_features=128, out_features=num_output_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "    \n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.r1(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = self.r2(x)\n",
    "        x = self.gcn3(x, edge_index)\n",
    "        x = self.r3(x)\n",
    "        x = self.gcn4(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p = 0.5, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        probs = F.log_softmax(x, dim=-1)\n",
    "        \n",
    "        return probs\n",
    "        \n",
    "        \n",
    "        \n",
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.1402, Test Acc: 0.0435\n",
      "Epoch: 002, Train Acc: 0.1215, Test Acc: 0.0652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Acc: 0.1449, Test Acc: 0.0870\n",
      "Epoch: 004, Train Acc: 0.1495, Test Acc: 0.0870\n",
      "Epoch: 005, Train Acc: 0.1729, Test Acc: 0.0761\n",
      "Epoch: 006, Train Acc: 0.1963, Test Acc: 0.0978\n",
      "Epoch: 007, Train Acc: 0.1822, Test Acc: 0.0761\n",
      "Epoch: 008, Train Acc: 0.2523, Test Acc: 0.0870\n",
      "Epoch: 009, Train Acc: 0.2430, Test Acc: 0.2174\n",
      "Epoch: 010, Train Acc: 0.2243, Test Acc: 0.1413\n",
      "Epoch: 011, Train Acc: 0.2430, Test Acc: 0.1522\n",
      "Epoch: 012, Train Acc: 0.2430, Test Acc: 0.1413\n",
      "Epoch: 013, Train Acc: 0.2710, Test Acc: 0.2065\n",
      "Epoch: 014, Train Acc: 0.2757, Test Acc: 0.2391\n",
      "Epoch: 015, Train Acc: 0.2757, Test Acc: 0.2609\n",
      "Epoch: 016, Train Acc: 0.3318, Test Acc: 0.2935\n",
      "Epoch: 017, Train Acc: 0.3178, Test Acc: 0.2826\n",
      "Epoch: 018, Train Acc: 0.3318, Test Acc: 0.2717\n",
      "Epoch: 019, Train Acc: 0.3224, Test Acc: 0.3043\n",
      "Epoch: 020, Train Acc: 0.3785, Test Acc: 0.3478\n",
      "Epoch: 021, Train Acc: 0.3832, Test Acc: 0.3804\n",
      "Epoch: 022, Train Acc: 0.3598, Test Acc: 0.4130\n",
      "Epoch: 023, Train Acc: 0.3131, Test Acc: 0.4130\n",
      "Epoch: 024, Train Acc: 0.3972, Test Acc: 0.3804\n",
      "Epoch: 025, Train Acc: 0.4159, Test Acc: 0.4239\n",
      "Epoch: 026, Train Acc: 0.4393, Test Acc: 0.4674\n",
      "Epoch: 027, Train Acc: 0.4393, Test Acc: 0.3804\n",
      "Epoch: 028, Train Acc: 0.3972, Test Acc: 0.3913\n",
      "Epoch: 029, Train Acc: 0.4579, Test Acc: 0.4239\n",
      "Epoch: 030, Train Acc: 0.4393, Test Acc: 0.3913\n",
      "Epoch: 031, Train Acc: 0.4720, Test Acc: 0.4239\n",
      "Epoch: 032, Train Acc: 0.4299, Test Acc: 0.3587\n",
      "Epoch: 033, Train Acc: 0.4579, Test Acc: 0.3696\n",
      "Epoch: 034, Train Acc: 0.5047, Test Acc: 0.4348\n",
      "Epoch: 035, Train Acc: 0.4813, Test Acc: 0.4239\n",
      "Epoch: 036, Train Acc: 0.4533, Test Acc: 0.4565\n",
      "Epoch: 037, Train Acc: 0.4673, Test Acc: 0.4674\n",
      "Epoch: 038, Train Acc: 0.4720, Test Acc: 0.3913\n",
      "Epoch: 039, Train Acc: 0.5234, Test Acc: 0.4239\n",
      "Epoch: 040, Train Acc: 0.4159, Test Acc: 0.4130\n",
      "Epoch: 041, Train Acc: 0.5187, Test Acc: 0.4457\n",
      "Epoch: 042, Train Acc: 0.5093, Test Acc: 0.5109\n",
      "Epoch: 043, Train Acc: 0.4860, Test Acc: 0.4783\n",
      "Epoch: 044, Train Acc: 0.5093, Test Acc: 0.4348\n",
      "Epoch: 045, Train Acc: 0.5421, Test Acc: 0.5109\n",
      "Epoch: 046, Train Acc: 0.5140, Test Acc: 0.4239\n",
      "Epoch: 047, Train Acc: 0.5327, Test Acc: 0.4457\n",
      "Epoch: 048, Train Acc: 0.5701, Test Acc: 0.4565\n",
      "Epoch: 049, Train Acc: 0.5280, Test Acc: 0.4783\n",
      "Epoch: 050, Train Acc: 0.5234, Test Acc: 0.4239\n",
      "Epoch: 051, Train Acc: 0.5654, Test Acc: 0.4239\n",
      "Epoch: 052, Train Acc: 0.5748, Test Acc: 0.4674\n",
      "Epoch: 053, Train Acc: 0.4393, Test Acc: 0.4565\n",
      "Epoch: 054, Train Acc: 0.5654, Test Acc: 0.4891\n",
      "Epoch: 055, Train Acc: 0.5935, Test Acc: 0.4674\n",
      "Epoch: 056, Train Acc: 0.5935, Test Acc: 0.4783\n",
      "Epoch: 057, Train Acc: 0.5981, Test Acc: 0.4674\n",
      "Epoch: 058, Train Acc: 0.5701, Test Acc: 0.5217\n",
      "Epoch: 059, Train Acc: 0.5888, Test Acc: 0.5109\n",
      "Epoch: 060, Train Acc: 0.5888, Test Acc: 0.4783\n",
      "Epoch: 061, Train Acc: 0.5607, Test Acc: 0.5326\n",
      "Epoch: 062, Train Acc: 0.6449, Test Acc: 0.4674\n",
      "Epoch: 063, Train Acc: 0.6495, Test Acc: 0.5217\n",
      "Epoch: 064, Train Acc: 0.6402, Test Acc: 0.5217\n",
      "Epoch: 065, Train Acc: 0.5327, Test Acc: 0.4565\n",
      "Epoch: 066, Train Acc: 0.6402, Test Acc: 0.5217\n",
      "Epoch: 067, Train Acc: 0.6168, Test Acc: 0.5109\n",
      "Epoch: 068, Train Acc: 0.6028, Test Acc: 0.5217\n",
      "Epoch: 069, Train Acc: 0.6495, Test Acc: 0.5543\n",
      "Epoch: 070, Train Acc: 0.6822, Test Acc: 0.5217\n",
      "Epoch: 071, Train Acc: 0.5748, Test Acc: 0.5761\n",
      "Epoch: 072, Train Acc: 0.6075, Test Acc: 0.4891\n",
      "Epoch: 073, Train Acc: 0.6215, Test Acc: 0.5652\n",
      "Epoch: 074, Train Acc: 0.6589, Test Acc: 0.5543\n",
      "Epoch: 075, Train Acc: 0.7056, Test Acc: 0.5217\n",
      "Epoch: 076, Train Acc: 0.6776, Test Acc: 0.5543\n",
      "Epoch: 077, Train Acc: 0.6636, Test Acc: 0.5652\n",
      "Epoch: 078, Train Acc: 0.6776, Test Acc: 0.5543\n",
      "Epoch: 079, Train Acc: 0.7150, Test Acc: 0.5543\n",
      "Epoch: 080, Train Acc: 0.6729, Test Acc: 0.5543\n",
      "Epoch: 081, Train Acc: 0.6822, Test Acc: 0.5543\n",
      "Epoch: 082, Train Acc: 0.7243, Test Acc: 0.5326\n",
      "Epoch: 083, Train Acc: 0.7009, Test Acc: 0.5217\n",
      "Epoch: 084, Train Acc: 0.7150, Test Acc: 0.5652\n",
      "Epoch: 085, Train Acc: 0.7009, Test Acc: 0.5652\n",
      "Epoch: 086, Train Acc: 0.6916, Test Acc: 0.5543\n",
      "Epoch: 087, Train Acc: 0.7336, Test Acc: 0.5870\n",
      "Epoch: 088, Train Acc: 0.6869, Test Acc: 0.5870\n",
      "Epoch: 089, Train Acc: 0.7243, Test Acc: 0.5978\n",
      "Epoch: 090, Train Acc: 0.6495, Test Acc: 0.5217\n",
      "Epoch: 091, Train Acc: 0.7243, Test Acc: 0.5870\n",
      "Epoch: 092, Train Acc: 0.6963, Test Acc: 0.5217\n",
      "Epoch: 093, Train Acc: 0.7617, Test Acc: 0.5543\n",
      "Epoch: 094, Train Acc: 0.7383, Test Acc: 0.6087\n",
      "Epoch: 095, Train Acc: 0.7523, Test Acc: 0.5978\n",
      "Epoch: 096, Train Acc: 0.7710, Test Acc: 0.5870\n",
      "Epoch: 097, Train Acc: 0.7710, Test Acc: 0.5870\n",
      "Epoch: 098, Train Acc: 0.6495, Test Acc: 0.5217\n",
      "Epoch: 099, Train Acc: 0.7150, Test Acc: 0.5326\n",
      "Epoch: 100, Train Acc: 0.7430, Test Acc: 0.5326\n",
      "Epoch: 101, Train Acc: 0.7664, Test Acc: 0.5870\n",
      "Epoch: 102, Train Acc: 0.7804, Test Acc: 0.5435\n",
      "Epoch: 103, Train Acc: 0.7430, Test Acc: 0.5217\n",
      "Epoch: 104, Train Acc: 0.7430, Test Acc: 0.5761\n",
      "Epoch: 105, Train Acc: 0.7804, Test Acc: 0.5326\n",
      "Epoch: 106, Train Acc: 0.7664, Test Acc: 0.5543\n",
      "Epoch: 107, Train Acc: 0.7523, Test Acc: 0.5326\n",
      "Epoch: 108, Train Acc: 0.6776, Test Acc: 0.5326\n",
      "Epoch: 109, Train Acc: 0.7523, Test Acc: 0.5435\n",
      "Epoch: 110, Train Acc: 0.7290, Test Acc: 0.5761\n",
      "Epoch: 111, Train Acc: 0.8131, Test Acc: 0.5652\n",
      "Epoch: 112, Train Acc: 0.7897, Test Acc: 0.5870\n",
      "Epoch: 113, Train Acc: 0.7897, Test Acc: 0.5217\n",
      "Epoch: 114, Train Acc: 0.7056, Test Acc: 0.5217\n",
      "Epoch: 115, Train Acc: 0.7617, Test Acc: 0.5761\n",
      "Epoch: 116, Train Acc: 0.7991, Test Acc: 0.5870\n",
      "Epoch: 117, Train Acc: 0.8084, Test Acc: 0.5435\n",
      "Epoch: 118, Train Acc: 0.8178, Test Acc: 0.5761\n",
      "Epoch: 119, Train Acc: 0.8084, Test Acc: 0.5326\n",
      "Epoch: 120, Train Acc: 0.8037, Test Acc: 0.5870\n",
      "Epoch: 121, Train Acc: 0.7664, Test Acc: 0.5761\n",
      "Epoch: 122, Train Acc: 0.7804, Test Acc: 0.5652\n",
      "Epoch: 123, Train Acc: 0.8084, Test Acc: 0.5543\n",
      "Epoch: 124, Train Acc: 0.8131, Test Acc: 0.5435\n",
      "Epoch: 125, Train Acc: 0.8131, Test Acc: 0.5543\n",
      "Epoch: 126, Train Acc: 0.8037, Test Acc: 0.5435\n",
      "Epoch: 127, Train Acc: 0.8131, Test Acc: 0.5652\n",
      "Epoch: 128, Train Acc: 0.7757, Test Acc: 0.5217\n",
      "Epoch: 129, Train Acc: 0.7991, Test Acc: 0.5435\n",
      "Epoch: 130, Train Acc: 0.8178, Test Acc: 0.5761\n",
      "Epoch: 131, Train Acc: 0.8271, Test Acc: 0.5870\n",
      "Epoch: 132, Train Acc: 0.7991, Test Acc: 0.5652\n",
      "Epoch: 133, Train Acc: 0.7944, Test Acc: 0.5543\n",
      "Epoch: 134, Train Acc: 0.7991, Test Acc: 0.5761\n",
      "Epoch: 135, Train Acc: 0.7850, Test Acc: 0.5435\n",
      "Epoch: 136, Train Acc: 0.7850, Test Acc: 0.5652\n",
      "Epoch: 137, Train Acc: 0.6682, Test Acc: 0.4674\n",
      "Epoch: 138, Train Acc: 0.7944, Test Acc: 0.5435\n",
      "Epoch: 139, Train Acc: 0.8084, Test Acc: 0.5652\n",
      "Epoch: 140, Train Acc: 0.8178, Test Acc: 0.5652\n",
      "Epoch: 141, Train Acc: 0.7710, Test Acc: 0.5217\n",
      "Epoch: 142, Train Acc: 0.7944, Test Acc: 0.5435\n",
      "Epoch: 143, Train Acc: 0.8084, Test Acc: 0.5326\n",
      "Epoch: 144, Train Acc: 0.7804, Test Acc: 0.5326\n",
      "Epoch: 145, Train Acc: 0.7710, Test Acc: 0.5217\n",
      "Epoch: 146, Train Acc: 0.7804, Test Acc: 0.5652\n",
      "Epoch: 147, Train Acc: 0.8084, Test Acc: 0.5761\n",
      "Epoch: 148, Train Acc: 0.8178, Test Acc: 0.5326\n",
      "Epoch: 149, Train Acc: 0.8178, Test Acc: 0.5652\n",
      "Epoch: 150, Train Acc: 0.8364, Test Acc: 0.5435\n",
      "Epoch: 151, Train Acc: 0.8271, Test Acc: 0.5109\n",
      "Epoch: 152, Train Acc: 0.8224, Test Acc: 0.5978\n",
      "Epoch: 153, Train Acc: 0.8178, Test Acc: 0.6196\n",
      "Epoch: 154, Train Acc: 0.8178, Test Acc: 0.5761\n",
      "Epoch: 155, Train Acc: 0.8178, Test Acc: 0.5761\n",
      "Epoch: 156, Train Acc: 0.7897, Test Acc: 0.5652\n",
      "Epoch: 157, Train Acc: 0.8224, Test Acc: 0.5870\n",
      "Epoch: 158, Train Acc: 0.7664, Test Acc: 0.5652\n",
      "Epoch: 159, Train Acc: 0.8271, Test Acc: 0.5652\n",
      "Epoch: 160, Train Acc: 0.8271, Test Acc: 0.5435\n",
      "Epoch: 161, Train Acc: 0.8131, Test Acc: 0.5543\n",
      "Epoch: 162, Train Acc: 0.8131, Test Acc: 0.5543\n",
      "Epoch: 163, Train Acc: 0.8271, Test Acc: 0.5978\n",
      "Epoch: 164, Train Acc: 0.8364, Test Acc: 0.5652\n",
      "Epoch: 165, Train Acc: 0.8178, Test Acc: 0.5326\n",
      "Epoch: 166, Train Acc: 0.8224, Test Acc: 0.5761\n",
      "Epoch: 167, Train Acc: 0.8037, Test Acc: 0.5761\n",
      "Epoch: 168, Train Acc: 0.7757, Test Acc: 0.5652\n",
      "Epoch: 169, Train Acc: 0.8364, Test Acc: 0.5217\n",
      "Epoch: 170, Train Acc: 0.7850, Test Acc: 0.5217\n",
      "Epoch: 171, Train Acc: 0.8458, Test Acc: 0.6196\n",
      "Epoch: 172, Train Acc: 0.7944, Test Acc: 0.5761\n",
      "Epoch: 173, Train Acc: 0.8364, Test Acc: 0.5870\n",
      "Epoch: 174, Train Acc: 0.8458, Test Acc: 0.5978\n",
      "Epoch: 175, Train Acc: 0.8084, Test Acc: 0.5761\n",
      "Epoch: 176, Train Acc: 0.8505, Test Acc: 0.5978\n",
      "Epoch: 177, Train Acc: 0.8458, Test Acc: 0.6087\n",
      "Epoch: 178, Train Acc: 0.8178, Test Acc: 0.5870\n",
      "Epoch: 179, Train Acc: 0.8318, Test Acc: 0.5978\n",
      "Epoch: 180, Train Acc: 0.8645, Test Acc: 0.5870\n",
      "Epoch: 181, Train Acc: 0.8364, Test Acc: 0.5435\n",
      "Epoch: 182, Train Acc: 0.8364, Test Acc: 0.6087\n",
      "Epoch: 183, Train Acc: 0.8505, Test Acc: 0.5652\n",
      "Epoch: 184, Train Acc: 0.8084, Test Acc: 0.5217\n",
      "Epoch: 185, Train Acc: 0.8458, Test Acc: 0.6196\n",
      "Epoch: 186, Train Acc: 0.8131, Test Acc: 0.5870\n",
      "Epoch: 187, Train Acc: 0.8131, Test Acc: 0.5870\n",
      "Epoch: 188, Train Acc: 0.8178, Test Acc: 0.5870\n",
      "Epoch: 189, Train Acc: 0.8364, Test Acc: 0.5870\n",
      "Epoch: 190, Train Acc: 0.8598, Test Acc: 0.6196\n",
      "Epoch: 191, Train Acc: 0.8318, Test Acc: 0.5543\n",
      "Epoch: 192, Train Acc: 0.8458, Test Acc: 0.6087\n",
      "Epoch: 193, Train Acc: 0.8551, Test Acc: 0.5761\n",
      "Epoch: 194, Train Acc: 0.8598, Test Acc: 0.5761\n",
      "Epoch: 195, Train Acc: 0.8505, Test Acc: 0.6087\n",
      "Epoch: 196, Train Acc: 0.8551, Test Acc: 0.5761\n",
      "Epoch: 197, Train Acc: 0.8505, Test Acc: 0.6087\n",
      "Epoch: 198, Train Acc: 0.8318, Test Acc: 0.5870\n",
      "Epoch: 199, Train Acc: 0.8411, Test Acc: 0.5543\n",
      "Epoch: 200, Train Acc: 0.8318, Test Acc: 0.6196\n",
      "Training duration: 967.6142129898071 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gcn = GCN()\n",
    "gcn = gcn.to(device)\n",
    "# print(gcn.parameters())\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "# loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "out_labels = []\n",
    "\n",
    "# training_running_loss = 0.0\n",
    "\n",
    "def train(train_loader):\n",
    "    \n",
    "    gcn.train()\n",
    "    # print(gcn.parameters())\n",
    "    for batch_data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        for data in batch_data:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            #forward pass\n",
    "            out = gcn(data.x, data.edge_index, data.batch)\n",
    "            # calculate the loss\n",
    "            loss = criterion(out, data.y)\n",
    "            # zero the gradients of the weights so that the gradients are not accumulated\n",
    "            # calculate the gradients using backpropagation\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss\n",
    "            # training_running_loss += loss.detach().item()\n",
    "            \n",
    "            out_labels.append((out, data.y))\n",
    "        \n",
    "        \n",
    "\n",
    "testing_labels = []\n",
    "def test(loader):\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    for batch_data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        for data in batch_data:\n",
    "            out = gcn(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            testing_labels.append(pred)\n",
    "            y_label = (data.y.tolist())\n",
    "            y_label = y_label[0].index(1.0)\n",
    "            pred_label = (pred.tolist())[0]\n",
    "            # print(pred_label)\n",
    "            # print(y_label)\n",
    "            if y_label == pred_label:\n",
    "                correct += 1            \n",
    "            # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 200\n",
    "# Your training code here\n",
    "for epoch in range(num_epochs):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch + 1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(\"Training duration:\", duration, \"seconds\")\n",
    "# with open(\"out_labels.txt\", \"w\") as output:\n",
    "#         output.write(str(out_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6195652173913043"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn.state_dict(), 'gcn_model83-61-0001-200.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\\comparator16.txt\n",
      "tensor([6])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "num = random.randint(0, len(verilog_files))\n",
    "print(verilog_files[num])\n",
    "data_trial = dataset[num]\n",
    "\n",
    "\n",
    "out = gcn(data_trial.x, data_trial.edge_index, data_trial.batch)\n",
    "pred = out.argmax(dim=1)\n",
    "print(pred)\n",
    "print((data_trial.y.tolist())[0].index(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[0.1118, 0.1097, 0.1018, 0.1003, 0.1178, 0.1129, 0.1081, 0.1210, 0.1167]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1059, 0.1114, 0.1021, 0.0979, 0.1187, 0.1096, 0.1097, 0.1217, 0.1228]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1078, 0.1124, 0.1022, 0.1001, 0.1166, 0.1129, 0.1110, 0.1191, 0.1179]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1095, 0.1032, 0.0960, 0.0981, 0.1257, 0.1091, 0.1091, 0.1260, 0.1233]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1096, 0.1091, 0.1006, 0.1017, 0.1147, 0.1134, 0.1094, 0.1264, 0.1150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1065, 0.1128, 0.0992, 0.1019, 0.1142, 0.1175, 0.1083, 0.1225, 0.1171]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1139, 0.1077, 0.0977, 0.0989, 0.1158, 0.1183, 0.1071, 0.1264, 0.1142]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1071, 0.1127, 0.0988, 0.0998, 0.1145, 0.1159, 0.1104, 0.1251, 0.1158]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1081, 0.1133, 0.1001, 0.0981, 0.1155, 0.1141, 0.1137, 0.1236, 0.1135]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1088, 0.1155, 0.0976, 0.0986, 0.1130, 0.1117, 0.1142, 0.1264, 0.1141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1208, 0.1097, 0.0887, 0.0928, 0.1110, 0.1075, 0.1250, 0.1413, 0.1031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1085, 0.1138, 0.0930, 0.0979, 0.1097, 0.1154, 0.1200, 0.1291, 0.1126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1081, 0.1128, 0.0957, 0.0979, 0.1128, 0.1196, 0.1140, 0.1266, 0.1124]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1090, 0.1068, 0.0943, 0.0958, 0.1132, 0.1194, 0.1234, 0.1300, 0.1082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.1046, 0.0898, 0.0919, 0.1132, 0.1343, 0.1389, 0.1294, 0.0975]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1079, 0.1126, 0.0969, 0.1008, 0.1130, 0.1213, 0.1146, 0.1225, 0.1105]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1040, 0.1074, 0.0939, 0.0977, 0.1184, 0.1195, 0.1201, 0.1306, 0.1084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1021, 0.1103, 0.1021, 0.1021, 0.1165, 0.1199, 0.1178, 0.1211, 0.1080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1002, 0.0999, 0.0859, 0.1045, 0.1209, 0.1353, 0.1212, 0.1336, 0.0984]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.1164, 0.0946, 0.0977, 0.1220, 0.1182, 0.1215, 0.1242, 0.1049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1010, 0.1056, 0.0961, 0.0998, 0.1263, 0.1173, 0.1179, 0.1289, 0.1071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1034, 0.1058, 0.0977, 0.1001, 0.1193, 0.1232, 0.1170, 0.1245, 0.1091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1033, 0.1066, 0.0917, 0.0978, 0.1267, 0.1307, 0.1195, 0.1241, 0.0995]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.1101, 0.1022, 0.1006, 0.1219, 0.1193, 0.1169, 0.1203, 0.1091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0970, 0.0955, 0.0993, 0.1000, 0.1293, 0.1240, 0.1209, 0.1316, 0.1024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0919, 0.1066, 0.1006, 0.1029, 0.1235, 0.1216, 0.1224, 0.1208, 0.1096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0944, 0.1025, 0.0988, 0.1020, 0.1265, 0.1249, 0.1308, 0.1184, 0.1018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0939, 0.1065, 0.1001, 0.1006, 0.1239, 0.1316, 0.1233, 0.1226, 0.0977]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1071, 0.1068, 0.0978, 0.1007, 0.1220, 0.1242, 0.1130, 0.1208, 0.1075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1054, 0.1044, 0.1000, 0.1063, 0.1206, 0.1223, 0.1200, 0.1145, 0.1065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0974, 0.1064, 0.1047, 0.1030, 0.1266, 0.1324, 0.1208, 0.1124, 0.0964]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0944, 0.1041, 0.1022, 0.0993, 0.1326, 0.1246, 0.1262, 0.1162, 0.1004]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0994, 0.1075, 0.1058, 0.1002, 0.1237, 0.1211, 0.1149, 0.1195, 0.1079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0977, 0.0997, 0.1061, 0.0960, 0.1364, 0.1253, 0.1237, 0.1167, 0.0984]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0930, 0.1051, 0.1022, 0.0985, 0.1289, 0.1288, 0.1202, 0.1214, 0.1020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0978, 0.1062, 0.1025, 0.0984, 0.1281, 0.1220, 0.1145, 0.1218, 0.1087]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0997, 0.1020, 0.1038, 0.1034, 0.1270, 0.1244, 0.1114, 0.1260, 0.1022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0759, 0.0706, 0.1188, 0.1135, 0.1482, 0.1511, 0.1388, 0.1186, 0.0643]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0805, 0.0973, 0.1074, 0.0960, 0.1498, 0.1348, 0.1199, 0.1220, 0.0925]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0894, 0.0933, 0.1086, 0.1078, 0.1390, 0.1399, 0.1188, 0.1172, 0.0860]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0858, 0.1033, 0.1062, 0.1049, 0.1399, 0.1306, 0.1281, 0.1101, 0.0910]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0884, 0.0970, 0.1113, 0.1019, 0.1403, 0.1238, 0.1221, 0.1135, 0.1018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0979, 0.1051, 0.1036, 0.1089, 0.1342, 0.1239, 0.1208, 0.1134, 0.0922]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.1024, 0.1066, 0.0980, 0.1387, 0.1230, 0.1176, 0.1191, 0.1021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0866, 0.1092, 0.1093, 0.1527, 0.1318, 0.1412, 0.1135, 0.0829]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0647, 0.0729, 0.1220, 0.1259, 0.1680, 0.1497, 0.1247, 0.1015, 0.0706]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0875, 0.0965, 0.1112, 0.1158, 0.1427, 0.1351, 0.1272, 0.1067, 0.0772]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0951, 0.1059, 0.1139, 0.1059, 0.1449, 0.1223, 0.1218, 0.1043, 0.0860]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0815, 0.0841, 0.1227, 0.1163, 0.1655, 0.1343, 0.1398, 0.0931, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0928, 0.0950, 0.1144, 0.1094, 0.1429, 0.1284, 0.1221, 0.1065, 0.0884]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0934, 0.1097, 0.1119, 0.1585, 0.1274, 0.1259, 0.1127, 0.0774]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0792, 0.0763, 0.1274, 0.1077, 0.1611, 0.1331, 0.1588, 0.0936, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0915, 0.0899, 0.1135, 0.1170, 0.1493, 0.1277, 0.1354, 0.1039, 0.0718]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0563, 0.0535, 0.1198, 0.1389, 0.2035, 0.1588, 0.1424, 0.0878, 0.0389]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0815, 0.0586, 0.1195, 0.1449, 0.1896, 0.1302, 0.1423, 0.0831, 0.0502]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0813, 0.0809, 0.1272, 0.1156, 0.1574, 0.1325, 0.1383, 0.0939, 0.0729]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0705, 0.0663, 0.1214, 0.1423, 0.1719, 0.1237, 0.1695, 0.0827, 0.0517]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0589, 0.0549, 0.1517, 0.1126, 0.2043, 0.1377, 0.1818, 0.0606, 0.0376]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0811, 0.0663, 0.1082, 0.1254, 0.1625, 0.1573, 0.1661, 0.0928, 0.0404]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0783, 0.0715, 0.1137, 0.1247, 0.1709, 0.1421, 0.1617, 0.0913, 0.0457]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0697, 0.0553, 0.1361, 0.1254, 0.1759, 0.1472, 0.1726, 0.0755, 0.0423]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0853, 0.0426, 0.1335, 0.1308, 0.1678, 0.1615, 0.1717, 0.0825, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0519, 0.1104, 0.1750, 0.1683, 0.1925, 0.1534, 0.0631, 0.0240]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1032, 0.0836, 0.1148, 0.1244, 0.1465, 0.1387, 0.1373, 0.0882, 0.0632]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0904, 0.0638, 0.1120, 0.1380, 0.1465, 0.1600, 0.1571, 0.0868, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0745, 0.0717, 0.1204, 0.1380, 0.1804, 0.1574, 0.1557, 0.0687, 0.0331]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1000, 0.0819, 0.1204, 0.1079, 0.1708, 0.1505, 0.1268, 0.0843, 0.0574]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0896, 0.0675, 0.0923, 0.1379, 0.1691, 0.1610, 0.1472, 0.0805, 0.0549]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0941, 0.0767, 0.1136, 0.1291, 0.1560, 0.1427, 0.1536, 0.0851, 0.0491]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1035, 0.0598, 0.1109, 0.1437, 0.1532, 0.1655, 0.1315, 0.0876, 0.0443]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1138, 0.0710, 0.1010, 0.1349, 0.1380, 0.1734, 0.1204, 0.0932, 0.0543]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0723, 0.0677, 0.1047, 0.1543, 0.1820, 0.1725, 0.1473, 0.0662, 0.0329]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0920, 0.0721, 0.1033, 0.1391, 0.1913, 0.1488, 0.1276, 0.0799, 0.0460]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0911, 0.0236, 0.0775, 0.1725, 0.2374, 0.1754, 0.1570, 0.0548, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0948, 0.0616, 0.1025, 0.1620, 0.1551, 0.1587, 0.1203, 0.0937, 0.0513]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1131, 0.0589, 0.0982, 0.1332, 0.1599, 0.1827, 0.1146, 0.0979, 0.0415]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1069, 0.0462, 0.0937, 0.1657, 0.1639, 0.2076, 0.1072, 0.0775, 0.0313]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0497, 0.1074, 0.1371, 0.1863, 0.1817, 0.1217, 0.0840, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0682, 0.0411, 0.1219, 0.1727, 0.2034, 0.1782, 0.1250, 0.0639, 0.0257]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0885, 0.0495, 0.0845, 0.1705, 0.1931, 0.1843, 0.1153, 0.0839, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1038, 0.0746, 0.1087, 0.1283, 0.1511, 0.1504, 0.1203, 0.0989, 0.0639]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0852, 0.0559, 0.0919, 0.1503, 0.1910, 0.1766, 0.1086, 0.0968, 0.0437]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1053, 0.0359, 0.0689, 0.2009, 0.2021, 0.2345, 0.0873, 0.0494, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0795, 0.0441, 0.0872, 0.1911, 0.1805, 0.1851, 0.1112, 0.0889, 0.0324]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0901, 0.0695, 0.0852, 0.1489, 0.1709, 0.1846, 0.0929, 0.1001, 0.0578]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0856, 0.0244, 0.0965, 0.1702, 0.2351, 0.2120, 0.0942, 0.0644, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0761, 0.0217, 0.0599, 0.3122, 0.2130, 0.1706, 0.0799, 0.0586, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0545, 0.0231, 0.0607, 0.2487, 0.2486, 0.2047, 0.0734, 0.0669, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0612, 0.0321, 0.0870, 0.1591, 0.2246, 0.2194, 0.0769, 0.1135, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0670, 0.0212, 0.0717, 0.2029, 0.2131, 0.2993, 0.0538, 0.0570, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0848, 0.0477, 0.0873, 0.1673, 0.1955, 0.1916, 0.0871, 0.0989, 0.0398]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0552, 0.0231, 0.0744, 0.1699, 0.3081, 0.2381, 0.0571, 0.0576, 0.0164]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0667, 0.0203, 0.0638, 0.2149, 0.2519, 0.2103, 0.0959, 0.0605, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0118, 0.0008, 0.0140, 0.2004, 0.5902, 0.1593, 0.0165, 0.0063, 0.0007]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0784, 0.0326, 0.0771, 0.1881, 0.2077, 0.2432, 0.0651, 0.0824, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0925, 0.0382, 0.0890, 0.1561, 0.1929, 0.1952, 0.1025, 0.0890, 0.0445]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0711, 0.0280, 0.0647, 0.1724, 0.2174, 0.2565, 0.0845, 0.0768, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0726, 0.0411, 0.0830, 0.1665, 0.2173, 0.2369, 0.0792, 0.0775, 0.0258]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0625, 0.0222, 0.0515, 0.1902, 0.2606, 0.2716, 0.0626, 0.0612, 0.0177]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0996, 0.0570, 0.0846, 0.1522, 0.1587, 0.1976, 0.0883, 0.1083, 0.0536]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0592, 0.0163, 0.0654, 0.2129, 0.2247, 0.2653, 0.0615, 0.0796, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0157, 0.0510, 0.2885, 0.1464, 0.3217, 0.0478, 0.0639, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0633, 0.0196, 0.0512, 0.2280, 0.1892, 0.3098, 0.0608, 0.0628, 0.0154]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0618, 0.0345, 0.0518, 0.2087, 0.2015, 0.2658, 0.0664, 0.0833, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0723, 0.0394, 0.0723, 0.1980, 0.1643, 0.2491, 0.0818, 0.0826, 0.0402]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0521, 0.0138, 0.0384, 0.2750, 0.1628, 0.3084, 0.0752, 0.0610, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0225, 0.0484, 0.2307, 0.1562, 0.3211, 0.0638, 0.0738, 0.0234]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0773, 0.0448, 0.0561, 0.2191, 0.1630, 0.2269, 0.0956, 0.0860, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0772, 0.0329, 0.0621, 0.2126, 0.1566, 0.2672, 0.0750, 0.0887, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0144, 0.0476, 0.2886, 0.1676, 0.3017, 0.0686, 0.0598, 0.0131]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0658, 0.0307, 0.0504, 0.2885, 0.1422, 0.2786, 0.0624, 0.0619, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0740, 0.0269, 0.0589, 0.2084, 0.1286, 0.2891, 0.0651, 0.1186, 0.0304]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0413, 0.0104, 0.0237, 0.2941, 0.0720, 0.4639, 0.0421, 0.0444, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0376, 0.0122, 0.0444, 0.3157, 0.1440, 0.3306, 0.0552, 0.0511, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0567, 0.0206, 0.0489, 0.2647, 0.1004, 0.3248, 0.0866, 0.0805, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0167, 0.0035, 0.0113, 0.3528, 0.0509, 0.4943, 0.0338, 0.0349, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0424, 0.0180, 0.0320, 0.3218, 0.0776, 0.3445, 0.0602, 0.0920, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0423, 0.0183, 0.0389, 0.2447, 0.0965, 0.4033, 0.0743, 0.0709, 0.0107]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0227, 0.0031, 0.0160, 0.4263, 0.0451, 0.3910, 0.0464, 0.0463, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0633, 0.0197, 0.0352, 0.2251, 0.0817, 0.4386, 0.0404, 0.0780, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.2967e-03, 3.0514e-04, 2.0014e-03, 1.5553e-01, 7.2512e-03, 8.1690e-01,\n",
       "           9.0341e-03, 6.6130e-03, 6.8204e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0160, 0.0048, 0.0108, 0.2517, 0.0270, 0.6313, 0.0183, 0.0374, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0218, 0.0034, 0.0113, 0.2594, 0.0311, 0.6152, 0.0244, 0.0303, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0123, 0.0342, 0.2747, 0.0854, 0.4173, 0.0630, 0.0620, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0105, 0.0009, 0.0044, 0.3423, 0.0098, 0.5994, 0.0120, 0.0200, 0.0006]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0441, 0.0102, 0.0225, 0.2536, 0.0616, 0.4867, 0.0705, 0.0429, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0168, 0.0254, 0.2354, 0.0877, 0.4852, 0.0450, 0.0477, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0320, 0.0451, 0.2063, 0.0737, 0.3835, 0.0722, 0.0753, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0107, 0.0011, 0.0042, 0.1618, 0.0138, 0.7829, 0.0092, 0.0152, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0242, 0.0096, 0.0235, 0.1577, 0.0781, 0.6078, 0.0467, 0.0437, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0210, 0.0051, 0.0193, 0.0960, 0.0370, 0.7492, 0.0304, 0.0382, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0413, 0.0133, 0.0331, 0.1980, 0.0813, 0.5237, 0.0415, 0.0568, 0.0111]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0428, 0.0267, 0.0480, 0.1580, 0.1125, 0.4335, 0.0817, 0.0763, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0555, 0.0151, 0.0317, 0.1643, 0.0679, 0.5386, 0.0625, 0.0497, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0107, 0.0018, 0.0122, 0.1424, 0.0372, 0.7502, 0.0222, 0.0223, 0.0010]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0197, 0.0044, 0.0104, 0.1213, 0.0348, 0.7547, 0.0367, 0.0160, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0420, 0.0072, 0.0246, 0.1530, 0.0494, 0.6289, 0.0403, 0.0496, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0195, 0.0047, 0.0170, 0.1810, 0.0489, 0.6487, 0.0382, 0.0388, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0715, 0.0339, 0.0470, 0.1921, 0.0884, 0.3864, 0.0743, 0.0822, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0259, 0.0101, 0.0224, 0.1371, 0.0530, 0.6529, 0.0502, 0.0430, 0.0054]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0673, 0.0236, 0.0417, 0.1817, 0.0845, 0.4283, 0.0647, 0.0840, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0288, 0.0098, 0.0206, 0.1397, 0.0657, 0.6240, 0.0501, 0.0550, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0168, 0.0033, 0.0104, 0.0998, 0.0304, 0.7608, 0.0308, 0.0446, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0494, 0.0166, 0.0277, 0.1261, 0.0675, 0.6007, 0.0445, 0.0546, 0.0129]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0538, 0.0188, 0.0332, 0.1528, 0.0581, 0.5535, 0.0438, 0.0731, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0126, 0.0072, 0.0149, 0.1236, 0.0663, 0.6905, 0.0314, 0.0505, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0772, 0.0320, 0.0585, 0.1501, 0.1274, 0.3228, 0.0883, 0.1123, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0074, 0.0192, 0.1550, 0.0760, 0.6231, 0.0337, 0.0557, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0584, 0.0169, 0.0469, 0.1486, 0.0997, 0.4695, 0.0648, 0.0787, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0343, 0.0166, 0.0389, 0.1245, 0.0596, 0.5945, 0.0518, 0.0719, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0442, 0.0201, 0.0515, 0.1662, 0.1167, 0.4086, 0.0670, 0.1088, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0284, 0.0092, 0.0192, 0.1370, 0.0848, 0.6219, 0.0372, 0.0553, 0.0069]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0060e-03, 4.8164e-05, 8.4278e-04, 3.1442e-02, 7.7538e-03, 9.4764e-01,\n",
       "           2.2061e-03, 9.0409e-03, 2.1262e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0292, 0.0057, 0.0174, 0.1454, 0.0467, 0.6747, 0.0343, 0.0433, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0292, 0.0074, 0.0287, 0.1566, 0.0963, 0.5007, 0.0693, 0.1013, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.8636e-04, 1.3970e-05, 2.9851e-04, 2.2131e-02, 5.7988e-03, 9.6613e-01,\n",
       "           1.6749e-03, 3.5520e-03, 1.3233e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0585, 0.0106, 0.0396, 0.2032, 0.0987, 0.4234, 0.0688, 0.0832, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0557, 0.0348, 0.0509, 0.1239, 0.1347, 0.3851, 0.0909, 0.0996, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0310, 0.0089, 0.0298, 0.1151, 0.0745, 0.5905, 0.0539, 0.0883, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0495, 0.0136, 0.0358, 0.1523, 0.0870, 0.5029, 0.0676, 0.0800, 0.0114]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0515, 0.0223, 0.0399, 0.1866, 0.1234, 0.3783, 0.0640, 0.1147, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0326, 0.0062, 0.0224, 0.1481, 0.0588, 0.6024, 0.0428, 0.0809, 0.0059]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0081, 0.0215, 0.1295, 0.0799, 0.5898, 0.0733, 0.0671, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.3440e-03, 3.1362e-04, 2.3222e-03, 4.9673e-02, 2.1704e-02, 9.0007e-01,\n",
       "           1.4016e-02, 9.3979e-03, 1.5432e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0295, 0.0074, 0.0268, 0.1803, 0.0807, 0.5572, 0.0526, 0.0592, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0349, 0.0148, 0.0383, 0.1368, 0.1252, 0.4874, 0.0633, 0.0867, 0.0127]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0164, 0.0012, 0.0075, 0.1054, 0.0504, 0.6958, 0.0151, 0.1064, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0215, 0.0070, 0.0232, 0.1430, 0.0970, 0.6010, 0.0468, 0.0557, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0252, 0.0138, 0.0354, 0.1752, 0.1157, 0.4686, 0.0717, 0.0833, 0.0112]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0256, 0.0094, 0.0228, 0.1536, 0.0922, 0.5211, 0.0784, 0.0859, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0114, 0.0046, 0.0171, 0.1237, 0.0899, 0.6408, 0.0432, 0.0641, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0665, 0.0367, 0.0558, 0.1542, 0.0992, 0.3254, 0.0823, 0.1403, 0.0397]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0083, 0.0021, 0.0080, 0.1071, 0.0438, 0.7587, 0.0298, 0.0407, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0395, 0.0159, 0.0461, 0.1422, 0.1251, 0.4356, 0.0660, 0.1136, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0246, 0.0059, 0.0250, 0.0913, 0.0848, 0.6198, 0.0596, 0.0843, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0528, 0.0212, 0.0483, 0.1493, 0.1153, 0.3800, 0.0746, 0.1331, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0244, 0.0054, 0.0199, 0.1647, 0.0671, 0.6103, 0.0545, 0.0493, 0.0044]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0622, 0.0226, 0.0542, 0.1412, 0.1273, 0.3620, 0.0961, 0.1132, 0.0212]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.8658e-03, 6.2723e-04, 3.5370e-03, 9.5234e-02, 3.5559e-02, 8.3107e-01,\n",
       "           1.1991e-02, 1.6639e-02, 4.7539e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0176, 0.0030, 0.0097, 0.1679, 0.0614, 0.6604, 0.0213, 0.0564, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0219, 0.0037, 0.0203, 0.1361, 0.0520, 0.6250, 0.0318, 0.1050, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0197, 0.0033, 0.0191, 0.1333, 0.0829, 0.6528, 0.0316, 0.0531, 0.0042]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0469, 0.0105, 0.0333, 0.1662, 0.0899, 0.4705, 0.0736, 0.1010, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0140, 0.0021, 0.0155, 0.1017, 0.0554, 0.7225, 0.0248, 0.0626, 0.0016]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0150, 0.0022, 0.0115, 0.1441, 0.0762, 0.6216, 0.0376, 0.0897, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0363, 0.0088, 0.0223, 0.1311, 0.1092, 0.5040, 0.0627, 0.1125, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0465, 0.0184, 0.0443, 0.1663, 0.1383, 0.3702, 0.0779, 0.1204, 0.0177]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0198, 0.0035, 0.0188, 0.1095, 0.1197, 0.6210, 0.0520, 0.0519, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0275, 0.0026, 0.0152, 0.1316, 0.0583, 0.6620, 0.0383, 0.0619, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0424, 0.0118, 0.0372, 0.1449, 0.1157, 0.4818, 0.0536, 0.1027, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0551, 0.0252, 0.0532, 0.1472, 0.1241, 0.3636, 0.0815, 0.1267, 0.0233]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0511, 0.0178, 0.0482, 0.1235, 0.1455, 0.3787, 0.0712, 0.1371, 0.0268]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0335, 0.0067, 0.0197, 0.1409, 0.1470, 0.4866, 0.0495, 0.1062, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0636, 0.0287, 0.0618, 0.1507, 0.1459, 0.2998, 0.0880, 0.1295, 0.0320]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0170, 0.0022, 0.0111, 0.1742, 0.0790, 0.5901, 0.0387, 0.0858, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0124, 0.0018, 0.0163, 0.1550, 0.1507, 0.5502, 0.0426, 0.0688, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0055, 0.0217, 0.1537, 0.1099, 0.5226, 0.0548, 0.0968, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0456, 0.0119, 0.0347, 0.1358, 0.0990, 0.5300, 0.0655, 0.0685, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0367, 0.0082, 0.0244, 0.1455, 0.1383, 0.4489, 0.0522, 0.1375, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0348, 0.0111, 0.0306, 0.1464, 0.1314, 0.4605, 0.0559, 0.1187, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0597, 0.0170, 0.0427, 0.1495, 0.1268, 0.3506, 0.0848, 0.1519, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0480, 0.0198, 0.0360, 0.1729, 0.1620, 0.3070, 0.1043, 0.1299, 0.0200]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0328, 0.0186, 0.0388, 0.1644, 0.1581, 0.3993, 0.0652, 0.1079, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0194, 0.0013, 0.0097, 0.1074, 0.0689, 0.6829, 0.0350, 0.0735, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0287, 0.0056, 0.0344, 0.1447, 0.1068, 0.4834, 0.0751, 0.1156, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0539, 0.0133, 0.0376, 0.1903, 0.1276, 0.3420, 0.0782, 0.1395, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0709, 0.0380, 0.0698, 0.1660, 0.1431, 0.2367, 0.1171, 0.1203, 0.0382]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0786, 0.0480, 0.0693, 0.1329, 0.1525, 0.2305, 0.1060, 0.1247, 0.0574]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1453e-04, 3.5857e-06, 1.4002e-04, 2.9182e-02, 1.0359e-02, 9.5206e-01,\n",
       "           1.7240e-03, 6.2075e-03, 4.9101e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0522, 0.0157, 0.0428, 0.2002, 0.1234, 0.3605, 0.0936, 0.0924, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0642, 0.0127, 0.0373, 0.1861, 0.1541, 0.3089, 0.0868, 0.1338, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0422, 0.0078, 0.0226, 0.1987, 0.1281, 0.4440, 0.0717, 0.0790, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0454, 0.0665, 0.1398, 0.1503, 0.2213, 0.1040, 0.1320, 0.0535]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0452, 0.0169, 0.0530, 0.1954, 0.1850, 0.2625, 0.1010, 0.1187, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0648, 0.0239, 0.0601, 0.1556, 0.1555, 0.2735, 0.1081, 0.1325, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0184, 0.0016, 0.0078, 0.2803, 0.1084, 0.4645, 0.0512, 0.0663, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0307, 0.0084, 0.0315, 0.2177, 0.1370, 0.3637, 0.0933, 0.1112, 0.0065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0484, 0.0202, 0.0409, 0.2243, 0.1178, 0.3321, 0.0908, 0.1061, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0749, 0.0443, 0.0776, 0.1501, 0.1372, 0.2306, 0.1131, 0.1237, 0.0485]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0449, 0.0150, 0.0365, 0.2476, 0.1387, 0.3220, 0.0778, 0.1009, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0594, 0.0181, 0.0502, 0.2369, 0.1422, 0.2327, 0.1003, 0.1389, 0.0214]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8595e-03, 2.9371e-04, 3.2481e-03, 4.9506e-01, 7.7968e-02, 3.8345e-01,\n",
       "           1.4730e-02, 2.3208e-02, 1.8038e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0186, 0.0454, 0.2229, 0.1327, 0.2922, 0.1099, 0.1021, 0.0161]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0331, 0.0019, 0.0139, 0.3394, 0.0541, 0.4479, 0.0442, 0.0636, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0696, 0.0310, 0.0660, 0.1993, 0.1384, 0.2412, 0.1159, 0.1047, 0.0340]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0268, 0.0033, 0.0176, 0.2968, 0.0719, 0.4533, 0.0566, 0.0708, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0868, 0.0319, 0.0816, 0.1807, 0.1281, 0.2245, 0.1037, 0.1210, 0.0418]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0319, 0.0671, 0.1893, 0.1471, 0.2572, 0.1081, 0.1043, 0.0299]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0473, 0.0076, 0.0395, 0.2757, 0.0817, 0.3803, 0.0890, 0.0714, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0609, 0.0134, 0.0432, 0.2169, 0.1124, 0.3579, 0.0850, 0.0923, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0824, 0.0327, 0.0672, 0.1671, 0.1294, 0.2695, 0.1142, 0.1042, 0.0334]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0761, 0.0481, 0.0783, 0.1547, 0.1374, 0.2297, 0.1329, 0.0999, 0.0429]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0231, 0.0655, 0.1663, 0.1173, 0.3025, 0.1101, 0.1151, 0.0268]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.8159e-03, 6.0923e-04, 8.4433e-03, 1.9427e-01, 4.0449e-02, 6.8429e-01,\n",
       "           4.1066e-02, 2.1164e-02, 8.9709e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0433, 0.0089, 0.0438, 0.2315, 0.0828, 0.4048, 0.0983, 0.0774, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0800, 0.0199, 0.0583, 0.2072, 0.1138, 0.3215, 0.1040, 0.0750, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0461, 0.0119, 0.0484, 0.2744, 0.1351, 0.3063, 0.0940, 0.0714, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0819, 0.0305, 0.0624, 0.2216, 0.1206, 0.2381, 0.1098, 0.0977, 0.0374]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0342, 0.0040, 0.0319, 0.2765, 0.0608, 0.4518, 0.0702, 0.0668, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0768, 0.0491, 0.0870, 0.1850, 0.1341, 0.2134, 0.1164, 0.0955, 0.0428]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.6946e-03, 3.6741e-05, 3.2339e-03, 3.5913e-01, 2.7354e-02, 5.4899e-01,\n",
       "           4.6323e-02, 1.1186e-02, 4.7200e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0487, 0.0136, 0.0673, 0.2761, 0.1132, 0.2925, 0.0964, 0.0783, 0.0139]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0451, 0.0122, 0.0546, 0.3136, 0.1221, 0.2881, 0.0880, 0.0660, 0.0103]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0710, 0.0205, 0.0566, 0.2243, 0.1320, 0.2992, 0.0890, 0.0832, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0141, 0.0008, 0.0094, 0.3070, 0.0438, 0.5808, 0.0197, 0.0227, 0.0017]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0432, 0.0132, 0.0467, 0.2279, 0.1138, 0.3841, 0.0817, 0.0746, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0402, 0.0096, 0.0451, 0.2318, 0.0735, 0.4340, 0.0905, 0.0643, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0186, 0.0025, 0.0135, 0.2251, 0.0594, 0.6136, 0.0337, 0.0308, 0.0028]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0593, 0.0283, 0.0546, 0.1926, 0.1407, 0.3131, 0.0909, 0.0883, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0506, 0.0145, 0.0570, 0.2065, 0.1396, 0.3212, 0.1200, 0.0748, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0023, 0.0212, 0.2556, 0.0576, 0.5332, 0.0522, 0.0368, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0143, 0.0427, 0.1893, 0.0848, 0.4213, 0.0808, 0.0920, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0554, 0.0174, 0.0620, 0.1878, 0.1382, 0.3451, 0.0982, 0.0784, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0136, 0.0500, 0.1865, 0.1194, 0.3949, 0.0948, 0.0776, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0975, 0.0496, 0.0829, 0.1439, 0.1242, 0.2422, 0.1104, 0.1035, 0.0458]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0830, 0.0409, 0.0779, 0.1494, 0.1242, 0.2694, 0.1024, 0.1057, 0.0470]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0277, 0.0059, 0.0556, 0.1602, 0.1238, 0.4471, 0.0946, 0.0782, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2805e-03, 4.9053e-05, 1.5539e-03, 3.5752e-02, 6.6068e-03, 9.4597e-01,\n",
       "           5.9896e-03, 2.7571e-03, 4.1143e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0423, 0.0145, 0.0526, 0.1376, 0.1227, 0.4660, 0.0754, 0.0700, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0446, 0.0082, 0.0374, 0.1851, 0.1199, 0.4737, 0.0659, 0.0592, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.5401e-05, 1.6089e-08, 1.3805e-05, 3.7343e-03, 2.3489e-04, 9.9582e-01,\n",
       "           5.4741e-05, 1.1343e-04, 3.3377e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0210, 0.0032, 0.0147, 0.1537, 0.0501, 0.6810, 0.0256, 0.0475, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0156, 0.0019, 0.0111, 0.0893, 0.0431, 0.7749, 0.0261, 0.0356, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0562, 0.0268, 0.0656, 0.1456, 0.1446, 0.3571, 0.0876, 0.0858, 0.0309]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0100, 0.0345, 0.1661, 0.1036, 0.5010, 0.0591, 0.0544, 0.0112]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0090, 0.0012, 0.0051, 0.1076, 0.0189, 0.8299, 0.0101, 0.0170, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0194, 0.0026, 0.0179, 0.1235, 0.0532, 0.6945, 0.0415, 0.0444, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0177, 0.0015, 0.0104, 0.1403, 0.0381, 0.7257, 0.0307, 0.0331, 0.0025]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6210e-03, 1.5184e-05, 5.4552e-04, 3.3802e-02, 2.6953e-03, 9.5394e-01,\n",
       "           1.9098e-03, 4.4471e-03, 2.6128e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0175, 0.0008, 0.0059, 0.1360, 0.0221, 0.7778, 0.0181, 0.0208, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0507, 0.0149, 0.0498, 0.1736, 0.1394, 0.4221, 0.0671, 0.0643, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0197, 0.0016, 0.0145, 0.0822, 0.0375, 0.7911, 0.0210, 0.0301, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0077, 0.0013, 0.0109, 0.1134, 0.0667, 0.7431, 0.0247, 0.0311, 0.0012]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.3927e-03, 1.6984e-04, 1.2831e-03, 3.9222e-02, 8.3252e-03, 9.3051e-01,\n",
       "           4.3729e-03, 1.0525e-02, 1.9836e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0190, 0.0011, 0.0119, 0.1156, 0.0352, 0.7718, 0.0158, 0.0283, 0.0014]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0430, 0.0139, 0.0354, 0.1266, 0.0713, 0.5775, 0.0608, 0.0574, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0184, 0.0029, 0.0136, 0.0632, 0.0542, 0.8005, 0.0204, 0.0243, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[9.5568e-03, 8.3943e-04, 5.7512e-03, 9.8759e-02, 2.9747e-02, 8.2081e-01,\n",
       "           1.2525e-02, 2.1329e-02, 6.8718e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0504e-02, 5.4642e-04, 6.0157e-03, 6.3735e-02, 3.3636e-02, 8.4922e-01,\n",
       "           1.7857e-02, 1.7855e-02, 6.3194e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.3072e-02, 6.4665e-04, 6.2002e-03, 6.9584e-02, 2.1103e-02, 8.6498e-01,\n",
       "           1.1038e-02, 1.2744e-02, 6.3274e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0313, 0.0060, 0.0381, 0.1064, 0.0715, 0.6431, 0.0416, 0.0547, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0211, 0.0018, 0.0109, 0.1260, 0.0304, 0.7684, 0.0169, 0.0226, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0191, 0.0037, 0.0148, 0.1202, 0.0791, 0.6488, 0.0467, 0.0620, 0.0056]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[5.0436e-03, 1.5003e-04, 2.6697e-03, 3.4019e-02, 2.0853e-02, 9.2157e-01,\n",
       "           6.9617e-03, 8.5433e-03, 1.8611e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0902e-03, 3.4768e-05, 1.0194e-03, 3.4278e-02, 1.3362e-02, 9.4171e-01,\n",
       "           3.7113e-03, 4.7631e-03, 3.0669e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.1986e-05, 4.3504e-08, 2.3622e-05, 3.4064e-03, 7.6640e-04, 9.9550e-01,\n",
       "           1.7253e-04, 1.1876e-04, 1.3296e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.4232e-03, 3.8399e-04, 3.3108e-03, 6.1325e-02, 1.5853e-02, 8.9078e-01,\n",
       "           9.5876e-03, 1.1919e-02, 4.1320e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.6553e-03, 6.6659e-04, 9.6380e-03, 6.1867e-02, 3.3770e-02, 8.4947e-01,\n",
       "           2.0845e-02, 1.8174e-02, 9.1897e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[4.9640e-03, 4.3088e-04, 1.0027e-02, 6.3524e-02, 5.1438e-02, 8.2166e-01,\n",
       "           2.4959e-02, 2.2176e-02, 8.2189e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0098, 0.0025, 0.0105, 0.0913, 0.0529, 0.7947, 0.0188, 0.0177, 0.0016]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0071, 0.0288, 0.1358, 0.0795, 0.6047, 0.0402, 0.0592, 0.0075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6003e-03, 8.0909e-05, 2.4186e-03, 3.1774e-02, 1.8325e-02, 9.3202e-01,\n",
       "           6.1285e-03, 6.5577e-03, 9.1599e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.7956e-03, 7.0113e-05, 9.3268e-04, 3.4926e-02, 7.8277e-03, 9.4695e-01,\n",
       "           2.0287e-03, 5.3526e-03, 1.1869e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0288, 0.0077, 0.0299, 0.1553, 0.0914, 0.5716, 0.0571, 0.0500, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.5164e-03, 1.0616e-04, 1.1056e-03, 3.1736e-02, 1.6240e-02, 9.3937e-01,\n",
       "           4.2070e-03, 4.6406e-03, 7.8487e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.7266e-04, 2.0282e-05, 1.6437e-03, 3.0768e-02, 1.1347e-02, 9.4873e-01,\n",
       "           4.0269e-03, 2.5603e-03, 3.2302e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1991e-03, 1.1119e-04, 3.1791e-03, 6.0622e-02, 2.5335e-02, 8.8993e-01,\n",
       "           8.4732e-03, 8.8673e-03, 2.7930e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0181, 0.0022, 0.0175, 0.1138, 0.0664, 0.7212, 0.0310, 0.0266, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.8840e-03, 4.0429e-05, 1.2859e-03, 7.3859e-02, 7.9795e-03, 9.0702e-01,\n",
       "           3.5822e-03, 4.3059e-03, 4.0768e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0186, 0.0026, 0.0305, 0.1179, 0.0889, 0.6776, 0.0280, 0.0323, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.8308e-05, 1.3776e-07, 2.4991e-05, 7.2382e-03, 1.3879e-03, 9.9067e-01,\n",
       "           1.4896e-04, 4.4366e-04, 8.7022e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0183, 0.0009, 0.0122, 0.0770, 0.0421, 0.7833, 0.0259, 0.0382, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6675e-03, 8.4220e-05, 7.7132e-04, 3.1897e-02, 1.0839e-02, 9.4757e-01,\n",
       "           4.0726e-03, 2.0509e-03, 4.4120e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[3.7397e-03, 1.2030e-04, 2.2087e-03, 6.9303e-02, 1.7918e-02, 8.9554e-01,\n",
       "           5.5687e-03, 5.4395e-03, 1.5967e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0222, 0.0071, 0.0197, 0.1035, 0.0820, 0.6874, 0.0399, 0.0335, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0205, 0.0015, 0.0095, 0.1430, 0.0651, 0.7158, 0.0189, 0.0239, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.0100e-04, 8.2423e-07, 7.2047e-05, 8.7077e-03, 5.0910e-03, 9.8521e-01,\n",
       "           4.3402e-04, 3.8308e-04, 4.9143e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0244, 0.0012, 0.0144, 0.1215, 0.0627, 0.7194, 0.0287, 0.0258, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0274, 0.0053, 0.0214, 0.1306, 0.0667, 0.6492, 0.0355, 0.0555, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.6988e-03, 2.6830e-04, 1.8813e-03, 6.3423e-02, 1.7775e-02, 9.0330e-01,\n",
       "           5.1145e-03, 5.2169e-03, 3.1926e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0161, 0.0017, 0.0094, 0.0995, 0.0446, 0.7787, 0.0195, 0.0283, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0265, 0.0019, 0.0141, 0.1449, 0.0770, 0.6670, 0.0312, 0.0332, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.7467e-03, 2.7168e-04, 5.9011e-03, 8.7144e-02, 4.3795e-02, 8.3207e-01,\n",
       "           1.0871e-02, 1.3817e-02, 3.8490e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6484e-03, 8.2820e-05, 4.1505e-03, 1.8410e-01, 4.1590e-02, 7.4223e-01,\n",
       "           1.2327e-02, 1.2688e-02, 1.8509e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0300, 0.0058, 0.0203, 0.1219, 0.0947, 0.6365, 0.0440, 0.0401, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.3343e-03, 8.8411e-05, 1.6207e-03, 5.3760e-02, 2.7186e-02, 9.0170e-01,\n",
       "           5.8585e-03, 5.3147e-03, 1.3518e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0313, 0.0057, 0.0287, 0.1937, 0.1252, 0.5054, 0.0616, 0.0418, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.5503e-02, 7.5912e-04, 7.2919e-03, 8.6872e-02, 4.8623e-02, 7.8929e-01,\n",
       "           1.5896e-02, 3.4348e-02, 1.4177e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0366, 0.0098, 0.0433, 0.1356, 0.1517, 0.4391, 0.0708, 0.0992, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[6.9314e-05, 3.2684e-07, 5.6569e-05, 3.6625e-02, 6.2398e-03, 9.5645e-01,\n",
       "           3.6664e-04, 1.9371e-04, 3.7855e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0435, 0.0065, 0.0347, 0.1206, 0.2306, 0.4406, 0.0508, 0.0612, 0.0116]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0393, 0.0039, 0.0235, 0.1901, 0.1304, 0.4956, 0.0543, 0.0556, 0.0073]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[5.2883e-03, 2.8170e-04, 6.7409e-03, 2.0027e-01, 1.0574e-01, 6.4250e-01,\n",
       "           2.5106e-02, 1.3657e-02, 4.2640e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[8.2839e-03, 2.2496e-04, 4.4602e-03, 1.0420e-01, 4.4727e-02, 8.1230e-01,\n",
       "           1.1257e-02, 1.4117e-02, 4.2058e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0733, 0.0190, 0.0479, 0.1520, 0.1618, 0.3595, 0.0867, 0.0724, 0.0275]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0283, 0.0032, 0.0190, 0.1887, 0.1119, 0.5880, 0.0318, 0.0262, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0420, 0.0091, 0.0236, 0.1686, 0.1173, 0.5100, 0.0648, 0.0557, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0252, 0.0026, 0.0259, 0.1727, 0.1024, 0.5590, 0.0745, 0.0346, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0466, 0.0104, 0.0367, 0.2061, 0.1645, 0.3832, 0.0860, 0.0564, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0443, 0.0093, 0.0478, 0.1579, 0.1276, 0.4470, 0.0900, 0.0664, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0425, 0.0031, 0.0261, 0.2306, 0.0980, 0.4857, 0.0614, 0.0488, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0445, 0.0072, 0.0407, 0.2223, 0.1621, 0.3318, 0.0782, 0.1047, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0771, 0.0274, 0.0657, 0.1491, 0.1533, 0.3012, 0.0947, 0.0953, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0776, 0.0279, 0.0787, 0.1858, 0.1413, 0.2464, 0.1086, 0.1023, 0.0315]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0509, 0.0091, 0.0531, 0.1765, 0.1395, 0.3923, 0.0875, 0.0769, 0.0142]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0114, 0.0006, 0.0097, 0.2349, 0.1075, 0.5592, 0.0485, 0.0271, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0436, 0.0071, 0.0377, 0.2308, 0.1444, 0.3979, 0.0956, 0.0367, 0.0062]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0507, 0.0053, 0.0369, 0.1972, 0.1083, 0.4525, 0.0755, 0.0652, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0919, 0.0408, 0.0713, 0.1508, 0.1539, 0.2216, 0.1091, 0.1176, 0.0429]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0665, 0.0271, 0.0587, 0.1986, 0.1371, 0.3116, 0.0977, 0.0811, 0.0216]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0670, 0.0220, 0.0767, 0.1751, 0.1783, 0.2588, 0.1072, 0.0898, 0.0250]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0604, 0.0115, 0.0582, 0.2140, 0.1417, 0.3156, 0.1188, 0.0697, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0659, 0.0157, 0.0457, 0.2366, 0.1362, 0.2948, 0.1155, 0.0731, 0.0166]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0803, 0.0485, 0.1008, 0.1224, 0.1703, 0.1968, 0.1213, 0.1029, 0.0567]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0890, 0.0396, 0.0845, 0.1618, 0.1473, 0.2190, 0.1115, 0.1018, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0746, 0.0104, 0.0593, 0.1876, 0.1449, 0.3171, 0.0949, 0.0966, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0746, 0.0277, 0.0685, 0.2157, 0.1480, 0.2261, 0.1136, 0.0981, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0226, 0.0603, 0.2214, 0.1589, 0.2228, 0.1217, 0.1015, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0786, 0.0245, 0.0732, 0.1795, 0.1320, 0.2721, 0.1119, 0.0997, 0.0285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0650, 0.0133, 0.0720, 0.1894, 0.1435, 0.2973, 0.1290, 0.0761, 0.0145]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0964, 0.0352, 0.0840, 0.1629, 0.1350, 0.2207, 0.1017, 0.1167, 0.0474]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0764, 0.0229, 0.0644, 0.1655, 0.1450, 0.2854, 0.1061, 0.1056, 0.0287]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0034, 0.0349, 0.2296, 0.1143, 0.4141, 0.0997, 0.0611, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0554, 0.0113, 0.0588, 0.1689, 0.1335, 0.3548, 0.1162, 0.0892, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0244, 0.0027, 0.0348, 0.2348, 0.1532, 0.4080, 0.0973, 0.0419, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0459, 0.0108, 0.0497, 0.2229, 0.1487, 0.3053, 0.1107, 0.0903, 0.0156]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0258, 0.0752, 0.1888, 0.1569, 0.2336, 0.1069, 0.1231, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0107, 0.0558, 0.2186, 0.0932, 0.3460, 0.1013, 0.1006, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0822, 0.0457, 0.0946, 0.1396, 0.1650, 0.1629, 0.1216, 0.1237, 0.0646]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0127, 0.0644, 0.1967, 0.1635, 0.2622, 0.1201, 0.1022, 0.0183]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0129, 0.0653, 0.1751, 0.1582, 0.3020, 0.1173, 0.1042, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0125, 0.0007, 0.0146, 0.1895, 0.1341, 0.5538, 0.0505, 0.0435, 0.0008]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0862, 0.0509, 0.0893, 0.1384, 0.1519, 0.2062, 0.1164, 0.1084, 0.0523]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0739, 0.0286, 0.0728, 0.1880, 0.1669, 0.2312, 0.1230, 0.0900, 0.0255]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0635, 0.0287, 0.0832, 0.1634, 0.1728, 0.2328, 0.1411, 0.0849, 0.0296]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0640, 0.0384, 0.0777, 0.1670, 0.1888, 0.2056, 0.1257, 0.0930, 0.0397]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0439, 0.0150, 0.0640, 0.2379, 0.1636, 0.2340, 0.1582, 0.0686, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0719, 0.0320, 0.0793, 0.1705, 0.1449, 0.2370, 0.1216, 0.1045, 0.0382]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0232, 0.0715, 0.2035, 0.1507, 0.2406, 0.1297, 0.0914, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0450, 0.0152, 0.0763, 0.1846, 0.1853, 0.2156, 0.1445, 0.1162, 0.0173]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0616, 0.0178, 0.0596, 0.1941, 0.1677, 0.2767, 0.1165, 0.0906, 0.0154]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0520, 0.0122, 0.0614, 0.2046, 0.1908, 0.2576, 0.1176, 0.0863, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0430, 0.0081, 0.0690, 0.1884, 0.2250, 0.2692, 0.1072, 0.0812, 0.0088]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0849, 0.0473, 0.0910, 0.1595, 0.1687, 0.1652, 0.1311, 0.1040, 0.0484]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0079, 0.0005, 0.0150, 0.2699, 0.1815, 0.3820, 0.0781, 0.0646, 0.0007]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0849, 0.0500, 0.0824, 0.1680, 0.1675, 0.1863, 0.1162, 0.0990, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0262, 0.0023, 0.0258, 0.3760, 0.1281, 0.3271, 0.0690, 0.0427, 0.0028]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0761, 0.0322, 0.0946, 0.1528, 0.1708, 0.2173, 0.1220, 0.0976, 0.0367]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0690, 0.0447, 0.0864, 0.1760, 0.1789, 0.1810, 0.1362, 0.0904, 0.0373]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0368, 0.0069, 0.0427, 0.1944, 0.1866, 0.3589, 0.1088, 0.0553, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.9578e-03, 8.0168e-04, 1.1752e-02, 4.9535e-01, 1.2197e-01, 2.5994e-01,\n",
       "           7.7337e-02, 2.3489e-02, 3.9841e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6647e-04, 6.2516e-07, 3.0290e-04, 3.7969e-01, 1.4865e-02, 5.8800e-01,\n",
       "           1.5902e-02, 9.6785e-04, 2.4811e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0509, 0.0161, 0.0721, 0.2170, 0.1919, 0.2017, 0.1306, 0.0996, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0322, 0.0033, 0.0349, 0.2617, 0.1657, 0.3582, 0.0905, 0.0493, 0.0042]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0504, 0.0094, 0.0541, 0.1946, 0.1593, 0.3265, 0.1301, 0.0647, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0720, 0.0243, 0.0803, 0.1663, 0.1842, 0.2192, 0.1280, 0.0972, 0.0285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0673, 0.0285, 0.0808, 0.1600, 0.1804, 0.2134, 0.1237, 0.1113, 0.0347]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0440, 0.0136, 0.0595, 0.2061, 0.1603, 0.2939, 0.1353, 0.0759, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0104, 0.0004, 0.0191, 0.3209, 0.1864, 0.3894, 0.0483, 0.0245, 0.0005]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0450, 0.0108, 0.0488, 0.1771, 0.1614, 0.3827, 0.1025, 0.0611, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0698, 0.0285, 0.0755, 0.1608, 0.1428, 0.2701, 0.1251, 0.1002, 0.0272]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0414, 0.0083, 0.0554, 0.2321, 0.1420, 0.3078, 0.1094, 0.0915, 0.0122]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[6.7043e-03, 1.7912e-04, 1.3032e-02, 2.0665e-01, 6.4837e-02, 6.4179e-01,\n",
       "           4.0856e-02, 2.5534e-02, 4.1998e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0308, 0.0728, 0.1573, 0.1576, 0.2390, 0.1377, 0.0946, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0661, 0.0251, 0.0676, 0.1628, 0.1868, 0.2164, 0.1375, 0.1013, 0.0363]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0071, 0.0490, 0.1457, 0.2005, 0.3697, 0.1259, 0.0553, 0.0095]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0447, 0.0156, 0.0608, 0.1592, 0.2322, 0.2672, 0.1305, 0.0744, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0242, 0.0027, 0.0296, 0.1502, 0.1555, 0.5047, 0.0962, 0.0335, 0.0035]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0448, 0.0101, 0.0473, 0.1444, 0.1494, 0.4241, 0.1013, 0.0687, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0708, 0.0265, 0.0662, 0.1461, 0.1560, 0.2827, 0.1279, 0.0977, 0.0259]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0400, 0.0098, 0.0446, 0.1249, 0.1936, 0.3700, 0.1153, 0.0893, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0663, 0.0337, 0.0728, 0.1479, 0.1706, 0.2390, 0.1318, 0.0991, 0.0390]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0328, 0.0031, 0.0281, 0.1410, 0.1440, 0.4715, 0.1179, 0.0568, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0410, 0.0081, 0.0381, 0.1244, 0.1686, 0.4012, 0.1119, 0.0964, 0.0103]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0596, 0.0274, 0.0579, 0.1618, 0.1759, 0.2736, 0.1306, 0.0900, 0.0232]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0513, 0.0202, 0.0674, 0.1373, 0.1914, 0.2853, 0.1213, 0.0998, 0.0260]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0359, 0.0118, 0.0591, 0.1351, 0.1721, 0.3249, 0.1422, 0.1031, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0732, 0.0254, 0.0765, 0.1239, 0.1662, 0.2792, 0.1396, 0.0847, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.4173e-03, 4.7859e-05, 2.2589e-03, 5.8806e-02, 7.6475e-02, 8.2252e-01,\n",
       "           2.8194e-02, 9.2211e-03, 5.8832e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0324, 0.0041, 0.0411, 0.1300, 0.1556, 0.4719, 0.1210, 0.0382, 0.0056]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2923e-03, 1.0087e-05, 1.7392e-03, 3.8844e-02, 4.3297e-02, 8.6686e-01,\n",
       "           4.5495e-02, 2.4534e-03, 1.1604e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0496, 0.0064, 0.0430, 0.1363, 0.1574, 0.4018, 0.1289, 0.0680, 0.0088]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0351, 0.0051, 0.0450, 0.1220, 0.1745, 0.4161, 0.1382, 0.0532, 0.0107]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0689, 0.0354, 0.0763, 0.1468, 0.1620, 0.2199, 0.1570, 0.0973, 0.0364]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0486, 0.0082, 0.0501, 0.1208, 0.1654, 0.4033, 0.1221, 0.0707, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0483, 0.0849, 0.1145, 0.1578, 0.1929, 0.1327, 0.1277, 0.0581]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0571, 0.0115, 0.0431, 0.1587, 0.1381, 0.3788, 0.1235, 0.0774, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0499, 0.0136, 0.0545, 0.1545, 0.1386, 0.3715, 0.1246, 0.0787, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0515, 0.0124, 0.0584, 0.1361, 0.1679, 0.3472, 0.1417, 0.0691, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0760, 0.0335, 0.0785, 0.1366, 0.1624, 0.2361, 0.1395, 0.0960, 0.0413]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0478, 0.0084, 0.0410, 0.1700, 0.1682, 0.3381, 0.1517, 0.0632, 0.0116]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0578, 0.0150, 0.0592, 0.1339, 0.1514, 0.3810, 0.1106, 0.0726, 0.0185]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0579, 0.0163, 0.0547, 0.1367, 0.1766, 0.3329, 0.1201, 0.0844, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.7019e-03, 7.3369e-05, 2.8247e-03, 1.8651e-01, 7.0547e-02, 6.5508e-01,\n",
       "           6.9104e-02, 1.2043e-02, 1.1487e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0510, 0.0100, 0.0543, 0.1647, 0.1641, 0.2987, 0.1536, 0.0856, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0762, 0.0327, 0.0823, 0.1284, 0.1551, 0.2403, 0.1378, 0.1102, 0.0369]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0609, 0.0175, 0.0595, 0.1556, 0.1440, 0.3533, 0.1034, 0.0771, 0.0287]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0162, 0.0019, 0.0167, 0.1163, 0.1641, 0.5525, 0.0859, 0.0434, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0204, 0.0036, 0.0229, 0.1429, 0.1519, 0.5497, 0.0699, 0.0339, 0.0048]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0190, 0.0023, 0.0195, 0.1234, 0.0969, 0.6251, 0.0775, 0.0343, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0758, 0.0372, 0.0837, 0.1402, 0.1459, 0.2551, 0.1106, 0.1078, 0.0438]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.0897e-02, 5.6326e-04, 1.8740e-02, 1.3102e-01, 1.1400e-01, 6.6019e-01,\n",
       "           4.3292e-02, 1.9389e-02, 1.9030e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0595, 0.0117, 0.0670, 0.1201, 0.1552, 0.3589, 0.1298, 0.0810, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0188, 0.0614, 0.1353, 0.1318, 0.3362, 0.1176, 0.1122, 0.0258]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0335, 0.0043, 0.0397, 0.0916, 0.0782, 0.6359, 0.0719, 0.0403, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.3242e-03, 1.9575e-04, 9.5126e-03, 5.2268e-02, 4.6091e-02, 8.5048e-01,\n",
       "           1.9341e-02, 1.4496e-02, 2.9088e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.2930e-03, 6.9166e-04, 1.2036e-02, 1.0643e-01, 6.4548e-02, 7.2875e-01,\n",
       "           5.1762e-02, 2.7681e-02, 8.0644e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0576, 0.0144, 0.0659, 0.1609, 0.1333, 0.3467, 0.1140, 0.0876, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0298, 0.0039, 0.0452, 0.1570, 0.1234, 0.4695, 0.0916, 0.0718, 0.0078]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0548, 0.0161, 0.0566, 0.1510, 0.1307, 0.3665, 0.0949, 0.1125, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0555, 0.0146, 0.0670, 0.1592, 0.1261, 0.3364, 0.1024, 0.1163, 0.0225]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0262, 0.0029, 0.0307, 0.1368, 0.1043, 0.5494, 0.0875, 0.0573, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.7947e-03, 2.3045e-04, 8.5734e-03, 1.2327e-01, 4.1779e-02, 7.8245e-01,\n",
       "           2.3946e-02, 1.4582e-02, 3.7029e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0469, 0.0054, 0.0383, 0.1585, 0.1356, 0.4288, 0.1174, 0.0606, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.8013e-03, 3.5813e-05, 3.0361e-03, 7.1114e-02, 2.2653e-02, 8.6818e-01,\n",
       "           1.9187e-02, 1.1917e-02, 7.0728e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0502, 0.0058, 0.0372, 0.1526, 0.1166, 0.4760, 0.0702, 0.0832, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.4857e-06, 5.3940e-10, 1.1012e-05, 6.9655e-03, 2.8741e-04, 9.9229e-01,\n",
       "           1.4475e-04, 2.9511e-04, 1.7880e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0331, 0.0044, 0.0224, 0.1716, 0.0854, 0.5555, 0.0602, 0.0633, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0202, 0.0008, 0.0222, 0.0996, 0.0595, 0.7217, 0.0463, 0.0283, 0.0013]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0262, 0.0054, 0.0393, 0.1438, 0.1309, 0.5098, 0.0700, 0.0645, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0114, 0.0581, 0.1280, 0.1358, 0.4413, 0.0959, 0.0684, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0254, 0.0023, 0.0425, 0.1189, 0.1306, 0.5462, 0.0756, 0.0531, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0391, 0.0033, 0.0356, 0.1350, 0.1200, 0.5457, 0.0658, 0.0496, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0585, 0.0184, 0.0809, 0.1603, 0.1689, 0.2837, 0.1081, 0.0959, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0329, 0.0040, 0.0431, 0.1216, 0.1137, 0.5498, 0.0666, 0.0612, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0350, 0.0061, 0.0535, 0.1564, 0.1489, 0.4309, 0.0952, 0.0658, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0637, 0.0138, 0.0781, 0.1214, 0.1307, 0.3635, 0.1172, 0.0908, 0.0208]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0802, 0.0396, 0.0897, 0.1306, 0.1677, 0.2224, 0.1176, 0.1074, 0.0448]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0548, 0.0048, 0.0483, 0.1683, 0.1280, 0.4301, 0.0909, 0.0666, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0569, 0.0099, 0.0681, 0.1704, 0.1336, 0.3562, 0.0927, 0.0962, 0.0161]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0287, 0.0022, 0.0250, 0.1376, 0.0695, 0.6450, 0.0526, 0.0367, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2015e-02, 4.5406e-04, 1.5500e-02, 1.4146e-01, 1.1208e-01, 6.5504e-01,\n",
       "           4.2723e-02, 1.9327e-02, 1.4021e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0713, 0.0275, 0.0874, 0.1502, 0.1585, 0.2274, 0.1345, 0.1082, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.2820e-05, 1.1329e-08, 1.0387e-04, 2.2055e-02, 3.5955e-03, 9.7025e-01,\n",
       "           3.8165e-03, 9.6099e-05, 5.3351e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0768, 0.0159, 0.0622, 0.1489, 0.1397, 0.3203, 0.1130, 0.0979, 0.0253]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0366, 0.0820, 0.1404, 0.1557, 0.2358, 0.1113, 0.1049, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0551, 0.0038, 0.0483, 0.1364, 0.1232, 0.4956, 0.0674, 0.0631, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0014, 0.0401, 0.1013, 0.1198, 0.6062, 0.0639, 0.0362, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0722, 0.0131, 0.0534, 0.1789, 0.1357, 0.3513, 0.0994, 0.0796, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0928, 0.0349, 0.0927, 0.1365, 0.1434, 0.2305, 0.1181, 0.1054, 0.0458]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0792, 0.0224, 0.0721, 0.1606, 0.1516, 0.2858, 0.1071, 0.0927, 0.0284]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0908, 0.0308, 0.0795, 0.1626, 0.1487, 0.2455, 0.1035, 0.0975, 0.0412]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0786, 0.0235, 0.0785, 0.1535, 0.1396, 0.2746, 0.1312, 0.0927, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0794, 0.0248, 0.0799, 0.1347, 0.1688, 0.2491, 0.1258, 0.1006, 0.0369]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1011, 0.0585, 0.0964, 0.1261, 0.1353, 0.1884, 0.1143, 0.1154, 0.0644]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0739, 0.0284, 0.0833, 0.1446, 0.1619, 0.2403, 0.1266, 0.1011, 0.0399]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0567, 0.0101, 0.0549, 0.1738, 0.1442, 0.3448, 0.1011, 0.0961, 0.0182]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0718, 0.0182, 0.0735, 0.1780, 0.1539, 0.2903, 0.1069, 0.0835, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0945, 0.0273, 0.0798, 0.1458, 0.1533, 0.2334, 0.1235, 0.1061, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0982, 0.0596, 0.0991, 0.1260, 0.1363, 0.1673, 0.1206, 0.1203, 0.0728]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0355, 0.0962, 0.1342, 0.1663, 0.1942, 0.1288, 0.1067, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0963, 0.0588, 0.0989, 0.1282, 0.1377, 0.1780, 0.1241, 0.1088, 0.0692]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0906, 0.0392, 0.0843, 0.1411, 0.1484, 0.2221, 0.1215, 0.1027, 0.0501]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[2.8827e-02, 5.1956e-04, 1.5076e-02, 1.6954e-01, 9.7590e-02, 6.1624e-01,\n",
       "           4.2468e-02, 2.8756e-02, 9.7868e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0990, 0.0626, 0.0989, 0.1215, 0.1382, 0.1681, 0.1203, 0.1199, 0.0716]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0973, 0.0422, 0.0887, 0.1193, 0.1558, 0.2064, 0.1202, 0.1118, 0.0582]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1056, 0.0656, 0.1009, 0.1204, 0.1327, 0.1553, 0.1194, 0.1201, 0.0800]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.9091e-04, 2.5918e-07, 3.6338e-04, 4.5238e-02, 7.7582e-03, 9.3710e-01,\n",
       "           7.9220e-03, 8.2900e-04, 1.9724e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0519, 0.0100, 0.0541, 0.1644, 0.1357, 0.3764, 0.1084, 0.0829, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0805, 0.0209, 0.0768, 0.1664, 0.1434, 0.2556, 0.1134, 0.1101, 0.0329]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0009, 0.0396, 0.1281, 0.1211, 0.5467, 0.0667, 0.0553, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0291, 0.0894, 0.1441, 0.1522, 0.2217, 0.1219, 0.1105, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0883, 0.0338, 0.0885, 0.1315, 0.1625, 0.2205, 0.1176, 0.1085, 0.0488]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.0654, 0.1016, 0.1235, 0.1397, 0.1554, 0.1239, 0.1177, 0.0724]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0925, 0.0364, 0.0936, 0.1440, 0.1509, 0.2156, 0.1199, 0.1020, 0.0452]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0965, 0.0354, 0.0832, 0.1500, 0.1494, 0.1941, 0.1232, 0.1168, 0.0514]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0941, 0.0393, 0.0896, 0.1423, 0.1421, 0.2035, 0.1173, 0.1171, 0.0548]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0592, 0.0969, 0.1320, 0.1367, 0.1599, 0.1203, 0.1231, 0.0712]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0917, 0.0350, 0.0913, 0.1540, 0.1518, 0.1980, 0.1213, 0.1097, 0.0472]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1048, 0.0668, 0.0966, 0.1294, 0.1345, 0.1497, 0.1224, 0.1200, 0.0758]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0430, 0.0890, 0.1346, 0.1670, 0.1963, 0.1263, 0.1074, 0.0438]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0965, 0.0652, 0.0998, 0.1224, 0.1429, 0.1598, 0.1199, 0.1212, 0.0723]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0854, 0.0399, 0.0891, 0.1411, 0.1454, 0.2009, 0.1319, 0.1149, 0.0514]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0972, 0.0701, 0.1017, 0.1220, 0.1427, 0.1473, 0.1213, 0.1177, 0.0800]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1064, 0.0671, 0.1013, 0.1200, 0.1364, 0.1563, 0.1173, 0.1184, 0.0769]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5655e-02, 3.2035e-04, 2.2072e-02, 2.4972e-01, 1.1190e-01, 5.0702e-01,\n",
       "           4.6187e-02, 4.6268e-02, 8.6893e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0946, 0.0415, 0.0927, 0.1472, 0.1540, 0.1819, 0.1307, 0.1104, 0.0471]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0664, 0.0148, 0.0747, 0.1840, 0.1586, 0.2340, 0.1326, 0.1171, 0.0178]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0570, 0.0145, 0.0674, 0.1869, 0.1593, 0.2763, 0.1160, 0.1033, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1000, 0.0541, 0.1000, 0.1302, 0.1487, 0.1525, 0.1286, 0.1228, 0.0632]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0572, 0.0087, 0.0727, 0.2008, 0.1602, 0.2693, 0.1206, 0.0949, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1032, 0.0588, 0.0988, 0.1264, 0.1356, 0.1523, 0.1237, 0.1283, 0.0729]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0335, 0.0005, 0.0285, 0.2217, 0.0972, 0.4954, 0.0775, 0.0449, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1036, 0.0600, 0.0989, 0.1303, 0.1424, 0.1565, 0.1253, 0.1182, 0.0649]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1026, 0.0653, 0.1017, 0.1276, 0.1390, 0.1363, 0.1257, 0.1238, 0.0779]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0965, 0.0558, 0.0907, 0.1376, 0.1429, 0.1544, 0.1295, 0.1250, 0.0677]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0925, 0.0462, 0.0983, 0.1395, 0.1583, 0.1646, 0.1308, 0.1154, 0.0543]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0893, 0.0373, 0.0910, 0.1714, 0.1434, 0.1796, 0.1362, 0.1134, 0.0384]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0950, 0.0507, 0.1033, 0.1387, 0.1462, 0.1564, 0.1285, 0.1175, 0.0637]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0760, 0.0232, 0.0782, 0.2018, 0.1547, 0.2021, 0.1238, 0.1094, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0059, 0.0523, 0.2549, 0.1246, 0.2702, 0.1220, 0.1017, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0074, 0.0539, 0.2532, 0.1362, 0.2557, 0.1171, 0.1116, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0761, 0.0244, 0.0715, 0.1918, 0.1558, 0.2260, 0.1221, 0.1048, 0.0275]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0195, 0.0671, 0.1949, 0.1485, 0.1970, 0.1451, 0.1191, 0.0298]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0881, 0.0298, 0.0855, 0.1645, 0.1387, 0.1819, 0.1356, 0.1365, 0.0394]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0795, 0.0179, 0.0731, 0.1998, 0.1444, 0.2181, 0.1358, 0.1083, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0816, 0.0195, 0.0733, 0.1854, 0.1539, 0.2108, 0.1301, 0.1149, 0.0306]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0703, 0.0175, 0.0750, 0.1973, 0.1480, 0.2084, 0.1375, 0.1230, 0.0230]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0330, 0.0012, 0.0297, 0.3097, 0.1287, 0.3218, 0.1213, 0.0525, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.5077e-02, 2.4557e-04, 1.5038e-02, 3.8496e-01, 9.9823e-02, 3.4351e-01,\n",
       "           9.0183e-02, 4.0070e-02, 1.0899e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0737, 0.0162, 0.0753, 0.1998, 0.1478, 0.2200, 0.1484, 0.0994, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0697, 0.0141, 0.0651, 0.2239, 0.1763, 0.1961, 0.1397, 0.0997, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0892, 0.0323, 0.0880, 0.1826, 0.1406, 0.1777, 0.1347, 0.1166, 0.0383]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0231, 0.0807, 0.1830, 0.1559, 0.2375, 0.1112, 0.1035, 0.0295]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0796, 0.0367, 0.0800, 0.1660, 0.1548, 0.1898, 0.1238, 0.1277, 0.0418]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0549, 0.0077, 0.0560, 0.2657, 0.1400, 0.2457, 0.1217, 0.0981, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0710, 0.0212, 0.0700, 0.1988, 0.1751, 0.2104, 0.1273, 0.1014, 0.0249]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0688, 0.0119, 0.0641, 0.2145, 0.1809, 0.2167, 0.1322, 0.0962, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0433, 0.0036, 0.0499, 0.2571, 0.1281, 0.3426, 0.0994, 0.0689, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0691, 0.0102, 0.0648, 0.2303, 0.1474, 0.2431, 0.1199, 0.1014, 0.0138]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0301, 0.0013, 0.0322, 0.2836, 0.1697, 0.3262, 0.0946, 0.0596, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0669, 0.0059, 0.0522, 0.2034, 0.1638, 0.2870, 0.1267, 0.0855, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0838, 0.0209, 0.0709, 0.2050, 0.1428, 0.2147, 0.1149, 0.1167, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0698, 0.0147, 0.0762, 0.1901, 0.1700, 0.2224, 0.1213, 0.1135, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0750, 0.0193, 0.0795, 0.1833, 0.1567, 0.2144, 0.1336, 0.1115, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0544, 0.0037, 0.0465, 0.2536, 0.1680, 0.2854, 0.1176, 0.0658, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0881, 0.0285, 0.0769, 0.1750, 0.1536, 0.1929, 0.1308, 0.1195, 0.0346]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0888, 0.0254, 0.0796, 0.1715, 0.1429, 0.2209, 0.1136, 0.1213, 0.0358]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0482, 0.0049, 0.0406, 0.2957, 0.1460, 0.3095, 0.0748, 0.0733, 0.0069]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0605, 0.0071, 0.0484, 0.2406, 0.1402, 0.2890, 0.1247, 0.0801, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0055, 0.0584, 0.1786, 0.1452, 0.3711, 0.0948, 0.0762, 0.0087]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0953, 0.0329, 0.0837, 0.1579, 0.1475, 0.1974, 0.1277, 0.1199, 0.0378]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0323, 0.0028, 0.0451, 0.2532, 0.2038, 0.2999, 0.1134, 0.0446, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0247, 0.0013, 0.0298, 0.2518, 0.1156, 0.4448, 0.0865, 0.0428, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.9489e-03, 7.8343e-06, 2.3157e-03, 2.9443e-01, 5.0124e-02, 6.1496e-01,\n",
       "           2.7495e-02, 6.7044e-03, 1.7184e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0690, 0.0103, 0.0576, 0.1627, 0.2089, 0.2771, 0.1173, 0.0797, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0670, 0.0167, 0.0679, 0.1900, 0.1670, 0.2374, 0.1318, 0.0986, 0.0236]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0499, 0.0061, 0.0446, 0.2175, 0.1666, 0.2907, 0.1137, 0.1008, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0294, 0.0792, 0.1641, 0.1806, 0.2213, 0.1223, 0.0886, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0319, 0.0046, 0.0392, 0.2143, 0.1791, 0.3728, 0.0967, 0.0537, 0.0077]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0850, 0.0083, 0.0476, 0.1795, 0.1638, 0.3067, 0.1262, 0.0736, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0262, 0.0880, 0.1538, 0.1648, 0.2380, 0.1096, 0.1090, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0526, 0.0035, 0.0328, 0.2001, 0.1617, 0.4071, 0.0830, 0.0555, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0129, 0.0592, 0.1814, 0.1602, 0.2971, 0.1076, 0.0923, 0.0221]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0388, 0.0006, 0.0263, 0.2817, 0.1812, 0.3852, 0.0569, 0.0275, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.1932e-03, 3.0251e-05, 4.1978e-03, 2.0855e-01, 7.6807e-02, 6.6607e-01,\n",
       "           3.4957e-02, 5.1268e-03, 6.4327e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0449, 0.0013, 0.0252, 0.2201, 0.1512, 0.4467, 0.0573, 0.0494, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0747, 0.0131, 0.0645, 0.1725, 0.1570, 0.3226, 0.0859, 0.0909, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0827, 0.0209, 0.0646, 0.1664, 0.1782, 0.2566, 0.1083, 0.0981, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0807, 0.0126, 0.0763, 0.1681, 0.1522, 0.2814, 0.1052, 0.1045, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0910, 0.0228, 0.0703, 0.1475, 0.1908, 0.2436, 0.1089, 0.0889, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0438, 0.0020, 0.0294, 0.1859, 0.1261, 0.4987, 0.0564, 0.0548, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0192, 0.0687, 0.1279, 0.1931, 0.2781, 0.1291, 0.0869, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0414, 0.0030, 0.0459, 0.1436, 0.1881, 0.4299, 0.0942, 0.0483, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0519, 0.0033, 0.0508, 0.1393, 0.1925, 0.4082, 0.0668, 0.0798, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8290e-03, 5.5726e-06, 3.1003e-03, 1.0511e-01, 8.1381e-02, 7.9835e-01,\n",
       "           6.3338e-03, 3.8572e-03, 2.7878e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0074, 0.0593, 0.1523, 0.1959, 0.3633, 0.0945, 0.0555, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0726, 0.0114, 0.0637, 0.1558, 0.1436, 0.3409, 0.0891, 0.1055, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0873, 0.0243, 0.0857, 0.1418, 0.1802, 0.2543, 0.0916, 0.1006, 0.0343]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.4432e-03, 3.1443e-06, 1.3684e-03, 3.3509e-02, 2.4225e-02, 9.2877e-01,\n",
       "           4.5620e-03, 2.1069e-03, 1.3580e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7651e-02, 4.8778e-04, 2.5753e-02, 1.4609e-01, 1.1653e-01, 6.3369e-01,\n",
       "           4.1731e-02, 1.7033e-02, 1.0313e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0428, 0.0027, 0.0305, 0.1323, 0.1782, 0.4999, 0.0663, 0.0436, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0049, 0.0672, 0.1455, 0.2048, 0.3686, 0.0772, 0.0586, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0716, 0.0075, 0.0725, 0.1613, 0.1747, 0.3259, 0.0804, 0.0916, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0205, 0.0808, 0.1478, 0.1823, 0.2649, 0.1076, 0.0818, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0615, 0.0104, 0.0843, 0.1266, 0.2101, 0.3197, 0.1021, 0.0679, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0896, 0.0287, 0.0818, 0.1375, 0.1690, 0.2618, 0.1067, 0.0910, 0.0339]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.4251e-03, 6.7285e-07, 1.2397e-03, 6.1020e-02, 3.8445e-02, 8.9126e-01,\n",
       "           4.0551e-03, 1.5471e-03, 6.2119e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0462, 0.0034, 0.0555, 0.1592, 0.1651, 0.4412, 0.0699, 0.0523, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7823e-03, 1.2831e-06, 9.3926e-04, 6.5138e-02, 3.5684e-02, 8.9302e-01,\n",
       "           1.7761e-03, 1.6490e-03, 5.0643e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0394, 0.0007, 0.0340, 0.1188, 0.1455, 0.5704, 0.0527, 0.0362, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.3101e-04, 1.2084e-07, 1.4915e-04, 2.3956e-02, 1.6401e-02, 9.5714e-01,\n",
       "           1.7196e-03, 1.9850e-04, 4.6423e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0475, 0.0025, 0.0407, 0.1173, 0.1317, 0.5566, 0.0617, 0.0385, 0.0036]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.1328e-04, 1.4467e-08, 6.4575e-05, 9.2537e-03, 6.8090e-03, 9.8306e-01,\n",
       "           5.1251e-04, 8.3288e-05, 9.5502e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0473, 0.0040, 0.0426, 0.1189, 0.1858, 0.4644, 0.0819, 0.0473, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0819, 0.0062, 0.0463, 0.1274, 0.1874, 0.3756, 0.1071, 0.0564, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0872, 0.0256, 0.0828, 0.1201, 0.1657, 0.2829, 0.1147, 0.0922, 0.0288]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0624, 0.0050, 0.0574, 0.1241, 0.1905, 0.3693, 0.1118, 0.0671, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0270, 0.0936, 0.1188, 0.1702, 0.2624, 0.1178, 0.0842, 0.0334]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0573, 0.0041, 0.0429, 0.1181, 0.2143, 0.4228, 0.0806, 0.0518, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.6853e-02, 5.6298e-04, 1.5784e-02, 9.1545e-02, 8.6926e-02, 7.3007e-01,\n",
       "           4.0239e-02, 1.7176e-02, 8.4273e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0081, 0.0639, 0.1238, 0.1747, 0.3919, 0.0763, 0.0585, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0354, 0.0015, 0.0308, 0.0847, 0.1599, 0.5967, 0.0559, 0.0333, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0966, 0.0293, 0.0932, 0.1156, 0.1698, 0.2276, 0.1187, 0.1082, 0.0410]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0264, 0.0010, 0.0198, 0.1171, 0.1896, 0.5609, 0.0596, 0.0238, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0621, 0.0039, 0.0553, 0.0914, 0.2162, 0.4195, 0.0761, 0.0675, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0844, 0.0139, 0.0769, 0.1191, 0.2203, 0.2753, 0.1077, 0.0797, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0823, 0.0142, 0.0801, 0.1173, 0.2054, 0.3069, 0.0908, 0.0799, 0.0232]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0401, 0.0032, 0.0538, 0.1006, 0.2025, 0.4880, 0.0681, 0.0384, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0616, 0.0052, 0.0500, 0.0940, 0.1745, 0.4974, 0.0670, 0.0456, 0.0045]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0300, 0.0008, 0.0190, 0.0875, 0.2364, 0.5720, 0.0383, 0.0149, 0.0010]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0442, 0.0028, 0.0469, 0.1383, 0.1988, 0.4525, 0.0654, 0.0472, 0.0039]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0577, 0.0072, 0.0481, 0.1170, 0.2191, 0.4102, 0.0842, 0.0487, 0.0078]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0483, 0.0061, 0.0510, 0.1506, 0.1720, 0.4289, 0.0691, 0.0604, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.8162e-06, 2.2930e-12, 2.4271e-06, 7.3854e-04, 6.3580e-03, 9.9284e-01,\n",
       "           4.9737e-05, 7.2994e-06, 8.6266e-11]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0158, 0.0745, 0.1332, 0.1811, 0.2944, 0.0986, 0.0899, 0.0224]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0009, 0.0272, 0.0911, 0.2100, 0.5481, 0.0631, 0.0302, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0653, 0.0072, 0.0520, 0.1201, 0.2212, 0.3826, 0.0820, 0.0555, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0387, 0.0011, 0.0416, 0.1048, 0.2073, 0.5245, 0.0504, 0.0290, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0385, 0.0013, 0.0392, 0.0870, 0.1951, 0.5612, 0.0493, 0.0267, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.9525e-03, 2.3720e-05, 4.3264e-03, 5.3099e-02, 1.7454e-01, 7.4670e-01,\n",
       "           1.1343e-02, 3.9583e-03, 6.0971e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0727, 0.0128, 0.0817, 0.1091, 0.1898, 0.3361, 0.0970, 0.0834, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0224, 0.0007, 0.0299, 0.1060, 0.1797, 0.5910, 0.0426, 0.0264, 0.0012]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0083, 0.0716, 0.1133, 0.1656, 0.3962, 0.0828, 0.0693, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0492, 0.0055, 0.0525, 0.1076, 0.2319, 0.3965, 0.0886, 0.0562, 0.0120]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[1.3127e-02, 1.9658e-04, 1.1524e-02, 1.3729e-01, 9.5579e-02, 7.0825e-01,\n",
       "           1.5768e-02, 1.7914e-02, 3.5064e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0640, 0.0061, 0.0541, 0.1266, 0.2086, 0.3989, 0.0905, 0.0429, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0554, 0.0026, 0.0346, 0.1478, 0.1811, 0.4774, 0.0481, 0.0496, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0683, 0.0113, 0.0666, 0.1368, 0.2092, 0.3226, 0.0850, 0.0800, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0046, 0.0578, 0.0987, 0.2086, 0.4085, 0.0789, 0.0706, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0534, 0.0050, 0.0486, 0.1256, 0.2325, 0.4091, 0.0851, 0.0357, 0.0050]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0465, 0.0007, 0.0284, 0.1410, 0.1566, 0.5308, 0.0583, 0.0360, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0516, 0.0071, 0.0494, 0.1450, 0.2047, 0.4208, 0.0728, 0.0422, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0708, 0.0081, 0.0577, 0.1591, 0.1901, 0.3212, 0.1049, 0.0754, 0.0127]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.9246e-02, 4.0045e-04, 1.6510e-02, 1.3925e-01, 1.5192e-01, 6.2869e-01,\n",
       "           2.9120e-02, 1.3993e-02, 8.6685e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0427, 0.0012, 0.0278, 0.1234, 0.1351, 0.5840, 0.0495, 0.0344, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0596, 0.0059, 0.0555, 0.1270, 0.2159, 0.3922, 0.0878, 0.0470, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0744, 0.0061, 0.0632, 0.1468, 0.2060, 0.3316, 0.0879, 0.0701, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0030, 0.0536, 0.1711, 0.1535, 0.4289, 0.0582, 0.0518, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1003, 0.0225, 0.0981, 0.1131, 0.1903, 0.2365, 0.1083, 0.0979, 0.0332]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0311, 0.0016, 0.0341, 0.1274, 0.1440, 0.5745, 0.0541, 0.0313, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0909, 0.0091, 0.0629, 0.1358, 0.1974, 0.3340, 0.0914, 0.0656, 0.0130]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2209e-02, 7.0802e-05, 1.0742e-02, 1.1461e-01, 1.0202e-01, 7.2496e-01,\n",
       "           2.8516e-02, 6.6424e-03, 2.2536e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0625, 0.0098, 0.0788, 0.1307, 0.1963, 0.3609, 0.0744, 0.0705, 0.0160]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0351, 0.0017, 0.0331, 0.1567, 0.1640, 0.5482, 0.0387, 0.0207, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0960, 0.0295, 0.1042, 0.1177, 0.1644, 0.2488, 0.1003, 0.0975, 0.0416]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0891, 0.0189, 0.0914, 0.1362, 0.1746, 0.2815, 0.0900, 0.0899, 0.0284]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0768, 0.0055, 0.0714, 0.1610, 0.1315, 0.3973, 0.0687, 0.0794, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.9307e-03, 3.2678e-06, 5.2637e-03, 7.3315e-02, 4.7274e-02, 8.5575e-01,\n",
       "           8.6316e-03, 5.7878e-03, 4.3043e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0658, 0.0055, 0.0727, 0.1459, 0.1771, 0.3839, 0.0755, 0.0639, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0791, 0.0131, 0.0765, 0.1558, 0.1660, 0.2916, 0.0997, 0.0957, 0.0225]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1007, 0.0261, 0.0981, 0.1229, 0.1636, 0.2330, 0.1184, 0.1050, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0321, 0.0014, 0.0415, 0.1342, 0.1877, 0.4849, 0.0730, 0.0416, 0.0035]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0909, 0.0283, 0.0978, 0.1340, 0.1710, 0.2381, 0.1125, 0.0966, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1027, 0.0143, 0.0894, 0.1490, 0.1601, 0.2840, 0.0923, 0.0859, 0.0223]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1001, 0.0118, 0.0957, 0.1332, 0.1661, 0.2885, 0.1059, 0.0799, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.2112e-03, 5.9346e-06, 2.4503e-03, 5.8930e-02, 2.6802e-02, 8.9852e-01,\n",
       "           8.7037e-03, 1.3715e-03, 8.1023e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0712, 0.0056, 0.0546, 0.1786, 0.1697, 0.3722, 0.0782, 0.0605, 0.0095]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0885, 0.0193, 0.0886, 0.1283, 0.1931, 0.2631, 0.1177, 0.0772, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0036, 0.0437, 0.1582, 0.1400, 0.4391, 0.0758, 0.0713, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0888, 0.0280, 0.1025, 0.1346, 0.1739, 0.2109, 0.1232, 0.1061, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6727e-05, 2.9860e-10, 1.0854e-04, 4.5540e-02, 1.3337e-02, 9.4032e-01,\n",
       "           6.3379e-04, 3.0102e-05, 6.6746e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1036, 0.0473, 0.1112, 0.1233, 0.1517, 0.1913, 0.1136, 0.1024, 0.0556]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0968, 0.0359, 0.1096, 0.1189, 0.1605, 0.2083, 0.1139, 0.1106, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0912, 0.0229, 0.1013, 0.1318, 0.1819, 0.2390, 0.1172, 0.0905, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0721, 0.0104, 0.0794, 0.1469, 0.1671, 0.3230, 0.0807, 0.0999, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1011, 0.0197, 0.0802, 0.1453, 0.1499, 0.2951, 0.1033, 0.0805, 0.0249]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0901, 0.0145, 0.0810, 0.1389, 0.1589, 0.2918, 0.0933, 0.1037, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0356, 0.1094, 0.1257, 0.1709, 0.1959, 0.1206, 0.1080, 0.0427]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0117, 0.0750, 0.1512, 0.1582, 0.3462, 0.0984, 0.0713, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0655, 0.0139, 0.0806, 0.1282, 0.1662, 0.3072, 0.1277, 0.0907, 0.0201]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1038, 0.0335, 0.0986, 0.1334, 0.1564, 0.2097, 0.1107, 0.1136, 0.0402]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0951, 0.0281, 0.1109, 0.1243, 0.1600, 0.2168, 0.1158, 0.1031, 0.0461]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0553, 0.0044, 0.0591, 0.1814, 0.1348, 0.3988, 0.0996, 0.0586, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0880, 0.0231, 0.0907, 0.1390, 0.1798, 0.2378, 0.1350, 0.0817, 0.0250]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0967, 0.0374, 0.0994, 0.1208, 0.1854, 0.1838, 0.1253, 0.1053, 0.0459]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1101, 0.0517, 0.0996, 0.1235, 0.1525, 0.1713, 0.1191, 0.1157, 0.0564]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0354, 0.0927, 0.1192, 0.2079, 0.1921, 0.1282, 0.0938, 0.0394]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0795, 0.0108, 0.1096, 0.1282, 0.1859, 0.2479, 0.1180, 0.1004, 0.0198]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1143, 0.0238, 0.0914, 0.1548, 0.1670, 0.2165, 0.1091, 0.0935, 0.0297]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0908, 0.0334, 0.0983, 0.1245, 0.1749, 0.2033, 0.1228, 0.1112, 0.0409]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0937, 0.0329, 0.0970, 0.1136, 0.1829, 0.2139, 0.1219, 0.0993, 0.0448]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.4368e-02, 1.8603e-04, 2.8430e-02, 9.5073e-02, 1.9854e-01, 5.9114e-01,\n",
       "           4.7224e-02, 2.4552e-02, 4.8126e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.3616e-02, 2.1173e-04, 2.0069e-02, 9.8120e-02, 1.3305e-01, 6.8159e-01,\n",
       "           2.9999e-02, 2.2339e-02, 1.0126e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0049, 0.0739, 0.1424, 0.1618, 0.3354, 0.0880, 0.0999, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0335, 0.0017, 0.0505, 0.1622, 0.2086, 0.4059, 0.0899, 0.0447, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.1270e-03, 1.4251e-05, 6.2418e-03, 7.6829e-02, 1.4013e-01, 7.4336e-01,\n",
       "           2.0566e-02, 4.6611e-03, 7.9827e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0077, 0.0719, 0.1244, 0.1988, 0.3303, 0.1075, 0.0769, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0858, 0.0190, 0.0893, 0.1184, 0.1784, 0.2515, 0.1311, 0.0982, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0016, 0.0592, 0.1086, 0.2254, 0.4138, 0.0925, 0.0523, 0.0044]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0897, 0.0203, 0.0904, 0.1367, 0.1934, 0.2264, 0.1084, 0.1103, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5221e-03, 1.0784e-06, 2.6836e-03, 3.5026e-02, 2.5537e-02, 9.2305e-01,\n",
       "           9.7508e-03, 2.4187e-03, 8.3190e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0014, 0.0347, 0.1523, 0.1562, 0.5108, 0.0676, 0.0360, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0957, 0.0277, 0.0949, 0.1212, 0.1625, 0.2000, 0.1339, 0.1227, 0.0415]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.6040e-02, 4.0320e-04, 2.5700e-02, 9.1616e-02, 1.6311e-01, 5.9462e-01,\n",
       "           7.7645e-02, 2.9605e-02, 1.2599e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0900, 0.0125, 0.0775, 0.1324, 0.1604, 0.3011, 0.1121, 0.0933, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0282, 0.0010, 0.0385, 0.1336, 0.1744, 0.4848, 0.0900, 0.0471, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0881, 0.0237, 0.0991, 0.1056, 0.1717, 0.2506, 0.1152, 0.1092, 0.0366]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0942, 0.0208, 0.0868, 0.1377, 0.1597, 0.2599, 0.1129, 0.0997, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0061, 0.0636, 0.1073, 0.1848, 0.3957, 0.1132, 0.0687, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0813, 0.0218, 0.0834, 0.1176, 0.1827, 0.2705, 0.1263, 0.0924, 0.0240]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.2920e-03, 7.0075e-07, 1.1610e-03, 3.4530e-02, 2.3427e-02, 9.3137e-01,\n",
       "           5.5792e-03, 1.6311e-03, 1.3043e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0949, 0.0228, 0.0876, 0.1214, 0.1609, 0.2543, 0.1225, 0.1046, 0.0309]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8648e-04, 1.3539e-08, 2.4567e-04, 8.1128e-03, 5.7829e-03, 9.8395e-01,\n",
       "           1.2591e-03, 4.6725e-04, 1.4965e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0497, 0.0077, 0.0768, 0.0965, 0.2390, 0.3268, 0.1116, 0.0773, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0749, 0.0098, 0.0833, 0.0913, 0.2275, 0.2995, 0.1207, 0.0785, 0.0145]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0283, 0.1041, 0.1088, 0.1709, 0.2404, 0.1229, 0.1008, 0.0331]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.8445e-04, 9.0254e-08, 4.5316e-04, 9.0519e-03, 1.2230e-02, 9.7114e-01,\n",
       "           5.1886e-03, 1.0500e-03, 9.6520e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0978, 0.0341, 0.1009, 0.1137, 0.1774, 0.1842, 0.1268, 0.1181, 0.0472]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0716, 0.0060, 0.0622, 0.1219, 0.1873, 0.3514, 0.1098, 0.0789, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.1165e-03, 3.5691e-07, 1.0297e-03, 4.1795e-02, 5.3991e-02, 8.8261e-01,\n",
       "           1.7276e-02, 2.1749e-03, 1.6193e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0924, 0.0259, 0.0911, 0.1146, 0.1910, 0.2289, 0.1306, 0.0970, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0771, 0.0098, 0.0710, 0.1312, 0.1606, 0.3237, 0.1181, 0.0898, 0.0187]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1029, 0.0215, 0.0837, 0.0926, 0.2343, 0.2124, 0.1314, 0.0925, 0.0288]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0860, 0.0093, 0.0734, 0.1401, 0.1603, 0.3204, 0.1033, 0.0952, 0.0121]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.6616e-02, 1.8077e-04, 1.9768e-02, 1.0641e-01, 5.5606e-02, 6.9727e-01,\n",
       "           5.7520e-02, 2.6165e-02, 4.6786e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0319, 0.0011, 0.0308, 0.1040, 0.1343, 0.5680, 0.0968, 0.0310, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0931, 0.0218, 0.0888, 0.1192, 0.1627, 0.2609, 0.1367, 0.0905, 0.0264]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0772, 0.0116, 0.0766, 0.1201, 0.2022, 0.2728, 0.1182, 0.1019, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0651, 0.0070, 0.0716, 0.0984, 0.2632, 0.2868, 0.1346, 0.0631, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2308e-02, 5.8348e-05, 7.9575e-03, 9.3742e-02, 1.3380e-01, 6.8125e-01,\n",
       "           5.3034e-02, 1.7641e-02, 2.1243e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0146, 0.0689, 0.1200, 0.2116, 0.3092, 0.1024, 0.0725, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0826, 0.0184, 0.0968, 0.1009, 0.2198, 0.2225, 0.1277, 0.1041, 0.0273]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0573, 0.0052, 0.0597, 0.1242, 0.1557, 0.4121, 0.0953, 0.0803, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7785e-02, 3.0777e-04, 1.5647e-02, 1.0680e-01, 1.0254e-01, 6.4832e-01,\n",
       "           7.3165e-02, 3.4354e-02, 1.0917e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1065, 0.0250, 0.0906, 0.1138, 0.1608, 0.2316, 0.1201, 0.1148, 0.0366]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1114, 0.0280, 0.0837, 0.1219, 0.1617, 0.2195, 0.1243, 0.1141, 0.0353]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0893, 0.0089, 0.0637, 0.1392, 0.1666, 0.3086, 0.1081, 0.0980, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0007, 0.0249, 0.1260, 0.1733, 0.5370, 0.0656, 0.0428, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0062, 0.0675, 0.1069, 0.1693, 0.3913, 0.0990, 0.0805, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0813, 0.0158, 0.0756, 0.1159, 0.1934, 0.2754, 0.1223, 0.0981, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0833, 0.0204, 0.0826, 0.1153, 0.1840, 0.2727, 0.1225, 0.0910, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0997, 0.0264, 0.0904, 0.1211, 0.1681, 0.2302, 0.1060, 0.1126, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1008, 0.0277, 0.0948, 0.1017, 0.1758, 0.2188, 0.1246, 0.1187, 0.0371]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0946, 0.0106, 0.0807, 0.1145, 0.1624, 0.3020, 0.1246, 0.0970, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[5.9159e-03, 3.1939e-05, 3.2650e-03, 4.4915e-02, 4.8543e-02, 8.6180e-01,\n",
       "           2.5486e-02, 9.9636e-03, 7.7341e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0866, 0.0078, 0.0879, 0.1106, 0.1666, 0.3190, 0.0988, 0.1074, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0966, 0.0297, 0.0966, 0.0951, 0.1782, 0.2379, 0.1281, 0.1068, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[7.6997e-03, 2.2290e-04, 1.2393e-02, 7.0858e-02, 1.3513e-01, 7.0835e-01,\n",
       "           4.6153e-02, 1.8424e-02, 7.6963e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0372, 0.0013, 0.0302, 0.1089, 0.1280, 0.5358, 0.1070, 0.0484, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0531, 0.0035, 0.0573, 0.0906, 0.2166, 0.3889, 0.0934, 0.0873, 0.0092]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0375, 0.0010, 0.0335, 0.0910, 0.1614, 0.5576, 0.0625, 0.0533, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0983, 0.0368, 0.0993, 0.1125, 0.1694, 0.2006, 0.1142, 0.1149, 0.0539]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0876, 0.0091, 0.0595, 0.1375, 0.1562, 0.3253, 0.1110, 0.1012, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0026, 0.0443, 0.1269, 0.1676, 0.4418, 0.0940, 0.0701, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.7353e-07, 5.7695e-15, 1.0336e-07, 2.8528e-04, 7.6066e-05, 9.9959e-01,\n",
       "           4.4816e-05, 1.5243e-06, 6.5039e-13]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0675, 0.0038, 0.0539, 0.1063, 0.1885, 0.3558, 0.1003, 0.1103, 0.0136]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.0250, 0.0956, 0.1101, 0.1707, 0.2182, 0.1256, 0.1175, 0.0377]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0656, 0.0037, 0.0416, 0.1190, 0.1999, 0.3590, 0.1052, 0.0992, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0876, 0.0219, 0.0947, 0.1114, 0.1831, 0.2267, 0.1312, 0.1068, 0.0365]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0543, 0.0045, 0.0533, 0.1147, 0.1583, 0.4423, 0.0998, 0.0630, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.4268e-03, 4.6720e-07, 6.4393e-04, 4.8895e-02, 1.9255e-02, 9.1924e-01,\n",
       "           8.0745e-03, 2.4584e-03, 4.1139e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6752e-03, 6.7114e-06, 3.1123e-03, 4.7285e-02, 5.0628e-02, 8.6070e-01,\n",
       "           2.1109e-02, 1.4370e-02, 1.0898e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0575, 0.0054, 0.0569, 0.1321, 0.1611, 0.3555, 0.1199, 0.0991, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1105, 0.0285, 0.0935, 0.1228, 0.1536, 0.2125, 0.1295, 0.1104, 0.0388]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1043, 0.0389, 0.0991, 0.1172, 0.1491, 0.1947, 0.1284, 0.1178, 0.0504]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0810, 0.0213, 0.0881, 0.1213, 0.1952, 0.2409, 0.1332, 0.0983, 0.0207]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0508, 0.0034, 0.0536, 0.1153, 0.1663, 0.4315, 0.0991, 0.0717, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0593, 0.0042, 0.0640, 0.1269, 0.1498, 0.4084, 0.1177, 0.0620, 0.0077]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0024, 0.0524, 0.1190, 0.1119, 0.4918, 0.0841, 0.0711, 0.0073]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0033, 0.0551, 0.1108, 0.1715, 0.4211, 0.1005, 0.0673, 0.0059]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0897, 0.0062, 0.0582, 0.1438, 0.1668, 0.3545, 0.0973, 0.0721, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0202, 0.0863, 0.1322, 0.1588, 0.2712, 0.1089, 0.1006, 0.0312]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0604, 0.0049, 0.0409, 0.1596, 0.1217, 0.4481, 0.0850, 0.0711, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0872, 0.0059, 0.0705, 0.1339, 0.1288, 0.3612, 0.1120, 0.0884, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0170, 0.0906, 0.1354, 0.1851, 0.2276, 0.1156, 0.1070, 0.0317]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0542, 0.0032, 0.0503, 0.1350, 0.1460, 0.4586, 0.0889, 0.0590, 0.0050]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.0811e-02, 5.4054e-05, 9.3744e-03, 7.5009e-02, 4.6837e-02, 8.2909e-01,\n",
       "           2.0704e-02, 7.9453e-03, 1.7038e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0169, 0.0931, 0.1320, 0.1947, 0.2553, 0.1225, 0.0839, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.1149e-03, 3.7612e-05, 8.5657e-03, 1.0992e-01, 7.6846e-02, 7.6137e-01,\n",
       "           2.7724e-02, 7.2835e-03, 1.4314e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0035, 0.0610, 0.1263, 0.2177, 0.3624, 0.1013, 0.0722, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0458, 0.0020, 0.0538, 0.0997, 0.1549, 0.4961, 0.0810, 0.0591, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0326, 0.0017, 0.0421, 0.1260, 0.1518, 0.5121, 0.0867, 0.0435, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0580, 0.0022, 0.0523, 0.1693, 0.1790, 0.3799, 0.0882, 0.0662, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0868, 0.0212, 0.0971, 0.1320, 0.1741, 0.2455, 0.1187, 0.0947, 0.0299]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0470, 0.0068, 0.0688, 0.1024, 0.2194, 0.3640, 0.1191, 0.0608, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0840, 0.0206, 0.1038, 0.1120, 0.1990, 0.2355, 0.1094, 0.1012, 0.0343]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.3623e-02, 2.0746e-04, 1.5912e-02, 1.2704e-01, 6.0358e-02, 7.0189e-01,\n",
       "           4.1678e-02, 2.8704e-02, 5.8630e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.8285e-02, 4.6788e-04, 2.7925e-02, 1.9006e-01, 1.2382e-01, 5.0009e-01,\n",
       "           7.7816e-02, 3.0545e-02, 9.9719e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0408, 0.0022, 0.0753, 0.1063, 0.2753, 0.3488, 0.0930, 0.0524, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0198, 0.0920, 0.1306, 0.1878, 0.2445, 0.1163, 0.0882, 0.0295]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.4708e-02, 9.7714e-05, 2.5191e-02, 1.0275e-01, 1.7510e-01, 5.9959e-01,\n",
       "           5.9874e-02, 2.2239e-02, 4.4758e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.7086e-02, 1.9330e-04, 2.2845e-02, 9.5299e-02, 1.4435e-01, 6.5512e-01,\n",
       "           4.5925e-02, 1.8714e-02, 4.6421e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0655, 0.0044, 0.0665, 0.1320, 0.1935, 0.3731, 0.0886, 0.0695, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0541, 0.0012, 0.0564, 0.1515, 0.1735, 0.4067, 0.0835, 0.0685, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0870, 0.0172, 0.1112, 0.1045, 0.2208, 0.2164, 0.1062, 0.1059, 0.0310]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0578, 0.0055, 0.0875, 0.1037, 0.2693, 0.2970, 0.1110, 0.0567, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.6911e-02, 2.1569e-04, 2.2580e-02, 1.0265e-01, 1.5388e-01, 6.3877e-01,\n",
       "           4.4006e-02, 2.0296e-02, 6.9228e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0367e-02, 6.4999e-05, 2.0096e-02, 1.0162e-01, 1.6315e-01, 6.4748e-01,\n",
       "           4.4464e-02, 1.2581e-02, 1.7173e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0489, 0.0054, 0.0684, 0.1446, 0.2475, 0.3029, 0.1096, 0.0639, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0445, 0.0025, 0.0707, 0.1124, 0.2406, 0.3667, 0.0835, 0.0739, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0879, 0.0169, 0.0929, 0.1282, 0.2049, 0.2344, 0.1295, 0.0826, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1024, 0.0124, 0.0779, 0.1230, 0.2291, 0.2415, 0.1130, 0.0789, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.4176e-07, 2.2320e-14, 2.1612e-07, 2.2857e-03, 3.9561e-04, 9.9718e-01,\n",
       "           1.3615e-04, 9.0908e-07, 2.8579e-12]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0732, 0.0155, 0.0884, 0.1119, 0.2564, 0.2384, 0.1152, 0.0794, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0525, 0.0035, 0.0526, 0.1157, 0.2092, 0.3967, 0.1084, 0.0554, 0.0061]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0407, 0.0023, 0.0645, 0.1212, 0.2824, 0.3357, 0.0901, 0.0563, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0418, 0.0012, 0.0620, 0.1112, 0.2219, 0.4241, 0.0940, 0.0395, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0545, 0.0055, 0.0708, 0.1190, 0.2465, 0.3078, 0.1246, 0.0631, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0828, 0.0134, 0.0967, 0.1160, 0.2438, 0.2168, 0.0990, 0.1085, 0.0230]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0204, 0.1035, 0.1170, 0.1966, 0.2119, 0.1236, 0.1058, 0.0342]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.7175e-03, 5.0588e-07, 2.7210e-03, 4.2988e-02, 4.5778e-02, 8.9225e-01,\n",
       "           1.1728e-02, 2.8122e-03, 6.6120e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0505, 0.0036, 0.0699, 0.1168, 0.2406, 0.3397, 0.1062, 0.0640, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0294, 0.0023, 0.0603, 0.1146, 0.2229, 0.4158, 0.0976, 0.0507, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0694, 0.0092, 0.0997, 0.1049, 0.2453, 0.2497, 0.1277, 0.0772, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0097, 0.0766, 0.1200, 0.2186, 0.2785, 0.1183, 0.0917, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0737, 0.0147, 0.0915, 0.1195, 0.2124, 0.2506, 0.1245, 0.0893, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[9.3109e-03, 4.9946e-05, 1.4204e-02, 1.0392e-01, 1.2136e-01, 6.8483e-01,\n",
       "           4.8451e-02, 1.7622e-02, 2.4035e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0623, 0.0041, 0.0659, 0.1426, 0.1763, 0.3341, 0.1279, 0.0778, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0041, 0.0729, 0.1470, 0.2122, 0.3149, 0.0930, 0.0814, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.0691e-04, 7.3386e-08, 1.6676e-03, 5.7655e-02, 1.0786e-01, 8.1624e-01,\n",
       "           1.5558e-02, 7.1582e-04, 1.2455e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0895, 0.0204, 0.1057, 0.1211, 0.1971, 0.1972, 0.1341, 0.1077, 0.0272]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0706, 0.0077, 0.0819, 0.1185, 0.2045, 0.2999, 0.1314, 0.0702, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[3.3165e-02, 2.6503e-04, 2.5067e-02, 1.9928e-01, 1.0835e-01, 5.3649e-01,\n",
       "           6.9466e-02, 2.7250e-02, 6.7056e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6869e-02, 3.1362e-04, 3.9930e-02, 1.2672e-01, 1.4428e-01, 5.2683e-01,\n",
       "           1.0357e-01, 3.0718e-02, 7.7290e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5203e-07, 4.8981e-16, 5.2992e-08, 3.4080e-03, 2.9636e-04, 9.9627e-01,\n",
       "           2.4790e-05, 1.6476e-07, 1.5931e-13]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.2410e-04, 1.1237e-06, 2.4186e-03, 7.8492e-02, 1.3182e-01, 7.6124e-01,\n",
       "           2.4108e-02, 1.2977e-03, 4.4133e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0636, 0.0087, 0.0857, 0.1317, 0.2155, 0.2617, 0.1243, 0.0929, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0927, 0.0154, 0.0907, 0.1137, 0.2110, 0.2354, 0.1344, 0.0873, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0550, 0.0027, 0.0760, 0.1145, 0.2386, 0.3446, 0.0991, 0.0644, 0.0051]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0078, 0.0773, 0.1164, 0.2363, 0.2573, 0.1337, 0.0836, 0.0120]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0485, 0.0035, 0.0603, 0.0942, 0.2802, 0.3142, 0.1328, 0.0603, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0515, 0.0037, 0.0771, 0.1538, 0.2065, 0.3104, 0.1042, 0.0837, 0.0090]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0688, 0.0095, 0.0857, 0.1257, 0.2455, 0.2429, 0.1154, 0.0874, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0023, 0.0540, 0.1004, 0.2744, 0.3492, 0.1047, 0.0635, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.5990e-04, 2.3059e-08, 7.0870e-04, 2.7269e-02, 3.5484e-02, 9.2689e-01,\n",
       "           8.7109e-03, 5.7417e-04, 3.1722e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0512, 0.0041, 0.0684, 0.1127, 0.2463, 0.3206, 0.1209, 0.0691, 0.0067]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.7664e-02, 2.4165e-04, 1.8883e-02, 1.5936e-01, 1.8246e-01, 4.8703e-01,\n",
       "           6.8008e-02, 5.5708e-02, 6.3770e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0802, 0.0162, 0.0864, 0.0907, 0.3034, 0.1775, 0.1420, 0.0856, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0730, 0.0091, 0.0953, 0.1072, 0.2929, 0.2063, 0.1137, 0.0899, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0925, 0.0307, 0.1038, 0.1072, 0.2107, 0.1792, 0.1328, 0.1081, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1041, 0.0224, 0.1001, 0.1221, 0.2127, 0.1781, 0.1244, 0.1015, 0.0345]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0674, 0.0051, 0.0873, 0.1222, 0.2626, 0.2407, 0.1150, 0.0898, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0400, 0.0021, 0.0499, 0.1202, 0.2565, 0.3433, 0.1190, 0.0637, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0743, 0.0209, 0.1102, 0.0890, 0.2707, 0.1765, 0.1153, 0.1107, 0.0322]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0525, 0.0051, 0.0873, 0.0740, 0.4145, 0.1572, 0.1152, 0.0845, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.0065, 0.0819, 0.1037, 0.2572, 0.2027, 0.1146, 0.1229, 0.0109]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1044, 0.0443, 0.1151, 0.1025, 0.1870, 0.1543, 0.1227, 0.1174, 0.0524]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1008, 0.0392, 0.1028, 0.1133, 0.1899, 0.1473, 0.1259, 0.1319, 0.0489]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.2144e-02, 9.3604e-05, 3.2464e-02, 1.0779e-01, 2.8476e-01, 3.6789e-01,\n",
       "           9.0771e-02, 9.3553e-02, 5.2131e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0799, 0.0221, 0.1025, 0.1063, 0.2586, 0.1704, 0.1281, 0.1066, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1139, 0.0308, 0.1086, 0.1024, 0.2047, 0.1524, 0.1369, 0.1150, 0.0352]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1013, 0.0247, 0.1003, 0.0941, 0.2439, 0.1566, 0.1228, 0.1185, 0.0378]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1052, 0.0227, 0.1109, 0.1034, 0.2232, 0.1575, 0.1170, 0.1232, 0.0370]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1076, 0.0342, 0.1073, 0.1022, 0.2175, 0.1379, 0.1468, 0.1081, 0.0386]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1300, 0.0199, 0.0981, 0.1081, 0.2442, 0.1508, 0.1144, 0.1129, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0982, 0.0401, 0.1048, 0.0976, 0.2231, 0.1396, 0.1315, 0.1162, 0.0489]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1020, 0.0351, 0.1094, 0.0973, 0.1898, 0.1650, 0.1243, 0.1286, 0.0486]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0715, 0.0044, 0.0771, 0.0924, 0.3027, 0.2382, 0.1270, 0.0796, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1091, 0.0262, 0.0895, 0.1027, 0.2236, 0.1629, 0.1305, 0.1253, 0.0303]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0860, 0.0165, 0.0950, 0.0920, 0.2505, 0.1919, 0.1283, 0.1094, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1473e-02, 3.6097e-04, 2.5544e-02, 6.2497e-02, 3.9972e-01, 3.4227e-01,\n",
       "           9.3792e-02, 4.2964e-02, 1.3820e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1023, 0.0287, 0.0897, 0.1129, 0.2052, 0.1818, 0.1316, 0.1136, 0.0341]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0116, 0.0873, 0.0886, 0.3048, 0.1936, 0.1345, 0.0851, 0.0156]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0820, 0.0104, 0.0971, 0.1084, 0.2319, 0.2197, 0.1354, 0.1010, 0.0139]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.7931e-03, 7.7134e-07, 1.9165e-03, 9.4313e-02, 1.9546e-01, 6.4798e-01,\n",
       "           4.5701e-02, 1.1830e-02, 1.6143e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1074, 0.0388, 0.0999, 0.0953, 0.2073, 0.1503, 0.1299, 0.1249, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.9926e-03, 8.9203e-07, 6.4528e-03, 6.8655e-02, 3.0564e-01, 5.5360e-01,\n",
       "           5.0974e-02, 1.1681e-02, 1.0795e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1199, 0.0207, 0.1091, 0.1056, 0.2126, 0.1614, 0.1268, 0.1176, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0859, 0.0060, 0.0896, 0.0936, 0.3138, 0.1739, 0.1177, 0.1087, 0.0109]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0647, 0.0073, 0.0857, 0.0734, 0.3632, 0.1645, 0.1277, 0.1008, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.1832e-03, 3.8668e-06, 8.0238e-03, 1.0945e-01, 2.0991e-01, 6.0686e-01,\n",
       "           4.2355e-02, 1.8199e-02, 1.0463e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0447, 0.0012, 0.0564, 0.0799, 0.2565, 0.3989, 0.1116, 0.0476, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0910, 0.0345, 0.1097, 0.1133, 0.1781, 0.1864, 0.1197, 0.1198, 0.0475]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1041, 0.0175, 0.1058, 0.0966, 0.2018, 0.1944, 0.1205, 0.1277, 0.0316]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0404, 0.0010, 0.0326, 0.1266, 0.2324, 0.4058, 0.0946, 0.0639, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0859, 0.0058, 0.1056, 0.0881, 0.2488, 0.2574, 0.1185, 0.0781, 0.0119]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0620, 0.0031, 0.0453, 0.1210, 0.2414, 0.3461, 0.1220, 0.0543, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.5385e-03, 3.2538e-06, 2.7868e-03, 8.8315e-02, 8.8349e-02, 7.8382e-01,\n",
       "           2.5610e-02, 7.5539e-03, 2.2208e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1083, 0.0129, 0.1104, 0.0986, 0.2079, 0.2245, 0.1277, 0.0889, 0.0209]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0973, 0.0081, 0.0951, 0.1134, 0.2204, 0.2235, 0.1156, 0.1087, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0529, 0.0042, 0.0698, 0.1448, 0.2025, 0.3266, 0.1106, 0.0811, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1059, 0.0296, 0.1001, 0.1185, 0.1698, 0.2046, 0.1193, 0.1103, 0.0420]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0612, 0.0056, 0.0899, 0.1213, 0.2204, 0.3134, 0.1000, 0.0783, 0.0100]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.5863e-02, 9.5941e-05, 1.2847e-02, 1.1690e-01, 8.6257e-02, 7.0125e-01,\n",
       "           3.7886e-02, 1.8558e-02, 3.5094e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.8516e-02, 4.6675e-04, 3.8391e-02, 1.1118e-01, 2.3426e-01, 4.9378e-01,\n",
       "           6.8653e-02, 3.3754e-02, 9.9404e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0427, 0.0010, 0.0415, 0.1075, 0.2132, 0.4515, 0.0851, 0.0543, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0639, 0.0039, 0.0647, 0.1084, 0.2331, 0.3476, 0.1100, 0.0619, 0.0065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0759, 0.0072, 0.0843, 0.1186, 0.1784, 0.3123, 0.1105, 0.0979, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0669, 0.0050, 0.0819, 0.1149, 0.2157, 0.3069, 0.1226, 0.0788, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[9.8415e-03, 2.4956e-05, 8.3376e-03, 1.1613e-01, 8.3731e-02, 7.1423e-01,\n",
       "           3.8752e-02, 2.8769e-02, 1.7953e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0775, 0.0138, 0.0885, 0.1363, 0.1913, 0.2534, 0.1087, 0.1061, 0.0244]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0258, 0.0007, 0.0360, 0.1198, 0.1083, 0.5989, 0.0657, 0.0434, 0.0013]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.5079e-03, 1.3220e-05, 5.1300e-03, 6.7412e-02, 4.2691e-02, 8.5199e-01,\n",
       "           2.3475e-02, 5.7262e-03, 5.6945e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.0444e-10, 7.1713e-18, 1.0387e-08, 6.2350e-05, 2.5873e-06, 9.9993e-01,\n",
       "           2.2927e-06, 2.7043e-08, 7.5059e-16]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.3887e-03, 2.1214e-06, 1.8871e-03, 5.4368e-02, 1.9224e-02, 9.0862e-01,\n",
       "           9.9172e-03, 3.5748e-03, 1.4472e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.0677e-03, 6.5503e-06, 2.6923e-03, 1.0706e-01, 3.2269e-02, 8.3219e-01,\n",
       "           1.5061e-02, 6.6265e-03, 3.4436e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.0769e-02, 4.5637e-05, 1.3453e-02, 7.0605e-02, 8.9452e-02, 7.6650e-01,\n",
       "           3.0379e-02, 1.8569e-02, 2.3133e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0595, 0.0036, 0.0619, 0.1607, 0.1371, 0.3992, 0.0901, 0.0807, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0597, 0.0058, 0.0884, 0.1346, 0.1748, 0.3503, 0.0958, 0.0815, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0329, 0.0013, 0.0413, 0.1183, 0.1452, 0.5466, 0.0668, 0.0447, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0311e-05, 2.2504e-10, 4.2295e-05, 3.0573e-02, 1.2043e-03, 9.6769e-01,\n",
       "           3.1589e-04, 1.5360e-04, 6.3623e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0998e-03, 3.3322e-07, 9.9497e-04, 3.7305e-02, 8.7372e-03, 9.3760e-01,\n",
       "           8.9852e-03, 4.2697e-03, 4.9726e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.4477e-02, 6.7981e-05, 1.1033e-02, 1.0739e-01, 8.0783e-02, 7.3643e-01,\n",
       "           3.5594e-02, 1.4048e-02, 1.7565e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[2.6062e-03, 8.7317e-06, 3.4537e-03, 1.1920e-01, 1.0175e-01, 7.5274e-01,\n",
       "           1.4403e-02, 5.8138e-03, 2.3115e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0956, 0.0177, 0.0891, 0.1390, 0.1702, 0.2504, 0.1185, 0.0962, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0878, 0.0184, 0.0819, 0.1199, 0.1863, 0.2860, 0.1082, 0.0877, 0.0239]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[1.3884e-02, 9.1714e-05, 1.4851e-02, 9.2978e-02, 8.9825e-02, 7.2477e-01,\n",
       "           3.2364e-02, 3.1019e-02, 2.2142e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0889, 0.0122, 0.0992, 0.1552, 0.1549, 0.2637, 0.1074, 0.0984, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0678, 0.0017, 0.0465, 0.1390, 0.1059, 0.4918, 0.0757, 0.0674, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0880, 0.0172, 0.0978, 0.1167, 0.1898, 0.2734, 0.1090, 0.0866, 0.0214]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0664, 0.0043, 0.0778, 0.1582, 0.1355, 0.3470, 0.1076, 0.0944, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.1102e-03, 3.6902e-06, 5.7054e-03, 8.4879e-02, 1.9997e-02, 8.6807e-01,\n",
       "           1.0963e-02, 6.2410e-03, 2.8610e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0044, 0.0728, 0.1476, 0.2165, 0.3461, 0.0963, 0.0620, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0260, 0.0005, 0.0236, 0.2170, 0.1107, 0.4953, 0.0757, 0.0503, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0760, 0.0071, 0.0692, 0.1618, 0.1886, 0.2763, 0.1071, 0.1006, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1049, 0.0353, 0.1085, 0.1148, 0.1838, 0.1859, 0.1180, 0.1115, 0.0372]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0989, 0.0049, 0.0919, 0.1223, 0.1992, 0.2821, 0.1006, 0.0903, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0418, 0.0011, 0.0343, 0.1372, 0.1247, 0.5458, 0.0694, 0.0424, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.0434e-04, 6.9120e-08, 4.9615e-04, 8.6754e-02, 7.7581e-03, 8.9701e-01,\n",
       "           4.2065e-03, 3.1731e-03, 1.3034e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0980, 0.0108, 0.1059, 0.1130, 0.2006, 0.2390, 0.1200, 0.0931, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1023, 0.0249, 0.1054, 0.1197, 0.1953, 0.1806, 0.1194, 0.1180, 0.0344]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1065, 0.0245, 0.1171, 0.1052, 0.1876, 0.1903, 0.1243, 0.1069, 0.0377]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.2891e-03, 9.2138e-07, 2.2220e-03, 1.7685e-01, 3.5359e-02, 7.6637e-01,\n",
       "           1.3373e-02, 3.5212e-03, 6.4389e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1156, 0.0227, 0.1192, 0.1147, 0.1734, 0.1992, 0.1079, 0.1125, 0.0348]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.9055e-04, 3.5681e-08, 3.5296e-04, 6.6401e-02, 1.7277e-02, 9.1192e-01,\n",
       "           2.5999e-03, 1.1568e-03, 4.5870e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0923, 0.0104, 0.0904, 0.1401, 0.1626, 0.2781, 0.1101, 0.0914, 0.0246]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1119, 0.0166, 0.1344, 0.0987, 0.2165, 0.1741, 0.1091, 0.1057, 0.0330]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0905, 0.0119, 0.1102, 0.1103, 0.2200, 0.2196, 0.1201, 0.0980, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0974, 0.0174, 0.1123, 0.1072, 0.2046, 0.2004, 0.1306, 0.0987, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0977, 0.0253, 0.1059, 0.1207, 0.1875, 0.1938, 0.1097, 0.1193, 0.0401]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.4275e-05, 3.0579e-10, 1.2933e-05, 1.5366e-02, 1.3090e-03, 9.8279e-01,\n",
       "           4.5582e-04, 3.1928e-05, 5.5483e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1012, 0.0064, 0.0806, 0.1239, 0.2296, 0.2703, 0.1006, 0.0756, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.0595e-02, 5.1301e-04, 2.4407e-02, 1.8251e-01, 1.0728e-01, 5.4869e-01,\n",
       "           7.7425e-02, 2.6856e-02, 1.7343e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0947, 0.0112, 0.1060, 0.1270, 0.2148, 0.2550, 0.1059, 0.0691, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0767, 0.0048, 0.0586, 0.1772, 0.1474, 0.3645, 0.0936, 0.0681, 0.0092]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.7302e-03, 1.2483e-05, 6.1699e-03, 1.2581e-01, 1.0991e-01, 7.2828e-01,\n",
       "           2.0612e-02, 5.4066e-03, 7.4213e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0286, 0.1057, 0.1244, 0.1925, 0.2050, 0.1274, 0.0901, 0.0337]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0476, 0.0011, 0.0357, 0.1753, 0.1493, 0.4538, 0.0736, 0.0596, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0615, 0.0030, 0.0590, 0.1332, 0.1977, 0.3816, 0.0933, 0.0634, 0.0075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0745, 0.0038, 0.0582, 0.1569, 0.1946, 0.3484, 0.0922, 0.0655, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0393, 0.0008, 0.0313, 0.2029, 0.1268, 0.4891, 0.0557, 0.0510, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8835e-05, 2.2652e-11, 1.2516e-05, 5.2833e-02, 1.7567e-03, 9.4496e-01,\n",
       "           3.6673e-04, 5.2643e-05, 2.9691e-10]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0285, 0.0005, 0.0215, 0.2284, 0.1090, 0.5140, 0.0655, 0.0308, 0.0017]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.4803e-03, 6.8495e-05, 5.3047e-03, 2.2135e-01, 6.2316e-02, 6.7630e-01,\n",
       "           1.9128e-02, 1.0881e-02, 1.7490e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0823, 0.0138, 0.0877, 0.1367, 0.1824, 0.2584, 0.1185, 0.0972, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0249, 0.1013, 0.1394, 0.1533, 0.2303, 0.1118, 0.1013, 0.0368]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0748, 0.0148, 0.0756, 0.1519, 0.1888, 0.2677, 0.1132, 0.0916, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0144, 0.0894, 0.1526, 0.1733, 0.2665, 0.1187, 0.0905, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.4722e-02, 5.6117e-04, 2.7263e-02, 1.6292e-01, 1.3630e-01, 5.7504e-01,\n",
       "           4.1107e-02, 3.0220e-02, 1.8711e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0100, 0.0847, 0.1648, 0.1829, 0.2798, 0.1026, 0.0837, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.1835e-02, 3.2765e-04, 1.8945e-02, 2.9743e-01, 9.6197e-02, 4.9233e-01,\n",
       "           4.5319e-02, 2.6783e-02, 8.3265e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0731, 0.0104, 0.0799, 0.1666, 0.1622, 0.3226, 0.0876, 0.0784, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[6.1646e-03, 4.6419e-05, 6.8653e-03, 2.6123e-01, 6.9545e-02, 6.2841e-01,\n",
       "           1.9535e-02, 8.0310e-03, 1.7041e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0602, 0.0071, 0.0730, 0.1485, 0.2006, 0.3394, 0.0886, 0.0674, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.8084e-02, 4.4121e-04, 3.1435e-02, 2.0229e-01, 1.5840e-01, 4.9253e-01,\n",
       "           5.1216e-02, 3.4356e-02, 1.2547e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0324, 0.0014, 0.0414, 0.1923, 0.1490, 0.4790, 0.0548, 0.0464, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0323, 0.0014, 0.0271, 0.2367, 0.1668, 0.4393, 0.0581, 0.0350, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.8600e-03, 1.0620e-05, 4.4141e-03, 2.0484e-01, 5.5908e-02, 7.0997e-01,\n",
       "           1.2309e-02, 8.6411e-03, 4.3176e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[6.0688e-03, 6.5275e-05, 1.1643e-02, 1.7902e-01, 8.3932e-02, 6.6886e-01,\n",
       "           3.3561e-02, 1.6514e-02, 3.3803e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0412, 0.0015, 0.0502, 0.1829, 0.2399, 0.3693, 0.0632, 0.0467, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2636e-02, 2.6852e-04, 1.2798e-02, 3.3461e-01, 1.2990e-01, 4.6446e-01,\n",
       "           2.1808e-02, 2.2880e-02, 6.3752e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.9411e-02, 3.5399e-04, 1.1088e-02, 2.2676e-01, 6.7245e-02, 6.0513e-01,\n",
       "           4.8413e-02, 2.0955e-02, 6.4775e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.8343e-03, 4.5441e-06, 3.7663e-03, 2.8216e-01, 2.1070e-02, 6.7757e-01,\n",
       "           9.7356e-03, 3.8480e-03, 9.9684e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0105, 0.0872, 0.1682, 0.1864, 0.2849, 0.0933, 0.0804, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0644, 0.0106, 0.0701, 0.1930, 0.1720, 0.2824, 0.0987, 0.0872, 0.0215]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0478, 0.0026, 0.0502, 0.1787, 0.1921, 0.3732, 0.0896, 0.0601, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0566, 0.0056, 0.0639, 0.1912, 0.1543, 0.3559, 0.0910, 0.0718, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.0984e-02, 3.9326e-04, 2.9128e-02, 1.8745e-01, 1.5587e-01, 5.3281e-01,\n",
       "           5.0279e-02, 2.1983e-02, 1.0968e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0763, 0.0182, 0.0782, 0.1866, 0.1624, 0.2515, 0.1162, 0.0853, 0.0252]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.3975e-08, 8.5826e-17, 1.3355e-08, 3.6971e-02, 6.5444e-06, 9.6302e-01,\n",
       "           1.5335e-06, 7.1980e-08, 1.2645e-14]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0852, 0.0153, 0.0892, 0.1575, 0.1917, 0.2471, 0.1004, 0.0880, 0.0256]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.5030e-03, 6.3248e-05, 1.1026e-02, 1.8980e-01, 7.3889e-02, 6.6412e-01,\n",
       "           2.9785e-02, 2.3600e-02, 2.1140e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.1595e-04, 1.8085e-08, 9.5959e-05, 1.0947e-01, 1.1182e-02, 8.7767e-01,\n",
       "           1.2545e-03, 2.1418e-04, 6.6928e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0770, 0.0061, 0.0607, 0.1781, 0.1608, 0.3532, 0.0801, 0.0736, 0.0104]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.4726e-04, 1.4753e-07, 9.9070e-04, 1.6566e-01, 1.4706e-02, 8.0544e-01,\n",
       "           1.0052e-02, 2.6990e-03, 2.6216e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.3584e-03, 2.0956e-05, 6.4765e-03, 2.1117e-01, 7.9289e-02, 6.5709e-01,\n",
       "           2.8121e-02, 1.1342e-02, 1.3634e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[9.4993e-04, 1.4701e-07, 9.3432e-04, 2.1226e-01, 1.5681e-02, 7.6726e-01,\n",
       "           2.1420e-03, 7.7323e-04, 8.5606e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0800, 0.0063, 0.1000, 0.1383, 0.2353, 0.2372, 0.0949, 0.0929, 0.0151]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1103, 0.0313, 0.1097, 0.1252, 0.1997, 0.1782, 0.1139, 0.0931, 0.0386]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0674, 0.0024, 0.0701, 0.1442, 0.2412, 0.3390, 0.0799, 0.0502, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1084, 0.0321, 0.1118, 0.1134, 0.1867, 0.1789, 0.1122, 0.1115, 0.0450]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.0090e-03, 1.0226e-05, 4.5788e-03, 1.3396e-01, 4.4911e-02, 7.8960e-01,\n",
       "           1.7192e-02, 4.6910e-03, 4.8637e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1116, 0.0213, 0.1091, 0.1105, 0.2228, 0.1866, 0.1214, 0.0901, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.0156e-02, 1.5859e-05, 8.4047e-03, 2.3805e-01, 1.0142e-01, 6.0195e-01,\n",
       "           2.3603e-02, 1.6301e-02, 1.0082e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1142, 0.0407, 0.1081, 0.1129, 0.1899, 0.1655, 0.1123, 0.1082, 0.0481]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0969, 0.0257, 0.0933, 0.1391, 0.1658, 0.2187, 0.1135, 0.1085, 0.0385]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.6885e-02, 8.0813e-05, 1.1198e-02, 2.2169e-01, 1.4521e-01, 5.3291e-01,\n",
       "           5.2568e-02, 1.9218e-02, 2.4215e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1308, 0.0453, 0.1123, 0.1042, 0.1874, 0.1430, 0.1096, 0.1059, 0.0614]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[3.7562e-02, 4.0708e-04, 2.5219e-02, 1.6415e-01, 1.0293e-01, 5.9702e-01,\n",
       "           4.4986e-02, 2.6374e-02, 1.3527e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.6135e-02, 1.1230e-04, 9.9799e-03, 1.9414e-01, 3.9284e-02, 6.9766e-01,\n",
       "           2.8173e-02, 1.4284e-02, 2.2789e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[8.7449e-05, 1.0089e-09, 6.0170e-05, 4.6628e-02, 3.7353e-03, 9.4876e-01,\n",
       "           5.8848e-04, 1.4144e-04, 6.8248e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0990, 0.0210, 0.1035, 0.1240, 0.2146, 0.1947, 0.1051, 0.1018, 0.0364]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0682e-02, 1.5268e-04, 1.6666e-02, 2.2732e-01, 8.4266e-02, 5.7524e-01,\n",
       "           3.9840e-02, 3.5054e-02, 7.7801e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1152, 0.0510, 0.1121, 0.1078, 0.1682, 0.1541, 0.1177, 0.1125, 0.0612]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0787, 0.0067, 0.0843, 0.1309, 0.2153, 0.2919, 0.1072, 0.0717, 0.0135]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1095, 0.0217, 0.0939, 0.1220, 0.1810, 0.2245, 0.1176, 0.0974, 0.0325]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.6393e-08, 2.0252e-17, 1.0312e-08, 4.4779e-03, 6.8078e-06, 9.9551e-01,\n",
       "           4.5188e-06, 8.9195e-08, 7.5604e-15]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[7.9107e-04, 2.9215e-07, 1.0523e-03, 2.1543e-01, 3.6378e-02, 7.3537e-01,\n",
       "           9.8032e-03, 1.1776e-03, 2.0309e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1165, 0.0279, 0.1145, 0.1044, 0.1898, 0.1823, 0.1165, 0.1069, 0.0411]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1025, 0.0170, 0.0985, 0.1038, 0.2261, 0.2123, 0.1038, 0.1042, 0.0318]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0062, 0.0688, 0.1384, 0.1629, 0.3662, 0.1097, 0.0665, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1223, 0.0398, 0.1158, 0.1046, 0.1719, 0.1648, 0.1119, 0.1110, 0.0580]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0720, 0.0063, 0.0712, 0.1394, 0.1626, 0.3484, 0.1112, 0.0758, 0.0131]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0550, 0.0016, 0.0497, 0.1734, 0.1453, 0.4565, 0.0694, 0.0455, 0.0036]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1102, 0.0470, 0.1072, 0.1123, 0.1582, 0.1661, 0.1210, 0.1175, 0.0604]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1042, 0.0129, 0.1061, 0.1161, 0.1834, 0.2613, 0.0972, 0.0955, 0.0234]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0495, 0.0010, 0.0336, 0.1784, 0.1146, 0.5267, 0.0573, 0.0349, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0048, 0.0557, 0.1652, 0.1365, 0.4045, 0.0998, 0.0636, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1153, 0.0128, 0.1055, 0.1125, 0.1849, 0.2492, 0.0963, 0.1032, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " ...]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn = GCN()\n",
    "# gcn.load_state_dict(torch.load('gcn_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 321 verilog files \n",
    "* only 3 features             [type, operation_type, num_of_connections]\n",
    "* no edge attribute\n",
    "* 18 classes \n",
    "* 200 epochs \n",
    "* learning rate = 0.01\n",
    "* Dropoout = 0.4\n",
    "* Adam Optimizer\n",
    "* train 70, test 30 (on whole dataset, not each class)\n",
    "* time of training = seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train acc:  0.2902\n",
    "* Test Acc: 0.1959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Clean dataset (by removing unnecessay, uninformative or wrong code files)\n",
    "2) remove reduntant parsing (different files but same parsing)\n",
    "3) include more informative features\n",
    "4) improve encoding format\n",
    "5) try using less classes (most important ones, so that less classes but more balanced dataset)\n",
    "6) adding more files\n",
    "7) adjusting hyperparameters such as learning rate, dropout, ...etc\n",
    "8) splitting train, val, test\n",
    "9) using equal percentages of each class (adjusting splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "droput 0.4\n",
    "\n",
    "314 files\n",
    "\n",
    "17 features (node_type)\n",
    "\n",
    "16 classes\n",
    "\n",
    "conv relu conv relu conv relu conv linear\n",
    "\n",
    "train = 40, test = 27\n",
    "\n",
    "200 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as 5 but 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = 43, test = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "conv relu conv relu conv dropout linear\n",
    "\n",
    "9 classes\n",
    "\n",
    "164 file\n",
    "\n",
    "train = 34, test = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "9 classes\n",
    "\n",
    "conv relu conv relu conv dropout linear \n",
    "\n",
    "train = 64, test = 52\n",
    "\n",
    "164 \n",
    "\n",
    "17 features (node type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
