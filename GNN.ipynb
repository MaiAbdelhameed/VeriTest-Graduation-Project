{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "!pip install torch_geometric -q\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "from torch_geometric.utils import to_dense_adj, add_self_loops\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import time\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DATA_PATH = \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['done\\\\adder11.txt', 'done\\\\adder12.txt', 'done\\\\adder13.txt', 'done\\\\adder14.txt', 'done\\\\adder15.txt', 'done\\\\adder16.txt', 'done\\\\adder17.txt', 'done\\\\adder18.txt', 'done\\\\adder19.txt', 'done\\\\adder2.txt', 'done\\\\adder20.txt', 'done\\\\adder5.txt', 'done\\\\adder6.txt', 'done\\\\adder8.txt', 'done\\\\ALU10.txt', 'done\\\\ALU13.txt', 'done\\\\ALU14.txt', 'done\\\\ALU15.txt', 'done\\\\ALU2.txt', 'done\\\\ALU6.txt', 'done\\\\ALU7.txt', 'done\\\\ALU8.txt', 'done\\\\ALU9.txt', 'done\\\\and1.txt', 'done\\\\and10.txt', 'done\\\\and12.txt', 'done\\\\and13.txt', 'done\\\\and14.txt', 'done\\\\and15.txt', 'done\\\\and16.txt', 'done\\\\and17.txt', 'done\\\\and18.txt', 'done\\\\and19.txt', 'done\\\\and2.txt', 'done\\\\and20.txt', 'done\\\\and21.txt', 'done\\\\and23.txt', 'done\\\\and25.txt', 'done\\\\and26.txt', 'done\\\\and27.txt', 'done\\\\and28.txt', 'done\\\\and29.txt', 'done\\\\and3.txt', 'done\\\\and30.txt', 'done\\\\and6.txt', 'done\\\\and7.txt', 'done\\\\and8.txt', 'done\\\\and9.txt', 'done\\\\comparator1.txt', 'done\\\\comparator13.txt', 'done\\\\comparator14.txt', 'done\\\\comparator15.txt', 'done\\\\comparator16.txt', 'done\\\\comparator17.txt', 'done\\\\comparator18.txt', 'done\\\\comparator19.txt', 'done\\\\comparator2.txt', 'done\\\\comparator20.txt', 'done\\\\comparator21.txt', 'done\\\\comparator22.txt', 'done\\\\comparator23.txt', 'done\\\\comparator6.txt', 'done\\\\comparator7.txt', 'done\\\\comparator8.txt', 'done\\\\comparator9.txt', 'done\\\\decoder1.txt', 'done\\\\decoder11.txt', 'done\\\\decoder12.txt', 'done\\\\decoder13.txt', 'done\\\\decoder14.txt', 'done\\\\decoder15.txt', 'done\\\\decoder16.txt', 'done\\\\decoder17.txt', 'done\\\\decoder18.txt', 'done\\\\decoder19.txt', 'done\\\\decoder2.txt', 'done\\\\decoder20.txt', 'done\\\\decoder21.txt', 'done\\\\decoder22.txt', 'done\\\\decoder23.txt', 'done\\\\decoder24.txt', 'done\\\\decoder25.txt', 'done\\\\decoder26.txt', 'done\\\\decoder27.txt', 'done\\\\decoder28.txt', 'done\\\\decoder29.txt', 'done\\\\decoder3.txt', 'done\\\\decoder30.txt', 'done\\\\decoder31.txt', 'done\\\\decoder32.txt', 'done\\\\decoder4.txt', 'done\\\\encoder1.txt', 'done\\\\encoder10.txt', 'done\\\\encoder11.txt', 'done\\\\encoder12.txt', 'done\\\\encoder13.txt', 'done\\\\encoder14.txt', 'done\\\\encoder15.txt', 'done\\\\encoder16.txt', 'done\\\\encoder17.txt', 'done\\\\encoder18.txt', 'done\\\\encoder19.txt', 'done\\\\encoder2.txt', 'done\\\\encoder20.txt', 'done\\\\encoder21.txt', 'done\\\\encoder24.txt', 'done\\\\encoder25.txt', 'done\\\\encoder3.txt', 'done\\\\encoder4.txt', 'done\\\\encoder5.txt', 'done\\\\encoder6.txt', 'done\\\\encoder7.txt', 'done\\\\encoder8.txt', 'done\\\\encoder9.txt', 'done\\\\mult1.txt', 'done\\\\mult10.txt', 'done\\\\mult11.txt', 'done\\\\mult12.txt', 'done\\\\mult13.txt', 'done\\\\mult14.txt', 'done\\\\mult15.txt', 'done\\\\mult16.txt', 'done\\\\mult17.txt', 'done\\\\mult18.txt', 'done\\\\mult19.txt', 'done\\\\mult2.txt', 'done\\\\mult3.txt', 'done\\\\mult30.txt', 'done\\\\mult31.txt', 'done\\\\mult32.txt', 'done\\\\mult33.txt', 'done\\\\mult34.txt', 'done\\\\mult35.txt', 'done\\\\mult4.txt', 'done\\\\mult5.txt', 'done\\\\mult6.txt', 'done\\\\mult8.txt', 'done\\\\mult9.txt', 'done\\\\mux1.txt', 'done\\\\mux10.txt', 'done\\\\mux11.txt', 'done\\\\mux12.txt', 'done\\\\mux13.txt', 'done\\\\mux14.txt', 'done\\\\mux15.txt', 'done\\\\mux16.txt', 'done\\\\mux17.txt', 'done\\\\mux18.txt', 'done\\\\mux19.txt', 'done\\\\mux2.txt', 'done\\\\mux20.txt', 'done\\\\mux21.txt', 'done\\\\mux22.txt', 'done\\\\mux23.txt', 'done\\\\mux24.txt', 'done\\\\mux25.txt', 'done\\\\mux26.txt', 'done\\\\mux3.txt', 'done\\\\mux4.txt', 'done\\\\mux5.txt', 'done\\\\mux6.txt', 'done\\\\mux7.txt', 'done\\\\mux8.txt', 'done\\\\mux9.txt', 'done\\\\nand1.txt', 'done\\\\nand10.txt', 'done\\\\nand11.txt', 'done\\\\nand12.txt', 'done\\\\nand14.txt', 'done\\\\nand15.txt', 'done\\\\nand16.txt', 'done\\\\nand17.txt', 'done\\\\nand18.txt', 'done\\\\nand19.txt', 'done\\\\nand2.txt', 'done\\\\nand20.txt', 'done\\\\nand21.txt', 'done\\\\nand22.txt', 'done\\\\nand3.txt', 'done\\\\nand4.txt', 'done\\\\nand6.txt', 'done\\\\nand7.txt', 'done\\\\nand8.txt', 'done\\\\nand9.txt', 'done\\\\nor1.txt', 'done\\\\nor10.txt', 'done\\\\nor11.txt', 'done\\\\nor12.txt', 'done\\\\nor13.txt', 'done\\\\nor14.txt', 'done\\\\nor15.txt', 'done\\\\nor16.txt', 'done\\\\nor17.txt', 'done\\\\nor2.txt', 'done\\\\nor3.txt', 'done\\\\nor4.txt', 'done\\\\nor6.txt', 'done\\\\nor7.txt', 'done\\\\nor8.txt', 'done\\\\nor9.txt', 'done\\\\not1.txt', 'done\\\\not10.txt', 'done\\\\not11.txt', 'done\\\\not12.txt', 'done\\\\not13.txt', 'done\\\\not14.txt', 'done\\\\not2.txt', 'done\\\\not3.txt', 'done\\\\not6.txt', 'done\\\\not7.txt', 'done\\\\not9.txt', 'done\\\\or1.txt', 'done\\\\or10.txt', 'done\\\\or11.txt', 'done\\\\or12.txt', 'done\\\\or13.txt', 'done\\\\or14.txt', 'done\\\\or15.txt', 'done\\\\or16.txt', 'done\\\\or17.txt', 'done\\\\or18.txt', 'done\\\\or19.txt', 'done\\\\or2.txt', 'done\\\\or20.txt', 'done\\\\or21.txt', 'done\\\\or22.txt', 'done\\\\or23.txt', 'done\\\\or24.txt', 'done\\\\or25.txt', 'done\\\\or26.txt', 'done\\\\or28.txt', 'done\\\\or4.txt', 'done\\\\or7.txt', 'done\\\\or8.txt', 'done\\\\or9.txt', 'done\\\\pe1.txt', 'done\\\\pe14.txt', 'done\\\\pe15.txt', 'done\\\\pe16.txt', 'done\\\\pe17.txt', 'done\\\\pe18.txt', 'done\\\\pe19.txt', 'done\\\\pe2.txt', 'done\\\\pe20.txt', 'done\\\\pe21.txt', 'done\\\\pe22.txt', 'done\\\\pe3.txt', 'done\\\\pe4.txt', 'done\\\\pe5.txt', 'done\\\\pe6.txt', 'done\\\\pe7.txt', 'done\\\\sub1.txt', 'done\\\\sub10.txt', 'done\\\\sub11.txt', 'done\\\\sub12.txt', 'done\\\\sub2.txt', 'done\\\\sub5.txt', 'done\\\\sub7.txt', 'done\\\\sub8.txt', 'done\\\\sub9.txt', 'done\\\\xnor1.txt', 'done\\\\xnor10.txt', 'done\\\\xnor11.txt', 'done\\\\xnor12.txt', 'done\\\\xnor13.txt', 'done\\\\xnor14.txt', 'done\\\\xnor15.txt', 'done\\\\xnor16.txt', 'done\\\\xnor17.txt', 'done\\\\xnor18.txt', 'done\\\\xnor19.txt', 'done\\\\xnor2.txt', 'done\\\\xnor20.txt', 'done\\\\xnor21.txt', 'done\\\\xnor22.txt', 'done\\\\xnor23.txt', 'done\\\\xnor24.txt', 'done\\\\xnor25.txt', 'done\\\\xnor4.txt', 'done\\\\xnor6.txt', 'done\\\\xnor7.txt', 'done\\\\xnor8.txt', 'done\\\\xnor9.txt', 'done\\\\xor1.txt', 'done\\\\xor10.txt', 'done\\\\xor12.txt', 'done\\\\xor13.txt', 'done\\\\xor14.txt', 'done\\\\xor15.txt', 'done\\\\xor16.txt', 'done\\\\xor17.txt', 'done\\\\xor18.txt', 'done\\\\xor19.txt', 'done\\\\xor2.txt', 'done\\\\xor20.txt', 'done\\\\xor21.txt', 'done\\\\xor22.txt', 'done\\\\xor23.txt', 'done\\\\xor24.txt', 'done\\\\xor25.txt', 'done\\\\xor26.txt', 'done\\\\xor3.txt', 'done\\\\xor6.txt', 'done\\\\xor7.txt', 'done\\\\xor8.txt', 'done\\\\xor9.txt']\n"
     ]
    }
   ],
   "source": [
    "def get_file_names(folder_path):\n",
    "    file_names_with_path = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "            file_names_with_path.append(os.path.join(folder_path, filename))\n",
    "    return file_names_with_path\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'done'\n",
    "verilog_files = get_file_names(folder_path)\n",
    "print(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Shuffle the dataset in-place\n",
    "# random.shuffle(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                # edge_atr = loaded_data[2]\n",
    "                label = loaded_data[3]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                # edge_atr = torch.tensor(edge_atr, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                num_nodes = x.size(0)\n",
    "                # print(num_nodes)\n",
    "                \n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                # data = Data(x=x, edge_index=edge_index, edge_attr=edge_atr ,y = y, batch = batch)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "# temp=extracting_attributes(\"./done/adder6.txt\")\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 306 Verilog files.\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[165, 17], edge_index=[2, 208], y=[1, 16], batch=[165])\n",
      "done\\adder11.txt\n",
      "done\\adder11.txt\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(verilog_files[0])\n",
    "print(dataset.verilog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data objects are unique.\n"
     ]
    }
   ],
   "source": [
    "def are_all_data_objects_unique(dataset):\n",
    "    data_objects = []\n",
    "    for data in dataset:\n",
    "        if data in data_objects:\n",
    "            return False\n",
    "        data_objects.append(data)\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "is_unique = are_all_data_objects_unique(dataset)\n",
    "if is_unique:\n",
    "    print(\"All data objects are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate data objects found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done\\\\sub5.txt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = random.randint(0, len(verilog_files))\n",
    "verilog_files[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset[num]\n",
    "data\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(range(data.num_nodes))\n",
    "edges = data.edge_index.t().tolist()\n",
    "# edge_attrs = {tuple(edge): attr.item() for edge, attr in zip(edges, data.edge_attr)}\n",
    "g.add_edges_from(edges)\n",
    "print(len(edges))\n",
    "\n",
    "# Draw the graph with edge attributes\n",
    "pos = nx.spring_layout(g)  # positions for all nodes\n",
    "nx.draw(g, pos, with_labels=True, node_color='skyblue', node_size=1500, edge_color='k', linewidths=1, font_size=15)\n",
    "nx.draw_networkx_edge_labels(g, pos, font_color='red', font_size=12)  # Add edge labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_num_nodes: 4\n",
      "max_num_nodes: 165\n",
      "mean_num_nodes: 21.76829268292683\n",
      "min_num_edges: 3\n",
      "max_num_edges: 208\n",
      "mean_num_edges: 28.341463414634145\n",
      "mean nodes degree: 1.3019607843137253\n"
     ]
    }
   ],
   "source": [
    "def graph_stat(dataset):\n",
    "    \"\"\"\n",
    "    TODO: calculate the statistics of the ENZYMES dataset.\n",
    "    \n",
    "    Outputs:\n",
    "        min_num_nodes: min number of nodes\n",
    "        max_num_nodes: max number of nodes\n",
    "        mean_num_nodes: average number of nodes\n",
    "        min_num_edges: min number of edges\n",
    "        max_num_edges: max number of edges\n",
    "        mean_num_edges: average number of edges\n",
    "    \"\"\"\n",
    "    # for ind,data in enumerate(dataset):\n",
    "        # print(verilog_files[ind])\n",
    "        # print(data)\n",
    "        # print(len(data.x[1]))\n",
    "        \n",
    "    nodes_edges = [(data.num_nodes, data.num_edges) for data in dataset]\n",
    "    num_nodes, num_edges = list(list(zip(*nodes_edges))[0]), list(list(zip(*nodes_edges))[1])\n",
    "    min_num_nodes = min(num_nodes)\n",
    "    max_num_nodes = max(num_nodes)\n",
    "    mean_num_nodes = np.mean(num_nodes)\n",
    "    min_num_edges = min(num_edges)\n",
    "    max_num_edges = max(num_edges)\n",
    "    mean_num_edges = np.mean(num_edges)\n",
    "    mean_degree = (mean_num_edges)/mean_num_nodes\n",
    "    \n",
    "    print(f\"min_num_nodes: {min_num_nodes}\")\n",
    "    print(f\"max_num_nodes: {max_num_nodes}\")\n",
    "    print(f\"mean_num_nodes: {mean_num_nodes}\")\n",
    "    print(f\"min_num_edges: {min_num_edges}\")\n",
    "    print(f\"max_num_edges: {max_num_edges}\")\n",
    "    print(f\"mean_num_edges: {mean_num_edges}\")\n",
    "    print(f\"mean nodes degree: {mean_degree}\")\n",
    "\n",
    "graph_stat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "# # Define the sizes of training, validation, and test sets\n",
    "# train_size = int(0.7 * len(dataset))  # 70% of the data for training\n",
    "# val_size = int(0.15 * len(dataset))   # 15% of the data for validation\n",
    "# test_size = len(dataset) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create DataLoader for each set\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# Define the size of the training set (e.g., 70% of the data)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "\n",
    "# Calculate the size of the testing set\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[8, 17], edge_index=[2, 7], y=[1, 9], batch=[8])\n"
     ]
    }
   ],
   "source": [
    "# len(train_loader.dataset)\n",
    "print(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "batch = next(loader_iter)\n",
    "# print(batch)\n",
    "# print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(in_channels, out_channels))\n",
    "        # Initialize the parameters.\n",
    "        stdv = 1. / math.sqrt(out_channels)\n",
    "        self.theta.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Generate the adjacency matrix with self-loop \\hat{A} using edge_index.\n",
    "            2. Calculate the diagonal degree matrix \\hat{D}.\n",
    "            3. Calculate the output X' with torch.mm using the equation above.\n",
    "        \"\"\"\n",
    "\n",
    "        num_nodes = x.shape[0]\n",
    "        A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (num_nodes, num_nodes))\n",
    "        A = A.to_dense()\n",
    "        A_hat = A + torch.eye(num_nodes)\n",
    "        \n",
    "        A_sum = torch.sum(A_hat, dim=1)\n",
    "        D = torch.pow(A_sum, -0.5)\n",
    "        D[D == float('inf')] = 0.0\n",
    "        D_hat_sqrt = torch.diag(D)\n",
    "        \n",
    "        first = torch.mm(torch.mm(D_hat_sqrt, A_hat), D_hat_sqrt)\n",
    "        second = torch.mm(x, self.theta)\n",
    "        \n",
    "        ret = torch.mm(first, second)\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gcn1): GCNConv()\n",
       "  (a1): ReLU()\n",
       "  (gcn2): GCNConv()\n",
       "  (a2): ReLU()\n",
       "  (gcn3): GCNConv()\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Define the first convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            2. Define the first activation layer using `nn.ReLU()`;\n",
    "            3. Define the second convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            4. Define the second activation layer using `nn.ReLU()`;\n",
    "            5. Define the third convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            6. Define the dropout layer using `nn.Dropout()`;\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        num_node_features = 17\n",
    "        num_output_classes = 9\n",
    "        \n",
    "        # num_channels = 32\n",
    "        \n",
    "        self.gcn1 = GCNConv(in_channels=num_node_features, out_channels=128)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.gcn2 = GCNConv(in_channels= 128, out_channels=128)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.gcn3 = GCNConv(in_channels=128, out_channels=128)\n",
    "        # self.a3 = nn.ReLU()\n",
    "        # self.gcn4 = GCNConv(in_channels=128, out_channels=128)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.linear = nn.Linear(in_features=128, out_features=num_output_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "    \n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.a1(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = self.a2(x)\n",
    "        x = self.gcn3(x, edge_index)\n",
    "        # x = self.a3(x)\n",
    "        # x = self.gcn4(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(x, dim=-1)\n",
    "        \n",
    "        return probs\n",
    "        \n",
    "        \n",
    "        \n",
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.2281, Test Acc: 0.1000\n",
      "Epoch: 002, Train Acc: 0.3246, Test Acc: 0.3200\n",
      "Epoch: 003, Train Acc: 0.3333, Test Acc: 0.3200\n",
      "Epoch: 004, Train Acc: 0.4211, Test Acc: 0.3000\n",
      "Epoch: 005, Train Acc: 0.4123, Test Acc: 0.2600\n",
      "Epoch: 006, Train Acc: 0.4211, Test Acc: 0.2600\n",
      "Epoch: 007, Train Acc: 0.4211, Test Acc: 0.2600\n",
      "Epoch: 008, Train Acc: 0.4298, Test Acc: 0.2600\n",
      "Epoch: 009, Train Acc: 0.4386, Test Acc: 0.2600\n",
      "Epoch: 010, Train Acc: 0.4298, Test Acc: 0.2800\n",
      "Epoch: 011, Train Acc: 0.4825, Test Acc: 0.3800\n",
      "Epoch: 012, Train Acc: 0.5000, Test Acc: 0.3800\n",
      "Epoch: 013, Train Acc: 0.5088, Test Acc: 0.3800\n",
      "Epoch: 014, Train Acc: 0.5088, Test Acc: 0.3600\n",
      "Epoch: 015, Train Acc: 0.5351, Test Acc: 0.4400\n",
      "Epoch: 016, Train Acc: 0.5351, Test Acc: 0.4000\n",
      "Epoch: 017, Train Acc: 0.5263, Test Acc: 0.4000\n",
      "Epoch: 018, Train Acc: 0.5351, Test Acc: 0.4200\n",
      "Epoch: 019, Train Acc: 0.5351, Test Acc: 0.3800\n",
      "Epoch: 020, Train Acc: 0.5526, Test Acc: 0.4400\n",
      "Epoch: 021, Train Acc: 0.5439, Test Acc: 0.4000\n",
      "Epoch: 022, Train Acc: 0.5614, Test Acc: 0.4000\n",
      "Epoch: 023, Train Acc: 0.5351, Test Acc: 0.3800\n",
      "Epoch: 024, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 025, Train Acc: 0.5614, Test Acc: 0.4000\n",
      "Epoch: 026, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 027, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 028, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 029, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 030, Train Acc: 0.5702, Test Acc: 0.4800\n",
      "Epoch: 031, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 032, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 033, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 034, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 035, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 036, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 037, Train Acc: 0.4912, Test Acc: 0.4000\n",
      "Epoch: 038, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 039, Train Acc: 0.5789, Test Acc: 0.3800\n",
      "Epoch: 040, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 041, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 042, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 043, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 044, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 045, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 046, Train Acc: 0.5614, Test Acc: 0.4200\n",
      "Epoch: 047, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 048, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 049, Train Acc: 0.5351, Test Acc: 0.4200\n",
      "Epoch: 050, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 051, Train Acc: 0.5965, Test Acc: 0.4400\n",
      "Epoch: 052, Train Acc: 0.5965, Test Acc: 0.4800\n",
      "Epoch: 053, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 054, Train Acc: 0.5965, Test Acc: 0.4600\n",
      "Epoch: 055, Train Acc: 0.5526, Test Acc: 0.3800\n",
      "Epoch: 056, Train Acc: 0.5877, Test Acc: 0.4400\n",
      "Epoch: 057, Train Acc: 0.5789, Test Acc: 0.3800\n",
      "Epoch: 058, Train Acc: 0.5789, Test Acc: 0.4000\n",
      "Epoch: 059, Train Acc: 0.5614, Test Acc: 0.3800\n",
      "Epoch: 060, Train Acc: 0.5965, Test Acc: 0.4400\n",
      "Epoch: 061, Train Acc: 0.5877, Test Acc: 0.4400\n",
      "Epoch: 062, Train Acc: 0.5702, Test Acc: 0.3800\n",
      "Epoch: 063, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 064, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 065, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 066, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 067, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 068, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 069, Train Acc: 0.5263, Test Acc: 0.4200\n",
      "Epoch: 070, Train Acc: 0.5614, Test Acc: 0.4600\n",
      "Epoch: 071, Train Acc: 0.5789, Test Acc: 0.4400\n",
      "Epoch: 072, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 073, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 074, Train Acc: 0.5965, Test Acc: 0.4600\n",
      "Epoch: 075, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 076, Train Acc: 0.5789, Test Acc: 0.4600\n",
      "Epoch: 077, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 078, Train Acc: 0.6053, Test Acc: 0.4400\n",
      "Epoch: 079, Train Acc: 0.5614, Test Acc: 0.4600\n",
      "Epoch: 080, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 081, Train Acc: 0.5965, Test Acc: 0.4000\n",
      "Epoch: 082, Train Acc: 0.5614, Test Acc: 0.4000\n",
      "Epoch: 083, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 084, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 085, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 086, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 087, Train Acc: 0.6140, Test Acc: 0.4600\n",
      "Epoch: 088, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 089, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 090, Train Acc: 0.6140, Test Acc: 0.4600\n",
      "Epoch: 091, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 092, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 093, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 094, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 095, Train Acc: 0.6140, Test Acc: 0.4800\n",
      "Epoch: 096, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 097, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 098, Train Acc: 0.5965, Test Acc: 0.4600\n",
      "Epoch: 099, Train Acc: 0.6140, Test Acc: 0.4800\n",
      "Epoch: 100, Train Acc: 0.5702, Test Acc: 0.4400\n",
      "Epoch: 101, Train Acc: 0.5789, Test Acc: 0.4800\n",
      "Epoch: 102, Train Acc: 0.5877, Test Acc: 0.4000\n",
      "Epoch: 103, Train Acc: 0.5877, Test Acc: 0.4600\n",
      "Epoch: 104, Train Acc: 0.5439, Test Acc: 0.4400\n",
      "Epoch: 105, Train Acc: 0.5789, Test Acc: 0.4600\n",
      "Epoch: 106, Train Acc: 0.5702, Test Acc: 0.4200\n",
      "Epoch: 107, Train Acc: 0.5965, Test Acc: 0.4800\n",
      "Epoch: 108, Train Acc: 0.6053, Test Acc: 0.4200\n",
      "Epoch: 109, Train Acc: 0.5789, Test Acc: 0.4400\n",
      "Epoch: 110, Train Acc: 0.6140, Test Acc: 0.4600\n",
      "Epoch: 111, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 112, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 113, Train Acc: 0.5965, Test Acc: 0.4200\n",
      "Epoch: 114, Train Acc: 0.5614, Test Acc: 0.4200\n",
      "Epoch: 115, Train Acc: 0.6053, Test Acc: 0.4200\n",
      "Epoch: 116, Train Acc: 0.6053, Test Acc: 0.4400\n",
      "Epoch: 117, Train Acc: 0.6053, Test Acc: 0.4200\n",
      "Epoch: 118, Train Acc: 0.5877, Test Acc: 0.4200\n",
      "Epoch: 119, Train Acc: 0.5702, Test Acc: 0.4400\n",
      "Epoch: 120, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 121, Train Acc: 0.6140, Test Acc: 0.4200\n",
      "Epoch: 122, Train Acc: 0.6140, Test Acc: 0.4800\n",
      "Epoch: 123, Train Acc: 0.6228, Test Acc: 0.4800\n",
      "Epoch: 124, Train Acc: 0.5877, Test Acc: 0.4600\n",
      "Epoch: 125, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 126, Train Acc: 0.5877, Test Acc: 0.4800\n",
      "Epoch: 127, Train Acc: 0.5702, Test Acc: 0.5000\n",
      "Epoch: 128, Train Acc: 0.5965, Test Acc: 0.4800\n",
      "Epoch: 129, Train Acc: 0.6053, Test Acc: 0.4800\n",
      "Epoch: 130, Train Acc: 0.6228, Test Acc: 0.4800\n",
      "Epoch: 131, Train Acc: 0.6228, Test Acc: 0.4600\n",
      "Epoch: 132, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 133, Train Acc: 0.6228, Test Acc: 0.4600\n",
      "Epoch: 134, Train Acc: 0.6316, Test Acc: 0.4800\n",
      "Epoch: 135, Train Acc: 0.6404, Test Acc: 0.4600\n",
      "Epoch: 136, Train Acc: 0.6491, Test Acc: 0.4800\n",
      "Epoch: 137, Train Acc: 0.5614, Test Acc: 0.5000\n",
      "Epoch: 138, Train Acc: 0.5702, Test Acc: 0.5000\n",
      "Epoch: 139, Train Acc: 0.6228, Test Acc: 0.5200\n",
      "Epoch: 140, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 141, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 142, Train Acc: 0.6491, Test Acc: 0.4800\n",
      "Epoch: 143, Train Acc: 0.6579, Test Acc: 0.5000\n",
      "Epoch: 144, Train Acc: 0.6579, Test Acc: 0.4800\n",
      "Epoch: 145, Train Acc: 0.6579, Test Acc: 0.4800\n",
      "Epoch: 146, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 147, Train Acc: 0.6053, Test Acc: 0.5000\n",
      "Epoch: 148, Train Acc: 0.5965, Test Acc: 0.5000\n",
      "Epoch: 149, Train Acc: 0.6316, Test Acc: 0.4600\n",
      "Epoch: 150, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 151, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 152, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 153, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 154, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 155, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 156, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 157, Train Acc: 0.6491, Test Acc: 0.5000\n",
      "Epoch: 158, Train Acc: 0.5877, Test Acc: 0.4400\n",
      "Epoch: 159, Train Acc: 0.6316, Test Acc: 0.4600\n",
      "Epoch: 160, Train Acc: 0.6228, Test Acc: 0.4600\n",
      "Epoch: 161, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 162, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 163, Train Acc: 0.6053, Test Acc: 0.4600\n",
      "Epoch: 164, Train Acc: 0.5702, Test Acc: 0.5000\n",
      "Epoch: 165, Train Acc: 0.5439, Test Acc: 0.4600\n",
      "Epoch: 166, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 167, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 168, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 169, Train Acc: 0.6491, Test Acc: 0.5200\n",
      "Epoch: 170, Train Acc: 0.6667, Test Acc: 0.5400\n",
      "Epoch: 171, Train Acc: 0.6579, Test Acc: 0.4600\n",
      "Epoch: 172, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 173, Train Acc: 0.6404, Test Acc: 0.4800\n",
      "Epoch: 174, Train Acc: 0.6491, Test Acc: 0.5000\n",
      "Epoch: 175, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 176, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 177, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 178, Train Acc: 0.6579, Test Acc: 0.5400\n",
      "Epoch: 179, Train Acc: 0.6667, Test Acc: 0.5000\n",
      "Epoch: 180, Train Acc: 0.6667, Test Acc: 0.5000\n",
      "Epoch: 181, Train Acc: 0.6579, Test Acc: 0.4800\n",
      "Epoch: 182, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 183, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 184, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 185, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 186, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 187, Train Acc: 0.6579, Test Acc: 0.5200\n",
      "Epoch: 188, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 189, Train Acc: 0.6667, Test Acc: 0.5200\n",
      "Epoch: 190, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 191, Train Acc: 0.6316, Test Acc: 0.5400\n",
      "Epoch: 192, Train Acc: 0.6491, Test Acc: 0.5400\n",
      "Epoch: 193, Train Acc: 0.6316, Test Acc: 0.4800\n",
      "Epoch: 194, Train Acc: 0.6491, Test Acc: 0.5400\n",
      "Epoch: 195, Train Acc: 0.6404, Test Acc: 0.5400\n",
      "Epoch: 196, Train Acc: 0.6404, Test Acc: 0.5000\n",
      "Epoch: 197, Train Acc: 0.6404, Test Acc: 0.5000\n",
      "Epoch: 198, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 199, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Epoch: 200, Train Acc: 0.6404, Test Acc: 0.5200\n",
      "Training duration: 256.8498830795288 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gcn = GCN()\n",
    "gcn = gcn.to(device)\n",
    "# print(gcn.parameters())\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.001)\n",
    "# loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "out_labels = []\n",
    "\n",
    "training_running_loss = 0.0\n",
    "def train(train_loader):\n",
    "    \n",
    "    gcn.train()\n",
    "    # print(gcn.parameters())\n",
    "    for batch_data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        for data in batch_data:\n",
    "            data = data.to(device)\n",
    "            #forward pass\n",
    "            out = gcn(data.x, data.edge_index, data.batch)\n",
    "            # calculate the loss\n",
    "            loss = criterion(out, data.y)\n",
    "            # zero the gradients of the weights so that the gradients are not accumulated\n",
    "            optimizer.zero_grad()\n",
    "            # calculate the gradients using backpropagation\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss\n",
    "            training_running_loss += loss.detach().item()\n",
    "            \n",
    "            out_labels.append((out, data.y))\n",
    "        \n",
    "        \n",
    "\n",
    "testing_labels = []\n",
    "def test(loader):\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    for batch_data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        for data in batch_data:\n",
    "            out = gcn(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            testing_labels.append(pred)\n",
    "            y_label = (data.y.tolist())\n",
    "            y_label = y_label[0].index(1.0)\n",
    "            pred_label = (pred.tolist())[0]\n",
    "            # print(pred_label)\n",
    "            # print(y_label)\n",
    "            if y_label == pred_label:\n",
    "                correct += 1            \n",
    "            # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 200\n",
    "# Your training code here\n",
    "for epoch in range(num_epochs):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch + 1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(\"Training duration:\", duration, \"seconds\")\n",
    "# with open(\"out_labels.txt\", \"w\") as output:\n",
    "#         output.write(str(out_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn.state_dict(), 'gcn_model8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\\comparator16.txt\n",
      "tensor([2])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "num = random.randint(0, len(verilog_files))\n",
    "print(verilog_files[num])\n",
    "data_trial = dataset[num]\n",
    "\n",
    "out = gcn(data_trial.x, data_trial.edge_index, data_trial.batch)\n",
    "pred = out.argmax(dim=1)\n",
    "print(pred)\n",
    "print((data_trial.y.tolist())[0].index(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[0.1118, 0.1097, 0.1018, 0.1003, 0.1178, 0.1129, 0.1081, 0.1210, 0.1167]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1059, 0.1114, 0.1021, 0.0979, 0.1187, 0.1096, 0.1097, 0.1217, 0.1228]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1078, 0.1124, 0.1022, 0.1001, 0.1166, 0.1129, 0.1110, 0.1191, 0.1179]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1095, 0.1032, 0.0960, 0.0981, 0.1257, 0.1091, 0.1091, 0.1260, 0.1233]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1096, 0.1091, 0.1006, 0.1017, 0.1147, 0.1134, 0.1094, 0.1264, 0.1150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1065, 0.1128, 0.0992, 0.1019, 0.1142, 0.1175, 0.1083, 0.1225, 0.1171]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1139, 0.1077, 0.0977, 0.0989, 0.1158, 0.1183, 0.1071, 0.1264, 0.1142]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1071, 0.1127, 0.0988, 0.0998, 0.1145, 0.1159, 0.1104, 0.1251, 0.1158]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1081, 0.1133, 0.1001, 0.0981, 0.1155, 0.1141, 0.1137, 0.1236, 0.1135]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1088, 0.1155, 0.0976, 0.0986, 0.1130, 0.1117, 0.1142, 0.1264, 0.1141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1208, 0.1097, 0.0887, 0.0928, 0.1110, 0.1075, 0.1250, 0.1413, 0.1031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1085, 0.1138, 0.0930, 0.0979, 0.1097, 0.1154, 0.1200, 0.1291, 0.1126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1081, 0.1128, 0.0957, 0.0979, 0.1128, 0.1196, 0.1140, 0.1266, 0.1124]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1090, 0.1068, 0.0943, 0.0958, 0.1132, 0.1194, 0.1234, 0.1300, 0.1082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.1046, 0.0898, 0.0919, 0.1132, 0.1343, 0.1389, 0.1294, 0.0975]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1079, 0.1126, 0.0969, 0.1008, 0.1130, 0.1213, 0.1146, 0.1225, 0.1105]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1040, 0.1074, 0.0939, 0.0977, 0.1184, 0.1195, 0.1201, 0.1306, 0.1084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1021, 0.1103, 0.1021, 0.1021, 0.1165, 0.1199, 0.1178, 0.1211, 0.1080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1002, 0.0999, 0.0859, 0.1045, 0.1209, 0.1353, 0.1212, 0.1336, 0.0984]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.1164, 0.0946, 0.0977, 0.1220, 0.1182, 0.1215, 0.1242, 0.1049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1010, 0.1056, 0.0961, 0.0998, 0.1263, 0.1173, 0.1179, 0.1289, 0.1071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1034, 0.1058, 0.0977, 0.1001, 0.1193, 0.1232, 0.1170, 0.1245, 0.1091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1033, 0.1066, 0.0917, 0.0978, 0.1267, 0.1307, 0.1195, 0.1241, 0.0995]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.1101, 0.1022, 0.1006, 0.1219, 0.1193, 0.1169, 0.1203, 0.1091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0970, 0.0955, 0.0993, 0.1000, 0.1293, 0.1240, 0.1209, 0.1316, 0.1024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0919, 0.1066, 0.1006, 0.1029, 0.1235, 0.1216, 0.1224, 0.1208, 0.1096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0944, 0.1025, 0.0988, 0.1020, 0.1265, 0.1249, 0.1308, 0.1184, 0.1018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0939, 0.1065, 0.1001, 0.1006, 0.1239, 0.1316, 0.1233, 0.1226, 0.0977]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1071, 0.1068, 0.0978, 0.1007, 0.1220, 0.1242, 0.1130, 0.1208, 0.1075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1054, 0.1044, 0.1000, 0.1063, 0.1206, 0.1223, 0.1200, 0.1145, 0.1065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0974, 0.1064, 0.1047, 0.1030, 0.1266, 0.1324, 0.1208, 0.1124, 0.0964]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0944, 0.1041, 0.1022, 0.0993, 0.1326, 0.1246, 0.1262, 0.1162, 0.1004]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0994, 0.1075, 0.1058, 0.1002, 0.1237, 0.1211, 0.1149, 0.1195, 0.1079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0977, 0.0997, 0.1061, 0.0960, 0.1364, 0.1253, 0.1237, 0.1167, 0.0984]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0930, 0.1051, 0.1022, 0.0985, 0.1289, 0.1288, 0.1202, 0.1214, 0.1020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0978, 0.1062, 0.1025, 0.0984, 0.1281, 0.1220, 0.1145, 0.1218, 0.1087]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0997, 0.1020, 0.1038, 0.1034, 0.1270, 0.1244, 0.1114, 0.1260, 0.1022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0759, 0.0706, 0.1188, 0.1135, 0.1482, 0.1511, 0.1388, 0.1186, 0.0643]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0805, 0.0973, 0.1074, 0.0960, 0.1498, 0.1348, 0.1199, 0.1220, 0.0925]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0894, 0.0933, 0.1086, 0.1078, 0.1390, 0.1399, 0.1188, 0.1172, 0.0860]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0858, 0.1033, 0.1062, 0.1049, 0.1399, 0.1306, 0.1281, 0.1101, 0.0910]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0884, 0.0970, 0.1113, 0.1019, 0.1403, 0.1238, 0.1221, 0.1135, 0.1018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0979, 0.1051, 0.1036, 0.1089, 0.1342, 0.1239, 0.1208, 0.1134, 0.0922]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.1024, 0.1066, 0.0980, 0.1387, 0.1230, 0.1176, 0.1191, 0.1021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0866, 0.1092, 0.1093, 0.1527, 0.1318, 0.1412, 0.1135, 0.0829]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0647, 0.0729, 0.1220, 0.1259, 0.1680, 0.1497, 0.1247, 0.1015, 0.0706]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0875, 0.0965, 0.1112, 0.1158, 0.1427, 0.1351, 0.1272, 0.1067, 0.0772]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0951, 0.1059, 0.1139, 0.1059, 0.1449, 0.1223, 0.1218, 0.1043, 0.0860]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0815, 0.0841, 0.1227, 0.1163, 0.1655, 0.1343, 0.1398, 0.0931, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0928, 0.0950, 0.1144, 0.1094, 0.1429, 0.1284, 0.1221, 0.1065, 0.0884]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0934, 0.1097, 0.1119, 0.1585, 0.1274, 0.1259, 0.1127, 0.0774]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0792, 0.0763, 0.1274, 0.1077, 0.1611, 0.1331, 0.1588, 0.0936, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0915, 0.0899, 0.1135, 0.1170, 0.1493, 0.1277, 0.1354, 0.1039, 0.0718]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0563, 0.0535, 0.1198, 0.1389, 0.2035, 0.1588, 0.1424, 0.0878, 0.0389]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0815, 0.0586, 0.1195, 0.1449, 0.1896, 0.1302, 0.1423, 0.0831, 0.0502]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0813, 0.0809, 0.1272, 0.1156, 0.1574, 0.1325, 0.1383, 0.0939, 0.0729]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0705, 0.0663, 0.1214, 0.1423, 0.1719, 0.1237, 0.1695, 0.0827, 0.0517]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0589, 0.0549, 0.1517, 0.1126, 0.2043, 0.1377, 0.1818, 0.0606, 0.0376]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0811, 0.0663, 0.1082, 0.1254, 0.1625, 0.1573, 0.1661, 0.0928, 0.0404]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0783, 0.0715, 0.1137, 0.1247, 0.1709, 0.1421, 0.1617, 0.0913, 0.0457]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0697, 0.0553, 0.1361, 0.1254, 0.1759, 0.1472, 0.1726, 0.0755, 0.0423]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0853, 0.0426, 0.1335, 0.1308, 0.1678, 0.1615, 0.1717, 0.0825, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0519, 0.1104, 0.1750, 0.1683, 0.1925, 0.1534, 0.0631, 0.0240]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1032, 0.0836, 0.1148, 0.1244, 0.1465, 0.1387, 0.1373, 0.0882, 0.0632]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0904, 0.0638, 0.1120, 0.1380, 0.1465, 0.1600, 0.1571, 0.0868, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0745, 0.0717, 0.1204, 0.1380, 0.1804, 0.1574, 0.1557, 0.0687, 0.0331]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1000, 0.0819, 0.1204, 0.1079, 0.1708, 0.1505, 0.1268, 0.0843, 0.0574]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0896, 0.0675, 0.0923, 0.1379, 0.1691, 0.1610, 0.1472, 0.0805, 0.0549]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0941, 0.0767, 0.1136, 0.1291, 0.1560, 0.1427, 0.1536, 0.0851, 0.0491]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1035, 0.0598, 0.1109, 0.1437, 0.1532, 0.1655, 0.1315, 0.0876, 0.0443]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1138, 0.0710, 0.1010, 0.1349, 0.1380, 0.1734, 0.1204, 0.0932, 0.0543]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0723, 0.0677, 0.1047, 0.1543, 0.1820, 0.1725, 0.1473, 0.0662, 0.0329]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0920, 0.0721, 0.1033, 0.1391, 0.1913, 0.1488, 0.1276, 0.0799, 0.0460]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0911, 0.0236, 0.0775, 0.1725, 0.2374, 0.1754, 0.1570, 0.0548, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0948, 0.0616, 0.1025, 0.1620, 0.1551, 0.1587, 0.1203, 0.0937, 0.0513]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1131, 0.0589, 0.0982, 0.1332, 0.1599, 0.1827, 0.1146, 0.0979, 0.0415]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1069, 0.0462, 0.0937, 0.1657, 0.1639, 0.2076, 0.1072, 0.0775, 0.0313]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0497, 0.1074, 0.1371, 0.1863, 0.1817, 0.1217, 0.0840, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0682, 0.0411, 0.1219, 0.1727, 0.2034, 0.1782, 0.1250, 0.0639, 0.0257]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0885, 0.0495, 0.0845, 0.1705, 0.1931, 0.1843, 0.1153, 0.0839, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1038, 0.0746, 0.1087, 0.1283, 0.1511, 0.1504, 0.1203, 0.0989, 0.0639]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0852, 0.0559, 0.0919, 0.1503, 0.1910, 0.1766, 0.1086, 0.0968, 0.0437]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1053, 0.0359, 0.0689, 0.2009, 0.2021, 0.2345, 0.0873, 0.0494, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0795, 0.0441, 0.0872, 0.1911, 0.1805, 0.1851, 0.1112, 0.0889, 0.0324]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0901, 0.0695, 0.0852, 0.1489, 0.1709, 0.1846, 0.0929, 0.1001, 0.0578]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0856, 0.0244, 0.0965, 0.1702, 0.2351, 0.2120, 0.0942, 0.0644, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0761, 0.0217, 0.0599, 0.3122, 0.2130, 0.1706, 0.0799, 0.0586, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0545, 0.0231, 0.0607, 0.2487, 0.2486, 0.2047, 0.0734, 0.0669, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0612, 0.0321, 0.0870, 0.1591, 0.2246, 0.2194, 0.0769, 0.1135, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0670, 0.0212, 0.0717, 0.2029, 0.2131, 0.2993, 0.0538, 0.0570, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0848, 0.0477, 0.0873, 0.1673, 0.1955, 0.1916, 0.0871, 0.0989, 0.0398]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0552, 0.0231, 0.0744, 0.1699, 0.3081, 0.2381, 0.0571, 0.0576, 0.0164]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0667, 0.0203, 0.0638, 0.2149, 0.2519, 0.2103, 0.0959, 0.0605, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0118, 0.0008, 0.0140, 0.2004, 0.5902, 0.1593, 0.0165, 0.0063, 0.0007]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0784, 0.0326, 0.0771, 0.1881, 0.2077, 0.2432, 0.0651, 0.0824, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0925, 0.0382, 0.0890, 0.1561, 0.1929, 0.1952, 0.1025, 0.0890, 0.0445]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0711, 0.0280, 0.0647, 0.1724, 0.2174, 0.2565, 0.0845, 0.0768, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0726, 0.0411, 0.0830, 0.1665, 0.2173, 0.2369, 0.0792, 0.0775, 0.0258]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0625, 0.0222, 0.0515, 0.1902, 0.2606, 0.2716, 0.0626, 0.0612, 0.0177]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0996, 0.0570, 0.0846, 0.1522, 0.1587, 0.1976, 0.0883, 0.1083, 0.0536]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0592, 0.0163, 0.0654, 0.2129, 0.2247, 0.2653, 0.0615, 0.0796, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0157, 0.0510, 0.2885, 0.1464, 0.3217, 0.0478, 0.0639, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0633, 0.0196, 0.0512, 0.2280, 0.1892, 0.3098, 0.0608, 0.0628, 0.0154]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0618, 0.0345, 0.0518, 0.2087, 0.2015, 0.2658, 0.0664, 0.0833, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0723, 0.0394, 0.0723, 0.1980, 0.1643, 0.2491, 0.0818, 0.0826, 0.0402]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0521, 0.0138, 0.0384, 0.2750, 0.1628, 0.3084, 0.0752, 0.0610, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0225, 0.0484, 0.2307, 0.1562, 0.3211, 0.0638, 0.0738, 0.0234]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0773, 0.0448, 0.0561, 0.2191, 0.1630, 0.2269, 0.0956, 0.0860, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0772, 0.0329, 0.0621, 0.2126, 0.1566, 0.2672, 0.0750, 0.0887, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0144, 0.0476, 0.2886, 0.1676, 0.3017, 0.0686, 0.0598, 0.0131]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0658, 0.0307, 0.0504, 0.2885, 0.1422, 0.2786, 0.0624, 0.0619, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0740, 0.0269, 0.0589, 0.2084, 0.1286, 0.2891, 0.0651, 0.1186, 0.0304]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0413, 0.0104, 0.0237, 0.2941, 0.0720, 0.4639, 0.0421, 0.0444, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0376, 0.0122, 0.0444, 0.3157, 0.1440, 0.3306, 0.0552, 0.0511, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0567, 0.0206, 0.0489, 0.2647, 0.1004, 0.3248, 0.0866, 0.0805, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0167, 0.0035, 0.0113, 0.3528, 0.0509, 0.4943, 0.0338, 0.0349, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0424, 0.0180, 0.0320, 0.3218, 0.0776, 0.3445, 0.0602, 0.0920, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0423, 0.0183, 0.0389, 0.2447, 0.0965, 0.4033, 0.0743, 0.0709, 0.0107]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0227, 0.0031, 0.0160, 0.4263, 0.0451, 0.3910, 0.0464, 0.0463, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0633, 0.0197, 0.0352, 0.2251, 0.0817, 0.4386, 0.0404, 0.0780, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.2967e-03, 3.0514e-04, 2.0014e-03, 1.5553e-01, 7.2512e-03, 8.1690e-01,\n",
       "           9.0341e-03, 6.6130e-03, 6.8204e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0160, 0.0048, 0.0108, 0.2517, 0.0270, 0.6313, 0.0183, 0.0374, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0218, 0.0034, 0.0113, 0.2594, 0.0311, 0.6152, 0.0244, 0.0303, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0123, 0.0342, 0.2747, 0.0854, 0.4173, 0.0630, 0.0620, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0105, 0.0009, 0.0044, 0.3423, 0.0098, 0.5994, 0.0120, 0.0200, 0.0006]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0441, 0.0102, 0.0225, 0.2536, 0.0616, 0.4867, 0.0705, 0.0429, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0168, 0.0254, 0.2354, 0.0877, 0.4852, 0.0450, 0.0477, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0320, 0.0451, 0.2063, 0.0737, 0.3835, 0.0722, 0.0753, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0107, 0.0011, 0.0042, 0.1618, 0.0138, 0.7829, 0.0092, 0.0152, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0242, 0.0096, 0.0235, 0.1577, 0.0781, 0.6078, 0.0467, 0.0437, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0210, 0.0051, 0.0193, 0.0960, 0.0370, 0.7492, 0.0304, 0.0382, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0413, 0.0133, 0.0331, 0.1980, 0.0813, 0.5237, 0.0415, 0.0568, 0.0111]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0428, 0.0267, 0.0480, 0.1580, 0.1125, 0.4335, 0.0817, 0.0763, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0555, 0.0151, 0.0317, 0.1643, 0.0679, 0.5386, 0.0625, 0.0497, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0107, 0.0018, 0.0122, 0.1424, 0.0372, 0.7502, 0.0222, 0.0223, 0.0010]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0197, 0.0044, 0.0104, 0.1213, 0.0348, 0.7547, 0.0367, 0.0160, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0420, 0.0072, 0.0246, 0.1530, 0.0494, 0.6289, 0.0403, 0.0496, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0195, 0.0047, 0.0170, 0.1810, 0.0489, 0.6487, 0.0382, 0.0388, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0715, 0.0339, 0.0470, 0.1921, 0.0884, 0.3864, 0.0743, 0.0822, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0259, 0.0101, 0.0224, 0.1371, 0.0530, 0.6529, 0.0502, 0.0430, 0.0054]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0673, 0.0236, 0.0417, 0.1817, 0.0845, 0.4283, 0.0647, 0.0840, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0288, 0.0098, 0.0206, 0.1397, 0.0657, 0.6240, 0.0501, 0.0550, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0168, 0.0033, 0.0104, 0.0998, 0.0304, 0.7608, 0.0308, 0.0446, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0494, 0.0166, 0.0277, 0.1261, 0.0675, 0.6007, 0.0445, 0.0546, 0.0129]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0538, 0.0188, 0.0332, 0.1528, 0.0581, 0.5535, 0.0438, 0.0731, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0126, 0.0072, 0.0149, 0.1236, 0.0663, 0.6905, 0.0314, 0.0505, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0772, 0.0320, 0.0585, 0.1501, 0.1274, 0.3228, 0.0883, 0.1123, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0074, 0.0192, 0.1550, 0.0760, 0.6231, 0.0337, 0.0557, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0584, 0.0169, 0.0469, 0.1486, 0.0997, 0.4695, 0.0648, 0.0787, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0343, 0.0166, 0.0389, 0.1245, 0.0596, 0.5945, 0.0518, 0.0719, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0442, 0.0201, 0.0515, 0.1662, 0.1167, 0.4086, 0.0670, 0.1088, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0284, 0.0092, 0.0192, 0.1370, 0.0848, 0.6219, 0.0372, 0.0553, 0.0069]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0060e-03, 4.8164e-05, 8.4278e-04, 3.1442e-02, 7.7538e-03, 9.4764e-01,\n",
       "           2.2061e-03, 9.0409e-03, 2.1262e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0292, 0.0057, 0.0174, 0.1454, 0.0467, 0.6747, 0.0343, 0.0433, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0292, 0.0074, 0.0287, 0.1566, 0.0963, 0.5007, 0.0693, 0.1013, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.8636e-04, 1.3970e-05, 2.9851e-04, 2.2131e-02, 5.7988e-03, 9.6613e-01,\n",
       "           1.6749e-03, 3.5520e-03, 1.3233e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0585, 0.0106, 0.0396, 0.2032, 0.0987, 0.4234, 0.0688, 0.0832, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0557, 0.0348, 0.0509, 0.1239, 0.1347, 0.3851, 0.0909, 0.0996, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0310, 0.0089, 0.0298, 0.1151, 0.0745, 0.5905, 0.0539, 0.0883, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0495, 0.0136, 0.0358, 0.1523, 0.0870, 0.5029, 0.0676, 0.0800, 0.0114]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0515, 0.0223, 0.0399, 0.1866, 0.1234, 0.3783, 0.0640, 0.1147, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0326, 0.0062, 0.0224, 0.1481, 0.0588, 0.6024, 0.0428, 0.0809, 0.0059]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0081, 0.0215, 0.1295, 0.0799, 0.5898, 0.0733, 0.0671, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.3440e-03, 3.1362e-04, 2.3222e-03, 4.9673e-02, 2.1704e-02, 9.0007e-01,\n",
       "           1.4016e-02, 9.3979e-03, 1.5432e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0295, 0.0074, 0.0268, 0.1803, 0.0807, 0.5572, 0.0526, 0.0592, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0349, 0.0148, 0.0383, 0.1368, 0.1252, 0.4874, 0.0633, 0.0867, 0.0127]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0164, 0.0012, 0.0075, 0.1054, 0.0504, 0.6958, 0.0151, 0.1064, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0215, 0.0070, 0.0232, 0.1430, 0.0970, 0.6010, 0.0468, 0.0557, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0252, 0.0138, 0.0354, 0.1752, 0.1157, 0.4686, 0.0717, 0.0833, 0.0112]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0256, 0.0094, 0.0228, 0.1536, 0.0922, 0.5211, 0.0784, 0.0859, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0114, 0.0046, 0.0171, 0.1237, 0.0899, 0.6408, 0.0432, 0.0641, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0665, 0.0367, 0.0558, 0.1542, 0.0992, 0.3254, 0.0823, 0.1403, 0.0397]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0083, 0.0021, 0.0080, 0.1071, 0.0438, 0.7587, 0.0298, 0.0407, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0395, 0.0159, 0.0461, 0.1422, 0.1251, 0.4356, 0.0660, 0.1136, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0246, 0.0059, 0.0250, 0.0913, 0.0848, 0.6198, 0.0596, 0.0843, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0528, 0.0212, 0.0483, 0.1493, 0.1153, 0.3800, 0.0746, 0.1331, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0244, 0.0054, 0.0199, 0.1647, 0.0671, 0.6103, 0.0545, 0.0493, 0.0044]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0622, 0.0226, 0.0542, 0.1412, 0.1273, 0.3620, 0.0961, 0.1132, 0.0212]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.8658e-03, 6.2723e-04, 3.5370e-03, 9.5234e-02, 3.5559e-02, 8.3107e-01,\n",
       "           1.1991e-02, 1.6639e-02, 4.7539e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0176, 0.0030, 0.0097, 0.1679, 0.0614, 0.6604, 0.0213, 0.0564, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0219, 0.0037, 0.0203, 0.1361, 0.0520, 0.6250, 0.0318, 0.1050, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0197, 0.0033, 0.0191, 0.1333, 0.0829, 0.6528, 0.0316, 0.0531, 0.0042]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0469, 0.0105, 0.0333, 0.1662, 0.0899, 0.4705, 0.0736, 0.1010, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0140, 0.0021, 0.0155, 0.1017, 0.0554, 0.7225, 0.0248, 0.0626, 0.0016]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0150, 0.0022, 0.0115, 0.1441, 0.0762, 0.6216, 0.0376, 0.0897, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0363, 0.0088, 0.0223, 0.1311, 0.1092, 0.5040, 0.0627, 0.1125, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0465, 0.0184, 0.0443, 0.1663, 0.1383, 0.3702, 0.0779, 0.1204, 0.0177]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0198, 0.0035, 0.0188, 0.1095, 0.1197, 0.6210, 0.0520, 0.0519, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0275, 0.0026, 0.0152, 0.1316, 0.0583, 0.6620, 0.0383, 0.0619, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0424, 0.0118, 0.0372, 0.1449, 0.1157, 0.4818, 0.0536, 0.1027, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0551, 0.0252, 0.0532, 0.1472, 0.1241, 0.3636, 0.0815, 0.1267, 0.0233]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0511, 0.0178, 0.0482, 0.1235, 0.1455, 0.3787, 0.0712, 0.1371, 0.0268]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0335, 0.0067, 0.0197, 0.1409, 0.1470, 0.4866, 0.0495, 0.1062, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0636, 0.0287, 0.0618, 0.1507, 0.1459, 0.2998, 0.0880, 0.1295, 0.0320]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0170, 0.0022, 0.0111, 0.1742, 0.0790, 0.5901, 0.0387, 0.0858, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0124, 0.0018, 0.0163, 0.1550, 0.1507, 0.5502, 0.0426, 0.0688, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0253, 0.0055, 0.0217, 0.1537, 0.1099, 0.5226, 0.0548, 0.0968, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0456, 0.0119, 0.0347, 0.1358, 0.0990, 0.5300, 0.0655, 0.0685, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0367, 0.0082, 0.0244, 0.1455, 0.1383, 0.4489, 0.0522, 0.1375, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0348, 0.0111, 0.0306, 0.1464, 0.1314, 0.4605, 0.0559, 0.1187, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0597, 0.0170, 0.0427, 0.1495, 0.1268, 0.3506, 0.0848, 0.1519, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0480, 0.0198, 0.0360, 0.1729, 0.1620, 0.3070, 0.1043, 0.1299, 0.0200]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0328, 0.0186, 0.0388, 0.1644, 0.1581, 0.3993, 0.0652, 0.1079, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0194, 0.0013, 0.0097, 0.1074, 0.0689, 0.6829, 0.0350, 0.0735, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0287, 0.0056, 0.0344, 0.1447, 0.1068, 0.4834, 0.0751, 0.1156, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0539, 0.0133, 0.0376, 0.1903, 0.1276, 0.3420, 0.0782, 0.1395, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0709, 0.0380, 0.0698, 0.1660, 0.1431, 0.2367, 0.1171, 0.1203, 0.0382]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0786, 0.0480, 0.0693, 0.1329, 0.1525, 0.2305, 0.1060, 0.1247, 0.0574]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1453e-04, 3.5857e-06, 1.4002e-04, 2.9182e-02, 1.0359e-02, 9.5206e-01,\n",
       "           1.7240e-03, 6.2075e-03, 4.9101e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0522, 0.0157, 0.0428, 0.2002, 0.1234, 0.3605, 0.0936, 0.0924, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0642, 0.0127, 0.0373, 0.1861, 0.1541, 0.3089, 0.0868, 0.1338, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0422, 0.0078, 0.0226, 0.1987, 0.1281, 0.4440, 0.0717, 0.0790, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0454, 0.0665, 0.1398, 0.1503, 0.2213, 0.1040, 0.1320, 0.0535]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0452, 0.0169, 0.0530, 0.1954, 0.1850, 0.2625, 0.1010, 0.1187, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0648, 0.0239, 0.0601, 0.1556, 0.1555, 0.2735, 0.1081, 0.1325, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0184, 0.0016, 0.0078, 0.2803, 0.1084, 0.4645, 0.0512, 0.0663, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0307, 0.0084, 0.0315, 0.2177, 0.1370, 0.3637, 0.0933, 0.1112, 0.0065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0484, 0.0202, 0.0409, 0.2243, 0.1178, 0.3321, 0.0908, 0.1061, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0749, 0.0443, 0.0776, 0.1501, 0.1372, 0.2306, 0.1131, 0.1237, 0.0485]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0449, 0.0150, 0.0365, 0.2476, 0.1387, 0.3220, 0.0778, 0.1009, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0594, 0.0181, 0.0502, 0.2369, 0.1422, 0.2327, 0.1003, 0.1389, 0.0214]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8595e-03, 2.9371e-04, 3.2481e-03, 4.9506e-01, 7.7968e-02, 3.8345e-01,\n",
       "           1.4730e-02, 2.3208e-02, 1.8038e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0186, 0.0454, 0.2229, 0.1327, 0.2922, 0.1099, 0.1021, 0.0161]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0331, 0.0019, 0.0139, 0.3394, 0.0541, 0.4479, 0.0442, 0.0636, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0696, 0.0310, 0.0660, 0.1993, 0.1384, 0.2412, 0.1159, 0.1047, 0.0340]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0268, 0.0033, 0.0176, 0.2968, 0.0719, 0.4533, 0.0566, 0.0708, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0868, 0.0319, 0.0816, 0.1807, 0.1281, 0.2245, 0.1037, 0.1210, 0.0418]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0319, 0.0671, 0.1893, 0.1471, 0.2572, 0.1081, 0.1043, 0.0299]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0473, 0.0076, 0.0395, 0.2757, 0.0817, 0.3803, 0.0890, 0.0714, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0609, 0.0134, 0.0432, 0.2169, 0.1124, 0.3579, 0.0850, 0.0923, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0824, 0.0327, 0.0672, 0.1671, 0.1294, 0.2695, 0.1142, 0.1042, 0.0334]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0761, 0.0481, 0.0783, 0.1547, 0.1374, 0.2297, 0.1329, 0.0999, 0.0429]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0231, 0.0655, 0.1663, 0.1173, 0.3025, 0.1101, 0.1151, 0.0268]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.8159e-03, 6.0923e-04, 8.4433e-03, 1.9427e-01, 4.0449e-02, 6.8429e-01,\n",
       "           4.1066e-02, 2.1164e-02, 8.9709e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0433, 0.0089, 0.0438, 0.2315, 0.0828, 0.4048, 0.0983, 0.0774, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0800, 0.0199, 0.0583, 0.2072, 0.1138, 0.3215, 0.1040, 0.0750, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0461, 0.0119, 0.0484, 0.2744, 0.1351, 0.3063, 0.0940, 0.0714, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0819, 0.0305, 0.0624, 0.2216, 0.1206, 0.2381, 0.1098, 0.0977, 0.0374]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0342, 0.0040, 0.0319, 0.2765, 0.0608, 0.4518, 0.0702, 0.0668, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0768, 0.0491, 0.0870, 0.1850, 0.1341, 0.2134, 0.1164, 0.0955, 0.0428]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.6946e-03, 3.6741e-05, 3.2339e-03, 3.5913e-01, 2.7354e-02, 5.4899e-01,\n",
       "           4.6323e-02, 1.1186e-02, 4.7200e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0487, 0.0136, 0.0673, 0.2761, 0.1132, 0.2925, 0.0964, 0.0783, 0.0139]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0451, 0.0122, 0.0546, 0.3136, 0.1221, 0.2881, 0.0880, 0.0660, 0.0103]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0710, 0.0205, 0.0566, 0.2243, 0.1320, 0.2992, 0.0890, 0.0832, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0141, 0.0008, 0.0094, 0.3070, 0.0438, 0.5808, 0.0197, 0.0227, 0.0017]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0432, 0.0132, 0.0467, 0.2279, 0.1138, 0.3841, 0.0817, 0.0746, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0402, 0.0096, 0.0451, 0.2318, 0.0735, 0.4340, 0.0905, 0.0643, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0186, 0.0025, 0.0135, 0.2251, 0.0594, 0.6136, 0.0337, 0.0308, 0.0028]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0593, 0.0283, 0.0546, 0.1926, 0.1407, 0.3131, 0.0909, 0.0883, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0506, 0.0145, 0.0570, 0.2065, 0.1396, 0.3212, 0.1200, 0.0748, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0023, 0.0212, 0.2556, 0.0576, 0.5332, 0.0522, 0.0368, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0143, 0.0427, 0.1893, 0.0848, 0.4213, 0.0808, 0.0920, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0554, 0.0174, 0.0620, 0.1878, 0.1382, 0.3451, 0.0982, 0.0784, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0136, 0.0500, 0.1865, 0.1194, 0.3949, 0.0948, 0.0776, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0975, 0.0496, 0.0829, 0.1439, 0.1242, 0.2422, 0.1104, 0.1035, 0.0458]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0830, 0.0409, 0.0779, 0.1494, 0.1242, 0.2694, 0.1024, 0.1057, 0.0470]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0277, 0.0059, 0.0556, 0.1602, 0.1238, 0.4471, 0.0946, 0.0782, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2805e-03, 4.9053e-05, 1.5539e-03, 3.5752e-02, 6.6068e-03, 9.4597e-01,\n",
       "           5.9896e-03, 2.7571e-03, 4.1143e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0423, 0.0145, 0.0526, 0.1376, 0.1227, 0.4660, 0.0754, 0.0700, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0446, 0.0082, 0.0374, 0.1851, 0.1199, 0.4737, 0.0659, 0.0592, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.5401e-05, 1.6089e-08, 1.3805e-05, 3.7343e-03, 2.3489e-04, 9.9582e-01,\n",
       "           5.4741e-05, 1.1343e-04, 3.3377e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0210, 0.0032, 0.0147, 0.1537, 0.0501, 0.6810, 0.0256, 0.0475, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0156, 0.0019, 0.0111, 0.0893, 0.0431, 0.7749, 0.0261, 0.0356, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0562, 0.0268, 0.0656, 0.1456, 0.1446, 0.3571, 0.0876, 0.0858, 0.0309]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0100, 0.0345, 0.1661, 0.1036, 0.5010, 0.0591, 0.0544, 0.0112]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0090, 0.0012, 0.0051, 0.1076, 0.0189, 0.8299, 0.0101, 0.0170, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0194, 0.0026, 0.0179, 0.1235, 0.0532, 0.6945, 0.0415, 0.0444, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0177, 0.0015, 0.0104, 0.1403, 0.0381, 0.7257, 0.0307, 0.0331, 0.0025]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6210e-03, 1.5184e-05, 5.4552e-04, 3.3802e-02, 2.6953e-03, 9.5394e-01,\n",
       "           1.9098e-03, 4.4471e-03, 2.6128e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0175, 0.0008, 0.0059, 0.1360, 0.0221, 0.7778, 0.0181, 0.0208, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0507, 0.0149, 0.0498, 0.1736, 0.1394, 0.4221, 0.0671, 0.0643, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0197, 0.0016, 0.0145, 0.0822, 0.0375, 0.7911, 0.0210, 0.0301, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0077, 0.0013, 0.0109, 0.1134, 0.0667, 0.7431, 0.0247, 0.0311, 0.0012]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.3927e-03, 1.6984e-04, 1.2831e-03, 3.9222e-02, 8.3252e-03, 9.3051e-01,\n",
       "           4.3729e-03, 1.0525e-02, 1.9836e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0190, 0.0011, 0.0119, 0.1156, 0.0352, 0.7718, 0.0158, 0.0283, 0.0014]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0430, 0.0139, 0.0354, 0.1266, 0.0713, 0.5775, 0.0608, 0.0574, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0184, 0.0029, 0.0136, 0.0632, 0.0542, 0.8005, 0.0204, 0.0243, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[9.5568e-03, 8.3943e-04, 5.7512e-03, 9.8759e-02, 2.9747e-02, 8.2081e-01,\n",
       "           1.2525e-02, 2.1329e-02, 6.8718e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0504e-02, 5.4642e-04, 6.0157e-03, 6.3735e-02, 3.3636e-02, 8.4922e-01,\n",
       "           1.7857e-02, 1.7855e-02, 6.3194e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.3072e-02, 6.4665e-04, 6.2002e-03, 6.9584e-02, 2.1103e-02, 8.6498e-01,\n",
       "           1.1038e-02, 1.2744e-02, 6.3274e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0313, 0.0060, 0.0381, 0.1064, 0.0715, 0.6431, 0.0416, 0.0547, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0211, 0.0018, 0.0109, 0.1260, 0.0304, 0.7684, 0.0169, 0.0226, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0191, 0.0037, 0.0148, 0.1202, 0.0791, 0.6488, 0.0467, 0.0620, 0.0056]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[5.0436e-03, 1.5003e-04, 2.6697e-03, 3.4019e-02, 2.0853e-02, 9.2157e-01,\n",
       "           6.9617e-03, 8.5433e-03, 1.8611e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0902e-03, 3.4768e-05, 1.0194e-03, 3.4278e-02, 1.3362e-02, 9.4171e-01,\n",
       "           3.7113e-03, 4.7631e-03, 3.0669e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.1986e-05, 4.3504e-08, 2.3622e-05, 3.4064e-03, 7.6640e-04, 9.9550e-01,\n",
       "           1.7253e-04, 1.1876e-04, 1.3296e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.4232e-03, 3.8399e-04, 3.3108e-03, 6.1325e-02, 1.5853e-02, 8.9078e-01,\n",
       "           9.5876e-03, 1.1919e-02, 4.1320e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.6553e-03, 6.6659e-04, 9.6380e-03, 6.1867e-02, 3.3770e-02, 8.4947e-01,\n",
       "           2.0845e-02, 1.8174e-02, 9.1897e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[4.9640e-03, 4.3088e-04, 1.0027e-02, 6.3524e-02, 5.1438e-02, 8.2166e-01,\n",
       "           2.4959e-02, 2.2176e-02, 8.2189e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0098, 0.0025, 0.0105, 0.0913, 0.0529, 0.7947, 0.0188, 0.0177, 0.0016]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0071, 0.0288, 0.1358, 0.0795, 0.6047, 0.0402, 0.0592, 0.0075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6003e-03, 8.0909e-05, 2.4186e-03, 3.1774e-02, 1.8325e-02, 9.3202e-01,\n",
       "           6.1285e-03, 6.5577e-03, 9.1599e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.7956e-03, 7.0113e-05, 9.3268e-04, 3.4926e-02, 7.8277e-03, 9.4695e-01,\n",
       "           2.0287e-03, 5.3526e-03, 1.1869e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0288, 0.0077, 0.0299, 0.1553, 0.0914, 0.5716, 0.0571, 0.0500, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.5164e-03, 1.0616e-04, 1.1056e-03, 3.1736e-02, 1.6240e-02, 9.3937e-01,\n",
       "           4.2070e-03, 4.6406e-03, 7.8487e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.7266e-04, 2.0282e-05, 1.6437e-03, 3.0768e-02, 1.1347e-02, 9.4873e-01,\n",
       "           4.0269e-03, 2.5603e-03, 3.2302e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1991e-03, 1.1119e-04, 3.1791e-03, 6.0622e-02, 2.5335e-02, 8.8993e-01,\n",
       "           8.4732e-03, 8.8673e-03, 2.7930e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0181, 0.0022, 0.0175, 0.1138, 0.0664, 0.7212, 0.0310, 0.0266, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.8840e-03, 4.0429e-05, 1.2859e-03, 7.3859e-02, 7.9795e-03, 9.0702e-01,\n",
       "           3.5822e-03, 4.3059e-03, 4.0768e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0186, 0.0026, 0.0305, 0.1179, 0.0889, 0.6776, 0.0280, 0.0323, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.8308e-05, 1.3776e-07, 2.4991e-05, 7.2382e-03, 1.3879e-03, 9.9067e-01,\n",
       "           1.4896e-04, 4.4366e-04, 8.7022e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0183, 0.0009, 0.0122, 0.0770, 0.0421, 0.7833, 0.0259, 0.0382, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6675e-03, 8.4220e-05, 7.7132e-04, 3.1897e-02, 1.0839e-02, 9.4757e-01,\n",
       "           4.0726e-03, 2.0509e-03, 4.4120e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[3.7397e-03, 1.2030e-04, 2.2087e-03, 6.9303e-02, 1.7918e-02, 8.9554e-01,\n",
       "           5.5687e-03, 5.4395e-03, 1.5967e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0222, 0.0071, 0.0197, 0.1035, 0.0820, 0.6874, 0.0399, 0.0335, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0205, 0.0015, 0.0095, 0.1430, 0.0651, 0.7158, 0.0189, 0.0239, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.0100e-04, 8.2423e-07, 7.2047e-05, 8.7077e-03, 5.0910e-03, 9.8521e-01,\n",
       "           4.3402e-04, 3.8308e-04, 4.9143e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0244, 0.0012, 0.0144, 0.1215, 0.0627, 0.7194, 0.0287, 0.0258, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0274, 0.0053, 0.0214, 0.1306, 0.0667, 0.6492, 0.0355, 0.0555, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.6988e-03, 2.6830e-04, 1.8813e-03, 6.3423e-02, 1.7775e-02, 9.0330e-01,\n",
       "           5.1145e-03, 5.2169e-03, 3.1926e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0161, 0.0017, 0.0094, 0.0995, 0.0446, 0.7787, 0.0195, 0.0283, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0265, 0.0019, 0.0141, 0.1449, 0.0770, 0.6670, 0.0312, 0.0332, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.7467e-03, 2.7168e-04, 5.9011e-03, 8.7144e-02, 4.3795e-02, 8.3207e-01,\n",
       "           1.0871e-02, 1.3817e-02, 3.8490e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6484e-03, 8.2820e-05, 4.1505e-03, 1.8410e-01, 4.1590e-02, 7.4223e-01,\n",
       "           1.2327e-02, 1.2688e-02, 1.8509e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0300, 0.0058, 0.0203, 0.1219, 0.0947, 0.6365, 0.0440, 0.0401, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.3343e-03, 8.8411e-05, 1.6207e-03, 5.3760e-02, 2.7186e-02, 9.0170e-01,\n",
       "           5.8585e-03, 5.3147e-03, 1.3518e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0313, 0.0057, 0.0287, 0.1937, 0.1252, 0.5054, 0.0616, 0.0418, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.5503e-02, 7.5912e-04, 7.2919e-03, 8.6872e-02, 4.8623e-02, 7.8929e-01,\n",
       "           1.5896e-02, 3.4348e-02, 1.4177e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0366, 0.0098, 0.0433, 0.1356, 0.1517, 0.4391, 0.0708, 0.0992, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[6.9314e-05, 3.2684e-07, 5.6569e-05, 3.6625e-02, 6.2398e-03, 9.5645e-01,\n",
       "           3.6664e-04, 1.9371e-04, 3.7855e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0435, 0.0065, 0.0347, 0.1206, 0.2306, 0.4406, 0.0508, 0.0612, 0.0116]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0393, 0.0039, 0.0235, 0.1901, 0.1304, 0.4956, 0.0543, 0.0556, 0.0073]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[5.2883e-03, 2.8170e-04, 6.7409e-03, 2.0027e-01, 1.0574e-01, 6.4250e-01,\n",
       "           2.5106e-02, 1.3657e-02, 4.2640e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[8.2839e-03, 2.2496e-04, 4.4602e-03, 1.0420e-01, 4.4727e-02, 8.1230e-01,\n",
       "           1.1257e-02, 1.4117e-02, 4.2058e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0733, 0.0190, 0.0479, 0.1520, 0.1618, 0.3595, 0.0867, 0.0724, 0.0275]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0283, 0.0032, 0.0190, 0.1887, 0.1119, 0.5880, 0.0318, 0.0262, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0420, 0.0091, 0.0236, 0.1686, 0.1173, 0.5100, 0.0648, 0.0557, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0252, 0.0026, 0.0259, 0.1727, 0.1024, 0.5590, 0.0745, 0.0346, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0466, 0.0104, 0.0367, 0.2061, 0.1645, 0.3832, 0.0860, 0.0564, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0443, 0.0093, 0.0478, 0.1579, 0.1276, 0.4470, 0.0900, 0.0664, 0.0097]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0425, 0.0031, 0.0261, 0.2306, 0.0980, 0.4857, 0.0614, 0.0488, 0.0038]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0445, 0.0072, 0.0407, 0.2223, 0.1621, 0.3318, 0.0782, 0.1047, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0771, 0.0274, 0.0657, 0.1491, 0.1533, 0.3012, 0.0947, 0.0953, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0776, 0.0279, 0.0787, 0.1858, 0.1413, 0.2464, 0.1086, 0.1023, 0.0315]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0509, 0.0091, 0.0531, 0.1765, 0.1395, 0.3923, 0.0875, 0.0769, 0.0142]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0114, 0.0006, 0.0097, 0.2349, 0.1075, 0.5592, 0.0485, 0.0271, 0.0011]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0436, 0.0071, 0.0377, 0.2308, 0.1444, 0.3979, 0.0956, 0.0367, 0.0062]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0507, 0.0053, 0.0369, 0.1972, 0.1083, 0.4525, 0.0755, 0.0652, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0919, 0.0408, 0.0713, 0.1508, 0.1539, 0.2216, 0.1091, 0.1176, 0.0429]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0665, 0.0271, 0.0587, 0.1986, 0.1371, 0.3116, 0.0977, 0.0811, 0.0216]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0670, 0.0220, 0.0767, 0.1751, 0.1783, 0.2588, 0.1072, 0.0898, 0.0250]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0604, 0.0115, 0.0582, 0.2140, 0.1417, 0.3156, 0.1188, 0.0697, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0659, 0.0157, 0.0457, 0.2366, 0.1362, 0.2948, 0.1155, 0.0731, 0.0166]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0803, 0.0485, 0.1008, 0.1224, 0.1703, 0.1968, 0.1213, 0.1029, 0.0567]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0890, 0.0396, 0.0845, 0.1618, 0.1473, 0.2190, 0.1115, 0.1018, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0746, 0.0104, 0.0593, 0.1876, 0.1449, 0.3171, 0.0949, 0.0966, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0746, 0.0277, 0.0685, 0.2157, 0.1480, 0.2261, 0.1136, 0.0981, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0226, 0.0603, 0.2214, 0.1589, 0.2228, 0.1217, 0.1015, 0.0261]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0786, 0.0245, 0.0732, 0.1795, 0.1320, 0.2721, 0.1119, 0.0997, 0.0285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0650, 0.0133, 0.0720, 0.1894, 0.1435, 0.2973, 0.1290, 0.0761, 0.0145]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0964, 0.0352, 0.0840, 0.1629, 0.1350, 0.2207, 0.1017, 0.1167, 0.0474]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0764, 0.0229, 0.0644, 0.1655, 0.1450, 0.2854, 0.1061, 0.1056, 0.0287]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0034, 0.0349, 0.2296, 0.1143, 0.4141, 0.0997, 0.0611, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0554, 0.0113, 0.0588, 0.1689, 0.1335, 0.3548, 0.1162, 0.0892, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0244, 0.0027, 0.0348, 0.2348, 0.1532, 0.4080, 0.0973, 0.0419, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0459, 0.0108, 0.0497, 0.2229, 0.1487, 0.3053, 0.1107, 0.0903, 0.0156]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0258, 0.0752, 0.1888, 0.1569, 0.2336, 0.1069, 0.1231, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0107, 0.0558, 0.2186, 0.0932, 0.3460, 0.1013, 0.1006, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0822, 0.0457, 0.0946, 0.1396, 0.1650, 0.1629, 0.1216, 0.1237, 0.0646]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0127, 0.0644, 0.1967, 0.1635, 0.2622, 0.1201, 0.1022, 0.0183]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0129, 0.0653, 0.1751, 0.1582, 0.3020, 0.1173, 0.1042, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0125, 0.0007, 0.0146, 0.1895, 0.1341, 0.5538, 0.0505, 0.0435, 0.0008]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0862, 0.0509, 0.0893, 0.1384, 0.1519, 0.2062, 0.1164, 0.1084, 0.0523]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0739, 0.0286, 0.0728, 0.1880, 0.1669, 0.2312, 0.1230, 0.0900, 0.0255]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0635, 0.0287, 0.0832, 0.1634, 0.1728, 0.2328, 0.1411, 0.0849, 0.0296]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0640, 0.0384, 0.0777, 0.1670, 0.1888, 0.2056, 0.1257, 0.0930, 0.0397]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0439, 0.0150, 0.0640, 0.2379, 0.1636, 0.2340, 0.1582, 0.0686, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0719, 0.0320, 0.0793, 0.1705, 0.1449, 0.2370, 0.1216, 0.1045, 0.0382]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0232, 0.0715, 0.2035, 0.1507, 0.2406, 0.1297, 0.0914, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0450, 0.0152, 0.0763, 0.1846, 0.1853, 0.2156, 0.1445, 0.1162, 0.0173]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0616, 0.0178, 0.0596, 0.1941, 0.1677, 0.2767, 0.1165, 0.0906, 0.0154]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0520, 0.0122, 0.0614, 0.2046, 0.1908, 0.2576, 0.1176, 0.0863, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0430, 0.0081, 0.0690, 0.1884, 0.2250, 0.2692, 0.1072, 0.0812, 0.0088]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0849, 0.0473, 0.0910, 0.1595, 0.1687, 0.1652, 0.1311, 0.1040, 0.0484]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0079, 0.0005, 0.0150, 0.2699, 0.1815, 0.3820, 0.0781, 0.0646, 0.0007]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0849, 0.0500, 0.0824, 0.1680, 0.1675, 0.1863, 0.1162, 0.0990, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0262, 0.0023, 0.0258, 0.3760, 0.1281, 0.3271, 0.0690, 0.0427, 0.0028]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0761, 0.0322, 0.0946, 0.1528, 0.1708, 0.2173, 0.1220, 0.0976, 0.0367]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0690, 0.0447, 0.0864, 0.1760, 0.1789, 0.1810, 0.1362, 0.0904, 0.0373]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0368, 0.0069, 0.0427, 0.1944, 0.1866, 0.3589, 0.1088, 0.0553, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.9578e-03, 8.0168e-04, 1.1752e-02, 4.9535e-01, 1.2197e-01, 2.5994e-01,\n",
       "           7.7337e-02, 2.3489e-02, 3.9841e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6647e-04, 6.2516e-07, 3.0290e-04, 3.7969e-01, 1.4865e-02, 5.8800e-01,\n",
       "           1.5902e-02, 9.6785e-04, 2.4811e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0509, 0.0161, 0.0721, 0.2170, 0.1919, 0.2017, 0.1306, 0.0996, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0322, 0.0033, 0.0349, 0.2617, 0.1657, 0.3582, 0.0905, 0.0493, 0.0042]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0504, 0.0094, 0.0541, 0.1946, 0.1593, 0.3265, 0.1301, 0.0647, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0720, 0.0243, 0.0803, 0.1663, 0.1842, 0.2192, 0.1280, 0.0972, 0.0285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0673, 0.0285, 0.0808, 0.1600, 0.1804, 0.2134, 0.1237, 0.1113, 0.0347]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0440, 0.0136, 0.0595, 0.2061, 0.1603, 0.2939, 0.1353, 0.0759, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0104, 0.0004, 0.0191, 0.3209, 0.1864, 0.3894, 0.0483, 0.0245, 0.0005]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0450, 0.0108, 0.0488, 0.1771, 0.1614, 0.3827, 0.1025, 0.0611, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0698, 0.0285, 0.0755, 0.1608, 0.1428, 0.2701, 0.1251, 0.1002, 0.0272]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0414, 0.0083, 0.0554, 0.2321, 0.1420, 0.3078, 0.1094, 0.0915, 0.0122]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[6.7043e-03, 1.7912e-04, 1.3032e-02, 2.0665e-01, 6.4837e-02, 6.4179e-01,\n",
       "           4.0856e-02, 2.5534e-02, 4.1998e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0308, 0.0728, 0.1573, 0.1576, 0.2390, 0.1377, 0.0946, 0.0311]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0661, 0.0251, 0.0676, 0.1628, 0.1868, 0.2164, 0.1375, 0.1013, 0.0363]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0373, 0.0071, 0.0490, 0.1457, 0.2005, 0.3697, 0.1259, 0.0553, 0.0095]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0447, 0.0156, 0.0608, 0.1592, 0.2322, 0.2672, 0.1305, 0.0744, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0242, 0.0027, 0.0296, 0.1502, 0.1555, 0.5047, 0.0962, 0.0335, 0.0035]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0448, 0.0101, 0.0473, 0.1444, 0.1494, 0.4241, 0.1013, 0.0687, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0708, 0.0265, 0.0662, 0.1461, 0.1560, 0.2827, 0.1279, 0.0977, 0.0259]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0400, 0.0098, 0.0446, 0.1249, 0.1936, 0.3700, 0.1153, 0.0893, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0663, 0.0337, 0.0728, 0.1479, 0.1706, 0.2390, 0.1318, 0.0991, 0.0390]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0328, 0.0031, 0.0281, 0.1410, 0.1440, 0.4715, 0.1179, 0.0568, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0410, 0.0081, 0.0381, 0.1244, 0.1686, 0.4012, 0.1119, 0.0964, 0.0103]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0596, 0.0274, 0.0579, 0.1618, 0.1759, 0.2736, 0.1306, 0.0900, 0.0232]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0513, 0.0202, 0.0674, 0.1373, 0.1914, 0.2853, 0.1213, 0.0998, 0.0260]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0359, 0.0118, 0.0591, 0.1351, 0.1721, 0.3249, 0.1422, 0.1031, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0732, 0.0254, 0.0765, 0.1239, 0.1662, 0.2792, 0.1396, 0.0847, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.4173e-03, 4.7859e-05, 2.2589e-03, 5.8806e-02, 7.6475e-02, 8.2252e-01,\n",
       "           2.8194e-02, 9.2211e-03, 5.8832e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0324, 0.0041, 0.0411, 0.1300, 0.1556, 0.4719, 0.1210, 0.0382, 0.0056]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2923e-03, 1.0087e-05, 1.7392e-03, 3.8844e-02, 4.3297e-02, 8.6686e-01,\n",
       "           4.5495e-02, 2.4534e-03, 1.1604e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0496, 0.0064, 0.0430, 0.1363, 0.1574, 0.4018, 0.1289, 0.0680, 0.0088]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0351, 0.0051, 0.0450, 0.1220, 0.1745, 0.4161, 0.1382, 0.0532, 0.0107]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0689, 0.0354, 0.0763, 0.1468, 0.1620, 0.2199, 0.1570, 0.0973, 0.0364]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0486, 0.0082, 0.0501, 0.1208, 0.1654, 0.4033, 0.1221, 0.0707, 0.0108]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0483, 0.0849, 0.1145, 0.1578, 0.1929, 0.1327, 0.1277, 0.0581]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0571, 0.0115, 0.0431, 0.1587, 0.1381, 0.3788, 0.1235, 0.0774, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0499, 0.0136, 0.0545, 0.1545, 0.1386, 0.3715, 0.1246, 0.0787, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0515, 0.0124, 0.0584, 0.1361, 0.1679, 0.3472, 0.1417, 0.0691, 0.0157]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0760, 0.0335, 0.0785, 0.1366, 0.1624, 0.2361, 0.1395, 0.0960, 0.0413]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0478, 0.0084, 0.0410, 0.1700, 0.1682, 0.3381, 0.1517, 0.0632, 0.0116]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0578, 0.0150, 0.0592, 0.1339, 0.1514, 0.3810, 0.1106, 0.0726, 0.0185]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0579, 0.0163, 0.0547, 0.1367, 0.1766, 0.3329, 0.1201, 0.0844, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.7019e-03, 7.3369e-05, 2.8247e-03, 1.8651e-01, 7.0547e-02, 6.5508e-01,\n",
       "           6.9104e-02, 1.2043e-02, 1.1487e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0510, 0.0100, 0.0543, 0.1647, 0.1641, 0.2987, 0.1536, 0.0856, 0.0181]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0762, 0.0327, 0.0823, 0.1284, 0.1551, 0.2403, 0.1378, 0.1102, 0.0369]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0609, 0.0175, 0.0595, 0.1556, 0.1440, 0.3533, 0.1034, 0.0771, 0.0287]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0162, 0.0019, 0.0167, 0.1163, 0.1641, 0.5525, 0.0859, 0.0434, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0204, 0.0036, 0.0229, 0.1429, 0.1519, 0.5497, 0.0699, 0.0339, 0.0048]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0190, 0.0023, 0.0195, 0.1234, 0.0969, 0.6251, 0.0775, 0.0343, 0.0021]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0758, 0.0372, 0.0837, 0.1402, 0.1459, 0.2551, 0.1106, 0.1078, 0.0438]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.0897e-02, 5.6326e-04, 1.8740e-02, 1.3102e-01, 1.1400e-01, 6.6019e-01,\n",
       "           4.3292e-02, 1.9389e-02, 1.9030e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0595, 0.0117, 0.0670, 0.1201, 0.1552, 0.3589, 0.1298, 0.0810, 0.0168]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0610, 0.0188, 0.0614, 0.1353, 0.1318, 0.3362, 0.1176, 0.1122, 0.0258]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0335, 0.0043, 0.0397, 0.0916, 0.0782, 0.6359, 0.0719, 0.0403, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.3242e-03, 1.9575e-04, 9.5126e-03, 5.2268e-02, 4.6091e-02, 8.5048e-01,\n",
       "           1.9341e-02, 1.4496e-02, 2.9088e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.2930e-03, 6.9166e-04, 1.2036e-02, 1.0643e-01, 6.4548e-02, 7.2875e-01,\n",
       "           5.1762e-02, 2.7681e-02, 8.0644e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0576, 0.0144, 0.0659, 0.1609, 0.1333, 0.3467, 0.1140, 0.0876, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0298, 0.0039, 0.0452, 0.1570, 0.1234, 0.4695, 0.0916, 0.0718, 0.0078]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0548, 0.0161, 0.0566, 0.1510, 0.1307, 0.3665, 0.0949, 0.1125, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0555, 0.0146, 0.0670, 0.1592, 0.1261, 0.3364, 0.1024, 0.1163, 0.0225]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0262, 0.0029, 0.0307, 0.1368, 0.1043, 0.5494, 0.0875, 0.0573, 0.0047]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.7947e-03, 2.3045e-04, 8.5734e-03, 1.2327e-01, 4.1779e-02, 7.8245e-01,\n",
       "           2.3946e-02, 1.4582e-02, 3.7029e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0469, 0.0054, 0.0383, 0.1585, 0.1356, 0.4288, 0.1174, 0.0606, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.8013e-03, 3.5813e-05, 3.0361e-03, 7.1114e-02, 2.2653e-02, 8.6818e-01,\n",
       "           1.9187e-02, 1.1917e-02, 7.0728e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0502, 0.0058, 0.0372, 0.1526, 0.1166, 0.4760, 0.0702, 0.0832, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[4.4857e-06, 5.3940e-10, 1.1012e-05, 6.9655e-03, 2.8741e-04, 9.9229e-01,\n",
       "           1.4475e-04, 2.9511e-04, 1.7880e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0331, 0.0044, 0.0224, 0.1716, 0.0854, 0.5555, 0.0602, 0.0633, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0202, 0.0008, 0.0222, 0.0996, 0.0595, 0.7217, 0.0463, 0.0283, 0.0013]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0262, 0.0054, 0.0393, 0.1438, 0.1309, 0.5098, 0.0700, 0.0645, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0114, 0.0581, 0.1280, 0.1358, 0.4413, 0.0959, 0.0684, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0254, 0.0023, 0.0425, 0.1189, 0.1306, 0.5462, 0.0756, 0.0531, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0391, 0.0033, 0.0356, 0.1350, 0.1200, 0.5457, 0.0658, 0.0496, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0585, 0.0184, 0.0809, 0.1603, 0.1689, 0.2837, 0.1081, 0.0959, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0329, 0.0040, 0.0431, 0.1216, 0.1137, 0.5498, 0.0666, 0.0612, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0350, 0.0061, 0.0535, 0.1564, 0.1489, 0.4309, 0.0952, 0.0658, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0637, 0.0138, 0.0781, 0.1214, 0.1307, 0.3635, 0.1172, 0.0908, 0.0208]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0802, 0.0396, 0.0897, 0.1306, 0.1677, 0.2224, 0.1176, 0.1074, 0.0448]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0548, 0.0048, 0.0483, 0.1683, 0.1280, 0.4301, 0.0909, 0.0666, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0569, 0.0099, 0.0681, 0.1704, 0.1336, 0.3562, 0.0927, 0.0962, 0.0161]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0287, 0.0022, 0.0250, 0.1376, 0.0695, 0.6450, 0.0526, 0.0367, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2015e-02, 4.5406e-04, 1.5500e-02, 1.4146e-01, 1.1208e-01, 6.5504e-01,\n",
       "           4.2723e-02, 1.9327e-02, 1.4021e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0713, 0.0275, 0.0874, 0.1502, 0.1585, 0.2274, 0.1345, 0.1082, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.2820e-05, 1.1329e-08, 1.0387e-04, 2.2055e-02, 3.5955e-03, 9.7025e-01,\n",
       "           3.8165e-03, 9.6099e-05, 5.3351e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0768, 0.0159, 0.0622, 0.1489, 0.1397, 0.3203, 0.1130, 0.0979, 0.0253]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0366, 0.0820, 0.1404, 0.1557, 0.2358, 0.1113, 0.1049, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0551, 0.0038, 0.0483, 0.1364, 0.1232, 0.4956, 0.0674, 0.0631, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0014, 0.0401, 0.1013, 0.1198, 0.6062, 0.0639, 0.0362, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0722, 0.0131, 0.0534, 0.1789, 0.1357, 0.3513, 0.0994, 0.0796, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0928, 0.0349, 0.0927, 0.1365, 0.1434, 0.2305, 0.1181, 0.1054, 0.0458]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0792, 0.0224, 0.0721, 0.1606, 0.1516, 0.2858, 0.1071, 0.0927, 0.0284]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0908, 0.0308, 0.0795, 0.1626, 0.1487, 0.2455, 0.1035, 0.0975, 0.0412]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0786, 0.0235, 0.0785, 0.1535, 0.1396, 0.2746, 0.1312, 0.0927, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0794, 0.0248, 0.0799, 0.1347, 0.1688, 0.2491, 0.1258, 0.1006, 0.0369]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1011, 0.0585, 0.0964, 0.1261, 0.1353, 0.1884, 0.1143, 0.1154, 0.0644]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0739, 0.0284, 0.0833, 0.1446, 0.1619, 0.2403, 0.1266, 0.1011, 0.0399]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0567, 0.0101, 0.0549, 0.1738, 0.1442, 0.3448, 0.1011, 0.0961, 0.0182]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0718, 0.0182, 0.0735, 0.1780, 0.1539, 0.2903, 0.1069, 0.0835, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0945, 0.0273, 0.0798, 0.1458, 0.1533, 0.2334, 0.1235, 0.1061, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0982, 0.0596, 0.0991, 0.1260, 0.1363, 0.1673, 0.1206, 0.1203, 0.0728]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0355, 0.0962, 0.1342, 0.1663, 0.1942, 0.1288, 0.1067, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0963, 0.0588, 0.0989, 0.1282, 0.1377, 0.1780, 0.1241, 0.1088, 0.0692]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0906, 0.0392, 0.0843, 0.1411, 0.1484, 0.2221, 0.1215, 0.1027, 0.0501]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[2.8827e-02, 5.1956e-04, 1.5076e-02, 1.6954e-01, 9.7590e-02, 6.1624e-01,\n",
       "           4.2468e-02, 2.8756e-02, 9.7868e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0990, 0.0626, 0.0989, 0.1215, 0.1382, 0.1681, 0.1203, 0.1199, 0.0716]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0973, 0.0422, 0.0887, 0.1193, 0.1558, 0.2064, 0.1202, 0.1118, 0.0582]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1056, 0.0656, 0.1009, 0.1204, 0.1327, 0.1553, 0.1194, 0.1201, 0.0800]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.9091e-04, 2.5918e-07, 3.6338e-04, 4.5238e-02, 7.7582e-03, 9.3710e-01,\n",
       "           7.9220e-03, 8.2900e-04, 1.9724e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0519, 0.0100, 0.0541, 0.1644, 0.1357, 0.3764, 0.1084, 0.0829, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0805, 0.0209, 0.0768, 0.1664, 0.1434, 0.2556, 0.1134, 0.1101, 0.0329]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0009, 0.0396, 0.1281, 0.1211, 0.5467, 0.0667, 0.0553, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0291, 0.0894, 0.1441, 0.1522, 0.2217, 0.1219, 0.1105, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0883, 0.0338, 0.0885, 0.1315, 0.1625, 0.2205, 0.1176, 0.1085, 0.0488]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1004, 0.0654, 0.1016, 0.1235, 0.1397, 0.1554, 0.1239, 0.1177, 0.0724]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0925, 0.0364, 0.0936, 0.1440, 0.1509, 0.2156, 0.1199, 0.1020, 0.0452]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0965, 0.0354, 0.0832, 0.1500, 0.1494, 0.1941, 0.1232, 0.1168, 0.0514]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0941, 0.0393, 0.0896, 0.1423, 0.1421, 0.2035, 0.1173, 0.1171, 0.0548]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0592, 0.0969, 0.1320, 0.1367, 0.1599, 0.1203, 0.1231, 0.0712]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0917, 0.0350, 0.0913, 0.1540, 0.1518, 0.1980, 0.1213, 0.1097, 0.0472]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1048, 0.0668, 0.0966, 0.1294, 0.1345, 0.1497, 0.1224, 0.1200, 0.0758]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0430, 0.0890, 0.1346, 0.1670, 0.1963, 0.1263, 0.1074, 0.0438]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0965, 0.0652, 0.0998, 0.1224, 0.1429, 0.1598, 0.1199, 0.1212, 0.0723]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0854, 0.0399, 0.0891, 0.1411, 0.1454, 0.2009, 0.1319, 0.1149, 0.0514]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0972, 0.0701, 0.1017, 0.1220, 0.1427, 0.1473, 0.1213, 0.1177, 0.0800]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1064, 0.0671, 0.1013, 0.1200, 0.1364, 0.1563, 0.1173, 0.1184, 0.0769]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5655e-02, 3.2035e-04, 2.2072e-02, 2.4972e-01, 1.1190e-01, 5.0702e-01,\n",
       "           4.6187e-02, 4.6268e-02, 8.6893e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0946, 0.0415, 0.0927, 0.1472, 0.1540, 0.1819, 0.1307, 0.1104, 0.0471]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0664, 0.0148, 0.0747, 0.1840, 0.1586, 0.2340, 0.1326, 0.1171, 0.0178]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0570, 0.0145, 0.0674, 0.1869, 0.1593, 0.2763, 0.1160, 0.1033, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1000, 0.0541, 0.1000, 0.1302, 0.1487, 0.1525, 0.1286, 0.1228, 0.0632]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0572, 0.0087, 0.0727, 0.2008, 0.1602, 0.2693, 0.1206, 0.0949, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1032, 0.0588, 0.0988, 0.1264, 0.1356, 0.1523, 0.1237, 0.1283, 0.0729]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0335, 0.0005, 0.0285, 0.2217, 0.0972, 0.4954, 0.0775, 0.0449, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1036, 0.0600, 0.0989, 0.1303, 0.1424, 0.1565, 0.1253, 0.1182, 0.0649]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1026, 0.0653, 0.1017, 0.1276, 0.1390, 0.1363, 0.1257, 0.1238, 0.0779]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0965, 0.0558, 0.0907, 0.1376, 0.1429, 0.1544, 0.1295, 0.1250, 0.0677]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0925, 0.0462, 0.0983, 0.1395, 0.1583, 0.1646, 0.1308, 0.1154, 0.0543]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0893, 0.0373, 0.0910, 0.1714, 0.1434, 0.1796, 0.1362, 0.1134, 0.0384]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0950, 0.0507, 0.1033, 0.1387, 0.1462, 0.1564, 0.1285, 0.1175, 0.0637]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0760, 0.0232, 0.0782, 0.2018, 0.1547, 0.2021, 0.1238, 0.1094, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0059, 0.0523, 0.2549, 0.1246, 0.2702, 0.1220, 0.1017, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0074, 0.0539, 0.2532, 0.1362, 0.2557, 0.1171, 0.1116, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0761, 0.0244, 0.0715, 0.1918, 0.1558, 0.2260, 0.1221, 0.1048, 0.0275]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0195, 0.0671, 0.1949, 0.1485, 0.1970, 0.1451, 0.1191, 0.0298]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0881, 0.0298, 0.0855, 0.1645, 0.1387, 0.1819, 0.1356, 0.1365, 0.0394]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0795, 0.0179, 0.0731, 0.1998, 0.1444, 0.2181, 0.1358, 0.1083, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0816, 0.0195, 0.0733, 0.1854, 0.1539, 0.2108, 0.1301, 0.1149, 0.0306]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0703, 0.0175, 0.0750, 0.1973, 0.1480, 0.2084, 0.1375, 0.1230, 0.0230]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0330, 0.0012, 0.0297, 0.3097, 0.1287, 0.3218, 0.1213, 0.0525, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.5077e-02, 2.4557e-04, 1.5038e-02, 3.8496e-01, 9.9823e-02, 3.4351e-01,\n",
       "           9.0183e-02, 4.0070e-02, 1.0899e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0737, 0.0162, 0.0753, 0.1998, 0.1478, 0.2200, 0.1484, 0.0994, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0697, 0.0141, 0.0651, 0.2239, 0.1763, 0.1961, 0.1397, 0.0997, 0.0155]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0892, 0.0323, 0.0880, 0.1826, 0.1406, 0.1777, 0.1347, 0.1166, 0.0383]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0231, 0.0807, 0.1830, 0.1559, 0.2375, 0.1112, 0.1035, 0.0295]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0796, 0.0367, 0.0800, 0.1660, 0.1548, 0.1898, 0.1238, 0.1277, 0.0418]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0549, 0.0077, 0.0560, 0.2657, 0.1400, 0.2457, 0.1217, 0.0981, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0710, 0.0212, 0.0700, 0.1988, 0.1751, 0.2104, 0.1273, 0.1014, 0.0249]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0688, 0.0119, 0.0641, 0.2145, 0.1809, 0.2167, 0.1322, 0.0962, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0433, 0.0036, 0.0499, 0.2571, 0.1281, 0.3426, 0.0994, 0.0689, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0691, 0.0102, 0.0648, 0.2303, 0.1474, 0.2431, 0.1199, 0.1014, 0.0138]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0301, 0.0013, 0.0322, 0.2836, 0.1697, 0.3262, 0.0946, 0.0596, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0669, 0.0059, 0.0522, 0.2034, 0.1638, 0.2870, 0.1267, 0.0855, 0.0085]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0838, 0.0209, 0.0709, 0.2050, 0.1428, 0.2147, 0.1149, 0.1167, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0698, 0.0147, 0.0762, 0.1901, 0.1700, 0.2224, 0.1213, 0.1135, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0750, 0.0193, 0.0795, 0.1833, 0.1567, 0.2144, 0.1336, 0.1115, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0544, 0.0037, 0.0465, 0.2536, 0.1680, 0.2854, 0.1176, 0.0658, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0881, 0.0285, 0.0769, 0.1750, 0.1536, 0.1929, 0.1308, 0.1195, 0.0346]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0888, 0.0254, 0.0796, 0.1715, 0.1429, 0.2209, 0.1136, 0.1213, 0.0358]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0482, 0.0049, 0.0406, 0.2957, 0.1460, 0.3095, 0.0748, 0.0733, 0.0069]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0605, 0.0071, 0.0484, 0.2406, 0.1402, 0.2890, 0.1247, 0.0801, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0614, 0.0055, 0.0584, 0.1786, 0.1452, 0.3711, 0.0948, 0.0762, 0.0087]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0953, 0.0329, 0.0837, 0.1579, 0.1475, 0.1974, 0.1277, 0.1199, 0.0378]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0323, 0.0028, 0.0451, 0.2532, 0.2038, 0.2999, 0.1134, 0.0446, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0247, 0.0013, 0.0298, 0.2518, 0.1156, 0.4448, 0.0865, 0.0428, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.9489e-03, 7.8343e-06, 2.3157e-03, 2.9443e-01, 5.0124e-02, 6.1496e-01,\n",
       "           2.7495e-02, 6.7044e-03, 1.7184e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0690, 0.0103, 0.0576, 0.1627, 0.2089, 0.2771, 0.1173, 0.0797, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0670, 0.0167, 0.0679, 0.1900, 0.1670, 0.2374, 0.1318, 0.0986, 0.0236]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0499, 0.0061, 0.0446, 0.2175, 0.1666, 0.2907, 0.1137, 0.1008, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0294, 0.0792, 0.1641, 0.1806, 0.2213, 0.1223, 0.0886, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0319, 0.0046, 0.0392, 0.2143, 0.1791, 0.3728, 0.0967, 0.0537, 0.0077]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0850, 0.0083, 0.0476, 0.1795, 0.1638, 0.3067, 0.1262, 0.0736, 0.0093]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0262, 0.0880, 0.1538, 0.1648, 0.2380, 0.1096, 0.1090, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0526, 0.0035, 0.0328, 0.2001, 0.1617, 0.4071, 0.0830, 0.0555, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0129, 0.0592, 0.1814, 0.1602, 0.2971, 0.1076, 0.0923, 0.0221]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0388, 0.0006, 0.0263, 0.2817, 0.1812, 0.3852, 0.0569, 0.0275, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.1932e-03, 3.0251e-05, 4.1978e-03, 2.0855e-01, 7.6807e-02, 6.6607e-01,\n",
       "           3.4957e-02, 5.1268e-03, 6.4327e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0449, 0.0013, 0.0252, 0.2201, 0.1512, 0.4467, 0.0573, 0.0494, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0747, 0.0131, 0.0645, 0.1725, 0.1570, 0.3226, 0.0859, 0.0909, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0827, 0.0209, 0.0646, 0.1664, 0.1782, 0.2566, 0.1083, 0.0981, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0807, 0.0126, 0.0763, 0.1681, 0.1522, 0.2814, 0.1052, 0.1045, 0.0189]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0910, 0.0228, 0.0703, 0.1475, 0.1908, 0.2436, 0.1089, 0.0889, 0.0362]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0438, 0.0020, 0.0294, 0.1859, 0.1261, 0.4987, 0.0564, 0.0548, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0192, 0.0687, 0.1279, 0.1931, 0.2781, 0.1291, 0.0869, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0414, 0.0030, 0.0459, 0.1436, 0.1881, 0.4299, 0.0942, 0.0483, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0519, 0.0033, 0.0508, 0.1393, 0.1925, 0.4082, 0.0668, 0.0798, 0.0074]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8290e-03, 5.5726e-06, 3.1003e-03, 1.0511e-01, 8.1381e-02, 7.9835e-01,\n",
       "           6.3338e-03, 3.8572e-03, 2.7878e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0074, 0.0593, 0.1523, 0.1959, 0.3633, 0.0945, 0.0555, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0726, 0.0114, 0.0637, 0.1558, 0.1436, 0.3409, 0.0891, 0.1055, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0873, 0.0243, 0.0857, 0.1418, 0.1802, 0.2543, 0.0916, 0.1006, 0.0343]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.4432e-03, 3.1443e-06, 1.3684e-03, 3.3509e-02, 2.4225e-02, 9.2877e-01,\n",
       "           4.5620e-03, 2.1069e-03, 1.3580e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7651e-02, 4.8778e-04, 2.5753e-02, 1.4609e-01, 1.1653e-01, 6.3369e-01,\n",
       "           4.1731e-02, 1.7033e-02, 1.0313e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0428, 0.0027, 0.0305, 0.1323, 0.1782, 0.4999, 0.0663, 0.0436, 0.0037]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0652, 0.0049, 0.0672, 0.1455, 0.2048, 0.3686, 0.0772, 0.0586, 0.0080]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0716, 0.0075, 0.0725, 0.1613, 0.1747, 0.3259, 0.0804, 0.0916, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0857, 0.0205, 0.0808, 0.1478, 0.1823, 0.2649, 0.1076, 0.0818, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0615, 0.0104, 0.0843, 0.1266, 0.2101, 0.3197, 0.1021, 0.0679, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0896, 0.0287, 0.0818, 0.1375, 0.1690, 0.2618, 0.1067, 0.0910, 0.0339]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.4251e-03, 6.7285e-07, 1.2397e-03, 6.1020e-02, 3.8445e-02, 8.9126e-01,\n",
       "           4.0551e-03, 1.5471e-03, 6.2119e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0462, 0.0034, 0.0555, 0.1592, 0.1651, 0.4412, 0.0699, 0.0523, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7823e-03, 1.2831e-06, 9.3926e-04, 6.5138e-02, 3.5684e-02, 8.9302e-01,\n",
       "           1.7761e-03, 1.6490e-03, 5.0643e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0394, 0.0007, 0.0340, 0.1188, 0.1455, 0.5704, 0.0527, 0.0362, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.3101e-04, 1.2084e-07, 1.4915e-04, 2.3956e-02, 1.6401e-02, 9.5714e-01,\n",
       "           1.7196e-03, 1.9850e-04, 4.6423e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0475, 0.0025, 0.0407, 0.1173, 0.1317, 0.5566, 0.0617, 0.0385, 0.0036]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.1328e-04, 1.4467e-08, 6.4575e-05, 9.2537e-03, 6.8090e-03, 9.8306e-01,\n",
       "           5.1251e-04, 8.3288e-05, 9.5502e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0473, 0.0040, 0.0426, 0.1189, 0.1858, 0.4644, 0.0819, 0.0473, 0.0079]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0819, 0.0062, 0.0463, 0.1274, 0.1874, 0.3756, 0.1071, 0.0564, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0872, 0.0256, 0.0828, 0.1201, 0.1657, 0.2829, 0.1147, 0.0922, 0.0288]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0624, 0.0050, 0.0574, 0.1241, 0.1905, 0.3693, 0.1118, 0.0671, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0270, 0.0936, 0.1188, 0.1702, 0.2624, 0.1178, 0.0842, 0.0334]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0573, 0.0041, 0.0429, 0.1181, 0.2143, 0.4228, 0.0806, 0.0518, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.6853e-02, 5.6298e-04, 1.5784e-02, 9.1545e-02, 8.6926e-02, 7.3007e-01,\n",
       "           4.0239e-02, 1.7176e-02, 8.4273e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0081, 0.0639, 0.1238, 0.1747, 0.3919, 0.0763, 0.0585, 0.0128]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0354, 0.0015, 0.0308, 0.0847, 0.1599, 0.5967, 0.0559, 0.0333, 0.0020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0966, 0.0293, 0.0932, 0.1156, 0.1698, 0.2276, 0.1187, 0.1082, 0.0410]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0264, 0.0010, 0.0198, 0.1171, 0.1896, 0.5609, 0.0596, 0.0238, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0621, 0.0039, 0.0553, 0.0914, 0.2162, 0.4195, 0.0761, 0.0675, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0844, 0.0139, 0.0769, 0.1191, 0.2203, 0.2753, 0.1077, 0.0797, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0823, 0.0142, 0.0801, 0.1173, 0.2054, 0.3069, 0.0908, 0.0799, 0.0232]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0401, 0.0032, 0.0538, 0.1006, 0.2025, 0.4880, 0.0681, 0.0384, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0616, 0.0052, 0.0500, 0.0940, 0.1745, 0.4974, 0.0670, 0.0456, 0.0045]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0300, 0.0008, 0.0190, 0.0875, 0.2364, 0.5720, 0.0383, 0.0149, 0.0010]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0442, 0.0028, 0.0469, 0.1383, 0.1988, 0.4525, 0.0654, 0.0472, 0.0039]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0577, 0.0072, 0.0481, 0.1170, 0.2191, 0.4102, 0.0842, 0.0487, 0.0078]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0483, 0.0061, 0.0510, 0.1506, 0.1720, 0.4289, 0.0691, 0.0604, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.8162e-06, 2.2930e-12, 2.4271e-06, 7.3854e-04, 6.3580e-03, 9.9284e-01,\n",
       "           4.9737e-05, 7.2994e-06, 8.6266e-11]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0158, 0.0745, 0.1332, 0.1811, 0.2944, 0.0986, 0.0899, 0.0224]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0009, 0.0272, 0.0911, 0.2100, 0.5481, 0.0631, 0.0302, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0653, 0.0072, 0.0520, 0.1201, 0.2212, 0.3826, 0.0820, 0.0555, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0387, 0.0011, 0.0416, 0.1048, 0.2073, 0.5245, 0.0504, 0.0290, 0.0026]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0385, 0.0013, 0.0392, 0.0870, 0.1951, 0.5612, 0.0493, 0.0267, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.9525e-03, 2.3720e-05, 4.3264e-03, 5.3099e-02, 1.7454e-01, 7.4670e-01,\n",
       "           1.1343e-02, 3.9583e-03, 6.0971e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0727, 0.0128, 0.0817, 0.1091, 0.1898, 0.3361, 0.0970, 0.0834, 0.0175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0224, 0.0007, 0.0299, 0.1060, 0.1797, 0.5910, 0.0426, 0.0264, 0.0012]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0083, 0.0716, 0.1133, 0.1656, 0.3962, 0.0828, 0.0693, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0492, 0.0055, 0.0525, 0.1076, 0.2319, 0.3965, 0.0886, 0.0562, 0.0120]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[1.3127e-02, 1.9658e-04, 1.1524e-02, 1.3729e-01, 9.5579e-02, 7.0825e-01,\n",
       "           1.5768e-02, 1.7914e-02, 3.5064e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0640, 0.0061, 0.0541, 0.1266, 0.2086, 0.3989, 0.0905, 0.0429, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0554, 0.0026, 0.0346, 0.1478, 0.1811, 0.4774, 0.0481, 0.0496, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0683, 0.0113, 0.0666, 0.1368, 0.2092, 0.3226, 0.0850, 0.0800, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0046, 0.0578, 0.0987, 0.2086, 0.4085, 0.0789, 0.0706, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0534, 0.0050, 0.0486, 0.1256, 0.2325, 0.4091, 0.0851, 0.0357, 0.0050]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0465, 0.0007, 0.0284, 0.1410, 0.1566, 0.5308, 0.0583, 0.0360, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0516, 0.0071, 0.0494, 0.1450, 0.2047, 0.4208, 0.0728, 0.0422, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0708, 0.0081, 0.0577, 0.1591, 0.1901, 0.3212, 0.1049, 0.0754, 0.0127]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.9246e-02, 4.0045e-04, 1.6510e-02, 1.3925e-01, 1.5192e-01, 6.2869e-01,\n",
       "           2.9120e-02, 1.3993e-02, 8.6685e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0427, 0.0012, 0.0278, 0.1234, 0.1351, 0.5840, 0.0495, 0.0344, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0596, 0.0059, 0.0555, 0.1270, 0.2159, 0.3922, 0.0878, 0.0470, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0744, 0.0061, 0.0632, 0.1468, 0.2060, 0.3316, 0.0879, 0.0701, 0.0140]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0030, 0.0536, 0.1711, 0.1535, 0.4289, 0.0582, 0.0518, 0.0064]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1003, 0.0225, 0.0981, 0.1131, 0.1903, 0.2365, 0.1083, 0.0979, 0.0332]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0311, 0.0016, 0.0341, 0.1274, 0.1440, 0.5745, 0.0541, 0.0313, 0.0019]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0909, 0.0091, 0.0629, 0.1358, 0.1974, 0.3340, 0.0914, 0.0656, 0.0130]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2209e-02, 7.0802e-05, 1.0742e-02, 1.1461e-01, 1.0202e-01, 7.2496e-01,\n",
       "           2.8516e-02, 6.6424e-03, 2.2536e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0625, 0.0098, 0.0788, 0.1307, 0.1963, 0.3609, 0.0744, 0.0705, 0.0160]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0351, 0.0017, 0.0331, 0.1567, 0.1640, 0.5482, 0.0387, 0.0207, 0.0018]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0960, 0.0295, 0.1042, 0.1177, 0.1644, 0.2488, 0.1003, 0.0975, 0.0416]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0891, 0.0189, 0.0914, 0.1362, 0.1746, 0.2815, 0.0900, 0.0899, 0.0284]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0768, 0.0055, 0.0714, 0.1610, 0.1315, 0.3973, 0.0687, 0.0794, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.9307e-03, 3.2678e-06, 5.2637e-03, 7.3315e-02, 4.7274e-02, 8.5575e-01,\n",
       "           8.6316e-03, 5.7878e-03, 4.3043e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0658, 0.0055, 0.0727, 0.1459, 0.1771, 0.3839, 0.0755, 0.0639, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0791, 0.0131, 0.0765, 0.1558, 0.1660, 0.2916, 0.0997, 0.0957, 0.0225]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1007, 0.0261, 0.0981, 0.1229, 0.1636, 0.2330, 0.1184, 0.1050, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0321, 0.0014, 0.0415, 0.1342, 0.1877, 0.4849, 0.0730, 0.0416, 0.0035]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0909, 0.0283, 0.0978, 0.1340, 0.1710, 0.2381, 0.1125, 0.0966, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1027, 0.0143, 0.0894, 0.1490, 0.1601, 0.2840, 0.0923, 0.0859, 0.0223]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1001, 0.0118, 0.0957, 0.1332, 0.1661, 0.2885, 0.1059, 0.0799, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.2112e-03, 5.9346e-06, 2.4503e-03, 5.8930e-02, 2.6802e-02, 8.9852e-01,\n",
       "           8.7037e-03, 1.3715e-03, 8.1023e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0712, 0.0056, 0.0546, 0.1786, 0.1697, 0.3722, 0.0782, 0.0605, 0.0095]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0885, 0.0193, 0.0886, 0.1283, 0.1931, 0.2631, 0.1177, 0.0772, 0.0241]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0036, 0.0437, 0.1582, 0.1400, 0.4391, 0.0758, 0.0713, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0888, 0.0280, 0.1025, 0.1346, 0.1739, 0.2109, 0.1232, 0.1061, 0.0321]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6727e-05, 2.9860e-10, 1.0854e-04, 4.5540e-02, 1.3337e-02, 9.4032e-01,\n",
       "           6.3379e-04, 3.0102e-05, 6.6746e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1036, 0.0473, 0.1112, 0.1233, 0.1517, 0.1913, 0.1136, 0.1024, 0.0556]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0968, 0.0359, 0.1096, 0.1189, 0.1605, 0.2083, 0.1139, 0.1106, 0.0454]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0912, 0.0229, 0.1013, 0.1318, 0.1819, 0.2390, 0.1172, 0.0905, 0.0242]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0721, 0.0104, 0.0794, 0.1469, 0.1671, 0.3230, 0.0807, 0.0999, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1011, 0.0197, 0.0802, 0.1453, 0.1499, 0.2951, 0.1033, 0.0805, 0.0249]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0901, 0.0145, 0.0810, 0.1389, 0.1589, 0.2918, 0.0933, 0.1037, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0356, 0.1094, 0.1257, 0.1709, 0.1959, 0.1206, 0.1080, 0.0427]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0734, 0.0117, 0.0750, 0.1512, 0.1582, 0.3462, 0.0984, 0.0713, 0.0146]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0655, 0.0139, 0.0806, 0.1282, 0.1662, 0.3072, 0.1277, 0.0907, 0.0201]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1038, 0.0335, 0.0986, 0.1334, 0.1564, 0.2097, 0.1107, 0.1136, 0.0402]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0951, 0.0281, 0.1109, 0.1243, 0.1600, 0.2168, 0.1158, 0.1031, 0.0461]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0553, 0.0044, 0.0591, 0.1814, 0.1348, 0.3988, 0.0996, 0.0586, 0.0081]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0880, 0.0231, 0.0907, 0.1390, 0.1798, 0.2378, 0.1350, 0.0817, 0.0250]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0967, 0.0374, 0.0994, 0.1208, 0.1854, 0.1838, 0.1253, 0.1053, 0.0459]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1101, 0.0517, 0.0996, 0.1235, 0.1525, 0.1713, 0.1191, 0.1157, 0.0564]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0354, 0.0927, 0.1192, 0.2079, 0.1921, 0.1282, 0.0938, 0.0394]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0795, 0.0108, 0.1096, 0.1282, 0.1859, 0.2479, 0.1180, 0.1004, 0.0198]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1143, 0.0238, 0.0914, 0.1548, 0.1670, 0.2165, 0.1091, 0.0935, 0.0297]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0908, 0.0334, 0.0983, 0.1245, 0.1749, 0.2033, 0.1228, 0.1112, 0.0409]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0937, 0.0329, 0.0970, 0.1136, 0.1829, 0.2139, 0.1219, 0.0993, 0.0448]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.4368e-02, 1.8603e-04, 2.8430e-02, 9.5073e-02, 1.9854e-01, 5.9114e-01,\n",
       "           4.7224e-02, 2.4552e-02, 4.8126e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.3616e-02, 2.1173e-04, 2.0069e-02, 9.8120e-02, 1.3305e-01, 6.8159e-01,\n",
       "           2.9999e-02, 2.2339e-02, 1.0126e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0831, 0.0049, 0.0739, 0.1424, 0.1618, 0.3354, 0.0880, 0.0999, 0.0106]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0335, 0.0017, 0.0505, 0.1622, 0.2086, 0.4059, 0.0899, 0.0447, 0.0029]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[8.1270e-03, 1.4251e-05, 6.2418e-03, 7.6829e-02, 1.4013e-01, 7.4336e-01,\n",
       "           2.0566e-02, 4.6611e-03, 7.9827e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0077, 0.0719, 0.1244, 0.1988, 0.3303, 0.1075, 0.0769, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0858, 0.0190, 0.0893, 0.1184, 0.1784, 0.2515, 0.1311, 0.0982, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0421, 0.0016, 0.0592, 0.1086, 0.2254, 0.4138, 0.0925, 0.0523, 0.0044]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0897, 0.0203, 0.0904, 0.1367, 0.1934, 0.2264, 0.1084, 0.1103, 0.0243]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5221e-03, 1.0784e-06, 2.6836e-03, 3.5026e-02, 2.5537e-02, 9.2305e-01,\n",
       "           9.7508e-03, 2.4187e-03, 8.3190e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0386, 0.0014, 0.0347, 0.1523, 0.1562, 0.5108, 0.0676, 0.0360, 0.0023]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0957, 0.0277, 0.0949, 0.1212, 0.1625, 0.2000, 0.1339, 0.1227, 0.0415]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.6040e-02, 4.0320e-04, 2.5700e-02, 9.1616e-02, 1.6311e-01, 5.9462e-01,\n",
       "           7.7645e-02, 2.9605e-02, 1.2599e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0900, 0.0125, 0.0775, 0.1324, 0.1604, 0.3011, 0.1121, 0.0933, 0.0206]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0282, 0.0010, 0.0385, 0.1336, 0.1744, 0.4848, 0.0900, 0.0471, 0.0024]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0881, 0.0237, 0.0991, 0.1056, 0.1717, 0.2506, 0.1152, 0.1092, 0.0366]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0942, 0.0208, 0.0868, 0.1377, 0.1597, 0.2599, 0.1129, 0.0997, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0523, 0.0061, 0.0636, 0.1073, 0.1848, 0.3957, 0.1132, 0.0687, 0.0084]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0813, 0.0218, 0.0834, 0.1176, 0.1827, 0.2705, 0.1263, 0.0924, 0.0240]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.2920e-03, 7.0075e-07, 1.1610e-03, 3.4530e-02, 2.3427e-02, 9.3137e-01,\n",
       "           5.5792e-03, 1.6311e-03, 1.3043e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0949, 0.0228, 0.0876, 0.1214, 0.1609, 0.2543, 0.1225, 0.1046, 0.0309]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8648e-04, 1.3539e-08, 2.4567e-04, 8.1128e-03, 5.7829e-03, 9.8395e-01,\n",
       "           1.2591e-03, 4.6725e-04, 1.4965e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0497, 0.0077, 0.0768, 0.0965, 0.2390, 0.3268, 0.1116, 0.0773, 0.0147]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0749, 0.0098, 0.0833, 0.0913, 0.2275, 0.2995, 0.1207, 0.0785, 0.0145]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0283, 0.1041, 0.1088, 0.1709, 0.2404, 0.1229, 0.1008, 0.0331]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.8445e-04, 9.0254e-08, 4.5316e-04, 9.0519e-03, 1.2230e-02, 9.7114e-01,\n",
       "           5.1886e-03, 1.0500e-03, 9.6520e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0978, 0.0341, 0.1009, 0.1137, 0.1774, 0.1842, 0.1268, 0.1181, 0.0472]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0716, 0.0060, 0.0622, 0.1219, 0.1873, 0.3514, 0.1098, 0.0789, 0.0110]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.1165e-03, 3.5691e-07, 1.0297e-03, 4.1795e-02, 5.3991e-02, 8.8261e-01,\n",
       "           1.7276e-02, 2.1749e-03, 1.6193e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0924, 0.0259, 0.0911, 0.1146, 0.1910, 0.2289, 0.1306, 0.0970, 0.0286]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0771, 0.0098, 0.0710, 0.1312, 0.1606, 0.3237, 0.1181, 0.0898, 0.0187]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1029, 0.0215, 0.0837, 0.0926, 0.2343, 0.2124, 0.1314, 0.0925, 0.0288]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0860, 0.0093, 0.0734, 0.1401, 0.1603, 0.3204, 0.1033, 0.0952, 0.0121]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.6616e-02, 1.8077e-04, 1.9768e-02, 1.0641e-01, 5.5606e-02, 6.9727e-01,\n",
       "           5.7520e-02, 2.6165e-02, 4.6786e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0319, 0.0011, 0.0308, 0.1040, 0.1343, 0.5680, 0.0968, 0.0310, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0931, 0.0218, 0.0888, 0.1192, 0.1627, 0.2609, 0.1367, 0.0905, 0.0264]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0772, 0.0116, 0.0766, 0.1201, 0.2022, 0.2728, 0.1182, 0.1019, 0.0194]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0651, 0.0070, 0.0716, 0.0984, 0.2632, 0.2868, 0.1346, 0.0631, 0.0101]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.2308e-02, 5.8348e-05, 7.9575e-03, 9.3742e-02, 1.3380e-01, 6.8125e-01,\n",
       "           5.3034e-02, 1.7641e-02, 2.1243e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0146, 0.0689, 0.1200, 0.2116, 0.3092, 0.1024, 0.0725, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0826, 0.0184, 0.0968, 0.1009, 0.2198, 0.2225, 0.1277, 0.1041, 0.0273]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0573, 0.0052, 0.0597, 0.1242, 0.1557, 0.4121, 0.0953, 0.0803, 0.0102]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.7785e-02, 3.0777e-04, 1.5647e-02, 1.0680e-01, 1.0254e-01, 6.4832e-01,\n",
       "           7.3165e-02, 3.4354e-02, 1.0917e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1065, 0.0250, 0.0906, 0.1138, 0.1608, 0.2316, 0.1201, 0.1148, 0.0366]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1114, 0.0280, 0.0837, 0.1219, 0.1617, 0.2195, 0.1243, 0.1141, 0.0353]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0893, 0.0089, 0.0637, 0.1392, 0.1666, 0.3086, 0.1081, 0.0980, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0280, 0.0007, 0.0249, 0.1260, 0.1733, 0.5370, 0.0656, 0.0428, 0.0015]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0062, 0.0675, 0.1069, 0.1693, 0.3913, 0.0990, 0.0805, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0813, 0.0158, 0.0756, 0.1159, 0.1934, 0.2754, 0.1223, 0.0981, 0.0222]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0833, 0.0204, 0.0826, 0.1153, 0.1840, 0.2727, 0.1225, 0.0910, 0.0283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0997, 0.0264, 0.0904, 0.1211, 0.1681, 0.2302, 0.1060, 0.1126, 0.0455]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1008, 0.0277, 0.0948, 0.1017, 0.1758, 0.2188, 0.1246, 0.1187, 0.0371]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0946, 0.0106, 0.0807, 0.1145, 0.1624, 0.3020, 0.1246, 0.0970, 0.0137]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[5.9159e-03, 3.1939e-05, 3.2650e-03, 4.4915e-02, 4.8543e-02, 8.6180e-01,\n",
       "           2.5486e-02, 9.9636e-03, 7.7341e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0866, 0.0078, 0.0879, 0.1106, 0.1666, 0.3190, 0.0988, 0.1074, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0966, 0.0297, 0.0966, 0.0951, 0.1782, 0.2379, 0.1281, 0.1068, 0.0308]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[7.6997e-03, 2.2290e-04, 1.2393e-02, 7.0858e-02, 1.3513e-01, 7.0835e-01,\n",
       "           4.6153e-02, 1.8424e-02, 7.6963e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0372, 0.0013, 0.0302, 0.1089, 0.1280, 0.5358, 0.1070, 0.0484, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0531, 0.0035, 0.0573, 0.0906, 0.2166, 0.3889, 0.0934, 0.0873, 0.0092]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0375, 0.0010, 0.0335, 0.0910, 0.1614, 0.5576, 0.0625, 0.0533, 0.0022]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0983, 0.0368, 0.0993, 0.1125, 0.1694, 0.2006, 0.1142, 0.1149, 0.0539]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0876, 0.0091, 0.0595, 0.1375, 0.1562, 0.3253, 0.1110, 0.1012, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0026, 0.0443, 0.1269, 0.1676, 0.4418, 0.0940, 0.0701, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.7353e-07, 5.7695e-15, 1.0336e-07, 2.8528e-04, 7.6066e-05, 9.9959e-01,\n",
       "           4.4816e-05, 1.5243e-06, 6.5039e-13]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0675, 0.0038, 0.0539, 0.1063, 0.1885, 0.3558, 0.1003, 0.1103, 0.0136]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.0250, 0.0956, 0.1101, 0.1707, 0.2182, 0.1256, 0.1175, 0.0377]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0656, 0.0037, 0.0416, 0.1190, 0.1999, 0.3590, 0.1052, 0.0992, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0876, 0.0219, 0.0947, 0.1114, 0.1831, 0.2267, 0.1312, 0.1068, 0.0365]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0543, 0.0045, 0.0533, 0.1147, 0.1583, 0.4423, 0.0998, 0.0630, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.4268e-03, 4.6720e-07, 6.4393e-04, 4.8895e-02, 1.9255e-02, 9.1924e-01,\n",
       "           8.0745e-03, 2.4584e-03, 4.1139e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.6752e-03, 6.7114e-06, 3.1123e-03, 4.7285e-02, 5.0628e-02, 8.6070e-01,\n",
       "           2.1109e-02, 1.4370e-02, 1.0898e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0575, 0.0054, 0.0569, 0.1321, 0.1611, 0.3555, 0.1199, 0.0991, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1105, 0.0285, 0.0935, 0.1228, 0.1536, 0.2125, 0.1295, 0.1104, 0.0388]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1043, 0.0389, 0.0991, 0.1172, 0.1491, 0.1947, 0.1284, 0.1178, 0.0504]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0810, 0.0213, 0.0881, 0.1213, 0.1952, 0.2409, 0.1332, 0.0983, 0.0207]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0508, 0.0034, 0.0536, 0.1153, 0.1663, 0.4315, 0.0991, 0.0717, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0593, 0.0042, 0.0640, 0.1269, 0.1498, 0.4084, 0.1177, 0.0620, 0.0077]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0600, 0.0024, 0.0524, 0.1190, 0.1119, 0.4918, 0.0841, 0.0711, 0.0073]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0033, 0.0551, 0.1108, 0.1715, 0.4211, 0.1005, 0.0673, 0.0059]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0897, 0.0062, 0.0582, 0.1438, 0.1668, 0.3545, 0.0973, 0.0721, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0907, 0.0202, 0.0863, 0.1322, 0.1588, 0.2712, 0.1089, 0.1006, 0.0312]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0604, 0.0049, 0.0409, 0.1596, 0.1217, 0.4481, 0.0850, 0.0711, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0872, 0.0059, 0.0705, 0.1339, 0.1288, 0.3612, 0.1120, 0.0884, 0.0123]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0900, 0.0170, 0.0906, 0.1354, 0.1851, 0.2276, 0.1156, 0.1070, 0.0317]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0542, 0.0032, 0.0503, 0.1350, 0.1460, 0.4586, 0.0889, 0.0590, 0.0050]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.0811e-02, 5.4054e-05, 9.3744e-03, 7.5009e-02, 4.6837e-02, 8.2909e-01,\n",
       "           2.0704e-02, 7.9453e-03, 1.7038e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0169, 0.0931, 0.1320, 0.1947, 0.2553, 0.1225, 0.0839, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[8.1149e-03, 3.7612e-05, 8.5657e-03, 1.0992e-01, 7.6846e-02, 7.6137e-01,\n",
       "           2.7724e-02, 7.2835e-03, 1.4314e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0035, 0.0610, 0.1263, 0.2177, 0.3624, 0.1013, 0.0722, 0.0082]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0458, 0.0020, 0.0538, 0.0997, 0.1549, 0.4961, 0.0810, 0.0591, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0326, 0.0017, 0.0421, 0.1260, 0.1518, 0.5121, 0.0867, 0.0435, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0580, 0.0022, 0.0523, 0.1693, 0.1790, 0.3799, 0.0882, 0.0662, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0868, 0.0212, 0.0971, 0.1320, 0.1741, 0.2455, 0.1187, 0.0947, 0.0299]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0470, 0.0068, 0.0688, 0.1024, 0.2194, 0.3640, 0.1191, 0.0608, 0.0117]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0840, 0.0206, 0.1038, 0.1120, 0.1990, 0.2355, 0.1094, 0.1012, 0.0343]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.3623e-02, 2.0746e-04, 1.5912e-02, 1.2704e-01, 6.0358e-02, 7.0189e-01,\n",
       "           4.1678e-02, 2.8704e-02, 5.8630e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.8285e-02, 4.6788e-04, 2.7925e-02, 1.9006e-01, 1.2382e-01, 5.0009e-01,\n",
       "           7.7816e-02, 3.0545e-02, 9.9719e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0408, 0.0022, 0.0753, 0.1063, 0.2753, 0.3488, 0.0930, 0.0524, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0913, 0.0198, 0.0920, 0.1306, 0.1878, 0.2445, 0.1163, 0.0882, 0.0295]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.4708e-02, 9.7714e-05, 2.5191e-02, 1.0275e-01, 1.7510e-01, 5.9959e-01,\n",
       "           5.9874e-02, 2.2239e-02, 4.4758e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.7086e-02, 1.9330e-04, 2.2845e-02, 9.5299e-02, 1.4435e-01, 6.5512e-01,\n",
       "           4.5925e-02, 1.8714e-02, 4.6421e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0655, 0.0044, 0.0665, 0.1320, 0.1935, 0.3731, 0.0886, 0.0695, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0541, 0.0012, 0.0564, 0.1515, 0.1735, 0.4067, 0.0835, 0.0685, 0.0046]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0870, 0.0172, 0.1112, 0.1045, 0.2208, 0.2164, 0.1062, 0.1059, 0.0310]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0578, 0.0055, 0.0875, 0.1037, 0.2693, 0.2970, 0.1110, 0.0567, 0.0115]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.6911e-02, 2.1569e-04, 2.2580e-02, 1.0265e-01, 1.5388e-01, 6.3877e-01,\n",
       "           4.4006e-02, 2.0296e-02, 6.9228e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.0367e-02, 6.4999e-05, 2.0096e-02, 1.0162e-01, 1.6315e-01, 6.4748e-01,\n",
       "           4.4464e-02, 1.2581e-02, 1.7173e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0489, 0.0054, 0.0684, 0.1446, 0.2475, 0.3029, 0.1096, 0.0639, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0445, 0.0025, 0.0707, 0.1124, 0.2406, 0.3667, 0.0835, 0.0739, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0879, 0.0169, 0.0929, 0.1282, 0.2049, 0.2344, 0.1295, 0.0826, 0.0227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1024, 0.0124, 0.0779, 0.1230, 0.2291, 0.2415, 0.1130, 0.0789, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.4176e-07, 2.2320e-14, 2.1612e-07, 2.2857e-03, 3.9561e-04, 9.9718e-01,\n",
       "           1.3615e-04, 9.0908e-07, 2.8579e-12]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0732, 0.0155, 0.0884, 0.1119, 0.2564, 0.2384, 0.1152, 0.0794, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0525, 0.0035, 0.0526, 0.1157, 0.2092, 0.3967, 0.1084, 0.0554, 0.0061]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0407, 0.0023, 0.0645, 0.1212, 0.2824, 0.3357, 0.0901, 0.0563, 0.0068]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0418, 0.0012, 0.0620, 0.1112, 0.2219, 0.4241, 0.0940, 0.0395, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0545, 0.0055, 0.0708, 0.1190, 0.2465, 0.3078, 0.1246, 0.0631, 0.0083]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0828, 0.0134, 0.0967, 0.1160, 0.2438, 0.2168, 0.0990, 0.1085, 0.0230]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0871, 0.0204, 0.1035, 0.1170, 0.1966, 0.2119, 0.1236, 0.1058, 0.0342]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.7175e-03, 5.0588e-07, 2.7210e-03, 4.2988e-02, 4.5778e-02, 8.9225e-01,\n",
       "           1.1728e-02, 2.8122e-03, 6.6120e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0505, 0.0036, 0.0699, 0.1168, 0.2406, 0.3397, 0.1062, 0.0640, 0.0086]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0294, 0.0023, 0.0603, 0.1146, 0.2229, 0.4158, 0.0976, 0.0507, 0.0063]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0694, 0.0092, 0.0997, 0.1049, 0.2453, 0.2497, 0.1277, 0.0772, 0.0169]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0097, 0.0766, 0.1200, 0.2186, 0.2785, 0.1183, 0.0917, 0.0188]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0737, 0.0147, 0.0915, 0.1195, 0.2124, 0.2506, 0.1245, 0.0893, 0.0238]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[9.3109e-03, 4.9946e-05, 1.4204e-02, 1.0392e-01, 1.2136e-01, 6.8483e-01,\n",
       "           4.8451e-02, 1.7622e-02, 2.4035e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0623, 0.0041, 0.0659, 0.1426, 0.1763, 0.3341, 0.1279, 0.0778, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0646, 0.0041, 0.0729, 0.1470, 0.2122, 0.3149, 0.0930, 0.0814, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.0691e-04, 7.3386e-08, 1.6676e-03, 5.7655e-02, 1.0786e-01, 8.1624e-01,\n",
       "           1.5558e-02, 7.1582e-04, 1.2455e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0895, 0.0204, 0.1057, 0.1211, 0.1971, 0.1972, 0.1341, 0.1077, 0.0272]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0706, 0.0077, 0.0819, 0.1185, 0.2045, 0.2999, 0.1314, 0.0702, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[3.3165e-02, 2.6503e-04, 2.5067e-02, 1.9928e-01, 1.0835e-01, 5.3649e-01,\n",
       "           6.9466e-02, 2.7250e-02, 6.7056e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.6869e-02, 3.1362e-04, 3.9930e-02, 1.2672e-01, 1.4428e-01, 5.2683e-01,\n",
       "           1.0357e-01, 3.0718e-02, 7.7290e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.5203e-07, 4.8981e-16, 5.2992e-08, 3.4080e-03, 2.9636e-04, 9.9627e-01,\n",
       "           2.4790e-05, 1.6476e-07, 1.5931e-13]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.2410e-04, 1.1237e-06, 2.4186e-03, 7.8492e-02, 1.3182e-01, 7.6124e-01,\n",
       "           2.4108e-02, 1.2977e-03, 4.4133e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0636, 0.0087, 0.0857, 0.1317, 0.2155, 0.2617, 0.1243, 0.0929, 0.0159]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0927, 0.0154, 0.0907, 0.1137, 0.2110, 0.2354, 0.1344, 0.0873, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0550, 0.0027, 0.0760, 0.1145, 0.2386, 0.3446, 0.0991, 0.0644, 0.0051]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0755, 0.0078, 0.0773, 0.1164, 0.2363, 0.2573, 0.1337, 0.0836, 0.0120]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0485, 0.0035, 0.0603, 0.0942, 0.2802, 0.3142, 0.1328, 0.0603, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0515, 0.0037, 0.0771, 0.1538, 0.2065, 0.3104, 0.1042, 0.0837, 0.0090]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0688, 0.0095, 0.0857, 0.1257, 0.2455, 0.2429, 0.1154, 0.0874, 0.0192]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0474, 0.0023, 0.0540, 0.1004, 0.2744, 0.3492, 0.1047, 0.0635, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.5990e-04, 2.3059e-08, 7.0870e-04, 2.7269e-02, 3.5484e-02, 9.2689e-01,\n",
       "           8.7109e-03, 5.7417e-04, 3.1722e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0512, 0.0041, 0.0684, 0.1127, 0.2463, 0.3206, 0.1209, 0.0691, 0.0067]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.7664e-02, 2.4165e-04, 1.8883e-02, 1.5936e-01, 1.8246e-01, 4.8703e-01,\n",
       "           6.8008e-02, 5.5708e-02, 6.3770e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0802, 0.0162, 0.0864, 0.0907, 0.3034, 0.1775, 0.1420, 0.0856, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0730, 0.0091, 0.0953, 0.1072, 0.2929, 0.2063, 0.1137, 0.0899, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0925, 0.0307, 0.1038, 0.1072, 0.2107, 0.1792, 0.1328, 0.1081, 0.0351]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1041, 0.0224, 0.1001, 0.1221, 0.2127, 0.1781, 0.1244, 0.1015, 0.0345]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0674, 0.0051, 0.0873, 0.1222, 0.2626, 0.2407, 0.1150, 0.0898, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0400, 0.0021, 0.0499, 0.1202, 0.2565, 0.3433, 0.1190, 0.0637, 0.0052]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0743, 0.0209, 0.1102, 0.0890, 0.2707, 0.1765, 0.1153, 0.1107, 0.0322]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0525, 0.0051, 0.0873, 0.0740, 0.4145, 0.1572, 0.1152, 0.0845, 0.0096]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0995, 0.0065, 0.0819, 0.1037, 0.2572, 0.2027, 0.1146, 0.1229, 0.0109]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1044, 0.0443, 0.1151, 0.1025, 0.1870, 0.1543, 0.1227, 0.1174, 0.0524]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1008, 0.0392, 0.1028, 0.1133, 0.1899, 0.1473, 0.1259, 0.1319, 0.0489]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.2144e-02, 9.3604e-05, 3.2464e-02, 1.0779e-01, 2.8476e-01, 3.6789e-01,\n",
       "           9.0771e-02, 9.3553e-02, 5.2131e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0799, 0.0221, 0.1025, 0.1063, 0.2586, 0.1704, 0.1281, 0.1066, 0.0254]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1139, 0.0308, 0.1086, 0.1024, 0.2047, 0.1524, 0.1369, 0.1150, 0.0352]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1013, 0.0247, 0.1003, 0.0941, 0.2439, 0.1566, 0.1228, 0.1185, 0.0378]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1052, 0.0227, 0.1109, 0.1034, 0.2232, 0.1575, 0.1170, 0.1232, 0.0370]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1076, 0.0342, 0.1073, 0.1022, 0.2175, 0.1379, 0.1468, 0.1081, 0.0386]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1300, 0.0199, 0.0981, 0.1081, 0.2442, 0.1508, 0.1144, 0.1129, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0982, 0.0401, 0.1048, 0.0976, 0.2231, 0.1396, 0.1315, 0.1162, 0.0489]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1020, 0.0351, 0.1094, 0.0973, 0.1898, 0.1650, 0.1243, 0.1286, 0.0486]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0715, 0.0044, 0.0771, 0.0924, 0.3027, 0.2382, 0.1270, 0.0796, 0.0071]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1091, 0.0262, 0.0895, 0.1027, 0.2236, 0.1629, 0.1305, 0.1253, 0.0303]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0860, 0.0165, 0.0950, 0.0920, 0.2505, 0.1919, 0.1283, 0.1094, 0.0305]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.1473e-02, 3.6097e-04, 2.5544e-02, 6.2497e-02, 3.9972e-01, 3.4227e-01,\n",
       "           9.3792e-02, 4.2964e-02, 1.3820e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1023, 0.0287, 0.0897, 0.1129, 0.2052, 0.1818, 0.1316, 0.1136, 0.0341]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0788, 0.0116, 0.0873, 0.0886, 0.3048, 0.1936, 0.1345, 0.0851, 0.0156]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0820, 0.0104, 0.0971, 0.1084, 0.2319, 0.2197, 0.1354, 0.1010, 0.0139]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.7931e-03, 7.7134e-07, 1.9165e-03, 9.4313e-02, 1.9546e-01, 6.4798e-01,\n",
       "           4.5701e-02, 1.1830e-02, 1.6143e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1074, 0.0388, 0.0999, 0.0953, 0.2073, 0.1503, 0.1299, 0.1249, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.9926e-03, 8.9203e-07, 6.4528e-03, 6.8655e-02, 3.0564e-01, 5.5360e-01,\n",
       "           5.0974e-02, 1.1681e-02, 1.0795e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1199, 0.0207, 0.1091, 0.1056, 0.2126, 0.1614, 0.1268, 0.1176, 0.0263]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0859, 0.0060, 0.0896, 0.0936, 0.3138, 0.1739, 0.1177, 0.1087, 0.0109]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0647, 0.0073, 0.0857, 0.0734, 0.3632, 0.1645, 0.1277, 0.1008, 0.0126]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.1832e-03, 3.8668e-06, 8.0238e-03, 1.0945e-01, 2.0991e-01, 6.0686e-01,\n",
       "           4.2355e-02, 1.8199e-02, 1.0463e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0447, 0.0012, 0.0564, 0.0799, 0.2565, 0.3989, 0.1116, 0.0476, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0910, 0.0345, 0.1097, 0.1133, 0.1781, 0.1864, 0.1197, 0.1198, 0.0475]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1041, 0.0175, 0.1058, 0.0966, 0.2018, 0.1944, 0.1205, 0.1277, 0.0316]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0404, 0.0010, 0.0326, 0.1266, 0.2324, 0.4058, 0.0946, 0.0639, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0859, 0.0058, 0.1056, 0.0881, 0.2488, 0.2574, 0.1185, 0.0781, 0.0119]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0620, 0.0031, 0.0453, 0.1210, 0.2414, 0.3461, 0.1220, 0.0543, 0.0049]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.5385e-03, 3.2538e-06, 2.7868e-03, 8.8315e-02, 8.8349e-02, 7.8382e-01,\n",
       "           2.5610e-02, 7.5539e-03, 2.2208e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1083, 0.0129, 0.1104, 0.0986, 0.2079, 0.2245, 0.1277, 0.0889, 0.0209]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0973, 0.0081, 0.0951, 0.1134, 0.2204, 0.2235, 0.1156, 0.1087, 0.0180]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0529, 0.0042, 0.0698, 0.1448, 0.2025, 0.3266, 0.1106, 0.0811, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1059, 0.0296, 0.1001, 0.1185, 0.1698, 0.2046, 0.1193, 0.1103, 0.0420]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0612, 0.0056, 0.0899, 0.1213, 0.2204, 0.3134, 0.1000, 0.0783, 0.0100]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.5863e-02, 9.5941e-05, 1.2847e-02, 1.1690e-01, 8.6257e-02, 7.0125e-01,\n",
       "           3.7886e-02, 1.8558e-02, 3.5094e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.8516e-02, 4.6675e-04, 3.8391e-02, 1.1118e-01, 2.3426e-01, 4.9378e-01,\n",
       "           6.8653e-02, 3.3754e-02, 9.9404e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0427, 0.0010, 0.0415, 0.1075, 0.2132, 0.4515, 0.0851, 0.0543, 0.0031]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0639, 0.0039, 0.0647, 0.1084, 0.2331, 0.3476, 0.1100, 0.0619, 0.0065]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0759, 0.0072, 0.0843, 0.1186, 0.1784, 0.3123, 0.1105, 0.0979, 0.0148]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0669, 0.0050, 0.0819, 0.1149, 0.2157, 0.3069, 0.1226, 0.0788, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[9.8415e-03, 2.4956e-05, 8.3376e-03, 1.1613e-01, 8.3731e-02, 7.1423e-01,\n",
       "           3.8752e-02, 2.8769e-02, 1.7953e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0775, 0.0138, 0.0885, 0.1363, 0.1913, 0.2534, 0.1087, 0.1061, 0.0244]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0258, 0.0007, 0.0360, 0.1198, 0.1083, 0.5989, 0.0657, 0.0434, 0.0013]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.5079e-03, 1.3220e-05, 5.1300e-03, 6.7412e-02, 4.2691e-02, 8.5199e-01,\n",
       "           2.3475e-02, 5.7262e-03, 5.6945e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.0444e-10, 7.1713e-18, 1.0387e-08, 6.2350e-05, 2.5873e-06, 9.9993e-01,\n",
       "           2.2927e-06, 2.7043e-08, 7.5059e-16]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.3887e-03, 2.1214e-06, 1.8871e-03, 5.4368e-02, 1.9224e-02, 9.0862e-01,\n",
       "           9.9172e-03, 3.5748e-03, 1.4472e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.0677e-03, 6.5503e-06, 2.6923e-03, 1.0706e-01, 3.2269e-02, 8.3219e-01,\n",
       "           1.5061e-02, 6.6265e-03, 3.4436e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.0769e-02, 4.5637e-05, 1.3453e-02, 7.0605e-02, 8.9452e-02, 7.6650e-01,\n",
       "           3.0379e-02, 1.8569e-02, 2.3133e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0595, 0.0036, 0.0619, 0.1607, 0.1371, 0.3992, 0.0901, 0.0807, 0.0072]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0597, 0.0058, 0.0884, 0.1346, 0.1748, 0.3503, 0.0958, 0.0815, 0.0091]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0329, 0.0013, 0.0413, 0.1183, 0.1452, 0.5466, 0.0668, 0.0447, 0.0030]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0311e-05, 2.2504e-10, 4.2295e-05, 3.0573e-02, 1.2043e-03, 9.6769e-01,\n",
       "           3.1589e-04, 1.5360e-04, 6.3623e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0998e-03, 3.3322e-07, 9.9497e-04, 3.7305e-02, 8.7372e-03, 9.3760e-01,\n",
       "           8.9852e-03, 4.2697e-03, 4.9726e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.4477e-02, 6.7981e-05, 1.1033e-02, 1.0739e-01, 8.0783e-02, 7.3643e-01,\n",
       "           3.5594e-02, 1.4048e-02, 1.7565e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[2.6062e-03, 8.7317e-06, 3.4537e-03, 1.1920e-01, 1.0175e-01, 7.5274e-01,\n",
       "           1.4403e-02, 5.8138e-03, 2.3115e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0956, 0.0177, 0.0891, 0.1390, 0.1702, 0.2504, 0.1185, 0.0962, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0878, 0.0184, 0.0819, 0.1199, 0.1863, 0.2860, 0.1082, 0.0877, 0.0239]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[1.3884e-02, 9.1714e-05, 1.4851e-02, 9.2978e-02, 8.9825e-02, 7.2477e-01,\n",
       "           3.2364e-02, 3.1019e-02, 2.2142e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0889, 0.0122, 0.0992, 0.1552, 0.1549, 0.2637, 0.1074, 0.0984, 0.0202]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0678, 0.0017, 0.0465, 0.1390, 0.1059, 0.4918, 0.0757, 0.0674, 0.0043]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0880, 0.0172, 0.0978, 0.1167, 0.1898, 0.2734, 0.1090, 0.0866, 0.0214]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0664, 0.0043, 0.0778, 0.1582, 0.1355, 0.3470, 0.1076, 0.0944, 0.0089]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.1102e-03, 3.6902e-06, 5.7054e-03, 8.4879e-02, 1.9997e-02, 8.6807e-01,\n",
       "           1.0963e-02, 6.2410e-03, 2.8610e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0467, 0.0044, 0.0728, 0.1476, 0.2165, 0.3461, 0.0963, 0.0620, 0.0076]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0260, 0.0005, 0.0236, 0.2170, 0.1107, 0.4953, 0.0757, 0.0503, 0.0009]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0760, 0.0071, 0.0692, 0.1618, 0.1886, 0.2763, 0.1071, 0.1006, 0.0133]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1049, 0.0353, 0.1085, 0.1148, 0.1838, 0.1859, 0.1180, 0.1115, 0.0372]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0989, 0.0049, 0.0919, 0.1223, 0.1992, 0.2821, 0.1006, 0.0903, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0418, 0.0011, 0.0343, 0.1372, 0.1247, 0.5458, 0.0694, 0.0424, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.0434e-04, 6.9120e-08, 4.9615e-04, 8.6754e-02, 7.7581e-03, 8.9701e-01,\n",
       "           4.2065e-03, 3.1731e-03, 1.3034e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0980, 0.0108, 0.1059, 0.1130, 0.2006, 0.2390, 0.1200, 0.0931, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1023, 0.0249, 0.1054, 0.1197, 0.1953, 0.1806, 0.1194, 0.1180, 0.0344]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1065, 0.0245, 0.1171, 0.1052, 0.1876, 0.1903, 0.1243, 0.1069, 0.0377]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.2891e-03, 9.2138e-07, 2.2220e-03, 1.7685e-01, 3.5359e-02, 7.6637e-01,\n",
       "           1.3373e-02, 3.5212e-03, 6.4389e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1156, 0.0227, 0.1192, 0.1147, 0.1734, 0.1992, 0.1079, 0.1125, 0.0348]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.9055e-04, 3.5681e-08, 3.5296e-04, 6.6401e-02, 1.7277e-02, 9.1192e-01,\n",
       "           2.5999e-03, 1.1568e-03, 4.5870e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0923, 0.0104, 0.0904, 0.1401, 0.1626, 0.2781, 0.1101, 0.0914, 0.0246]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1119, 0.0166, 0.1344, 0.0987, 0.2165, 0.1741, 0.1091, 0.1057, 0.0330]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0905, 0.0119, 0.1102, 0.1103, 0.2200, 0.2196, 0.1201, 0.0980, 0.0195]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0974, 0.0174, 0.1123, 0.1072, 0.2046, 0.2004, 0.1306, 0.0987, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0977, 0.0253, 0.1059, 0.1207, 0.1875, 0.1938, 0.1097, 0.1193, 0.0401]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.4275e-05, 3.0579e-10, 1.2933e-05, 1.5366e-02, 1.3090e-03, 9.8279e-01,\n",
       "           4.5582e-04, 3.1928e-05, 5.5483e-09]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1012, 0.0064, 0.0806, 0.1239, 0.2296, 0.2703, 0.1006, 0.0756, 0.0118]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[3.0595e-02, 5.1301e-04, 2.4407e-02, 1.8251e-01, 1.0728e-01, 5.4869e-01,\n",
       "           7.7425e-02, 2.6856e-02, 1.7343e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0947, 0.0112, 0.1060, 0.1270, 0.2148, 0.2550, 0.1059, 0.0691, 0.0165]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0767, 0.0048, 0.0586, 0.1772, 0.1474, 0.3645, 0.0936, 0.0681, 0.0092]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.7302e-03, 1.2483e-05, 6.1699e-03, 1.2581e-01, 1.0991e-01, 7.2828e-01,\n",
       "           2.0612e-02, 5.4066e-03, 7.4213e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0926, 0.0286, 0.1057, 0.1244, 0.1925, 0.2050, 0.1274, 0.0901, 0.0337]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0476, 0.0011, 0.0357, 0.1753, 0.1493, 0.4538, 0.0736, 0.0596, 0.0040]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0615, 0.0030, 0.0590, 0.1332, 0.1977, 0.3816, 0.0933, 0.0634, 0.0075]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0745, 0.0038, 0.0582, 0.1569, 0.1946, 0.3484, 0.0922, 0.0655, 0.0060]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0393, 0.0008, 0.0313, 0.2029, 0.1268, 0.4891, 0.0557, 0.0510, 0.0033]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.8835e-05, 2.2652e-11, 1.2516e-05, 5.2833e-02, 1.7567e-03, 9.4496e-01,\n",
       "           3.6673e-04, 5.2643e-05, 2.9691e-10]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0285, 0.0005, 0.0215, 0.2284, 0.1090, 0.5140, 0.0655, 0.0308, 0.0017]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[4.4803e-03, 6.8495e-05, 5.3047e-03, 2.2135e-01, 6.2316e-02, 6.7630e-01,\n",
       "           1.9128e-02, 1.0881e-02, 1.7490e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0823, 0.0138, 0.0877, 0.1367, 0.1824, 0.2584, 0.1185, 0.0972, 0.0231]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1009, 0.0249, 0.1013, 0.1394, 0.1533, 0.2303, 0.1118, 0.1013, 0.0368]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0748, 0.0148, 0.0756, 0.1519, 0.1888, 0.2677, 0.1132, 0.0916, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0679, 0.0144, 0.0894, 0.1526, 0.1733, 0.2665, 0.1187, 0.0905, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[2.4722e-02, 5.6117e-04, 2.7263e-02, 1.6292e-01, 1.3630e-01, 5.7504e-01,\n",
       "           4.1107e-02, 3.0220e-02, 1.8711e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0790, 0.0100, 0.0847, 0.1648, 0.1829, 0.2798, 0.1026, 0.0837, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[2.1835e-02, 3.2765e-04, 1.8945e-02, 2.9743e-01, 9.6197e-02, 4.9233e-01,\n",
       "           4.5319e-02, 2.6783e-02, 8.3265e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0731, 0.0104, 0.0799, 0.1666, 0.1622, 0.3226, 0.0876, 0.0784, 0.0193]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[6.1646e-03, 4.6419e-05, 6.8653e-03, 2.6123e-01, 6.9545e-02, 6.2841e-01,\n",
       "           1.9535e-02, 8.0310e-03, 1.7041e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0602, 0.0071, 0.0730, 0.1485, 0.2006, 0.3394, 0.0886, 0.0674, 0.0150]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.8084e-02, 4.4121e-04, 3.1435e-02, 2.0229e-01, 1.5840e-01, 4.9253e-01,\n",
       "           5.1216e-02, 3.4356e-02, 1.2547e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0324, 0.0014, 0.0414, 0.1923, 0.1490, 0.4790, 0.0548, 0.0464, 0.0034]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.0323, 0.0014, 0.0271, 0.2367, 0.1668, 0.4393, 0.0581, 0.0350, 0.0032]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[3.8600e-03, 1.0620e-05, 4.4141e-03, 2.0484e-01, 5.5908e-02, 7.0997e-01,\n",
       "           1.2309e-02, 8.6411e-03, 4.3176e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[6.0688e-03, 6.5275e-05, 1.1643e-02, 1.7902e-01, 8.3932e-02, 6.6886e-01,\n",
       "           3.3561e-02, 1.6514e-02, 3.3803e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0412, 0.0015, 0.0502, 0.1829, 0.2399, 0.3693, 0.0632, 0.0467, 0.0053]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.2636e-02, 2.6852e-04, 1.2798e-02, 3.3461e-01, 1.2990e-01, 4.6446e-01,\n",
       "           2.1808e-02, 2.2880e-02, 6.3752e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.9411e-02, 3.5399e-04, 1.1088e-02, 2.2676e-01, 6.7245e-02, 6.0513e-01,\n",
       "           4.8413e-02, 2.0955e-02, 6.4775e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.8343e-03, 4.5441e-06, 3.7663e-03, 2.8216e-01, 2.1070e-02, 6.7757e-01,\n",
       "           9.7356e-03, 3.8480e-03, 9.9684e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0728, 0.0105, 0.0872, 0.1682, 0.1864, 0.2849, 0.0933, 0.0804, 0.0162]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0644, 0.0106, 0.0701, 0.1930, 0.1720, 0.2824, 0.0987, 0.0872, 0.0215]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.0478, 0.0026, 0.0502, 0.1787, 0.1921, 0.3732, 0.0896, 0.0601, 0.0057]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0566, 0.0056, 0.0639, 0.1912, 0.1543, 0.3559, 0.0910, 0.0718, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.0984e-02, 3.9326e-04, 2.9128e-02, 1.8745e-01, 1.5587e-01, 5.3281e-01,\n",
       "           5.0279e-02, 2.1983e-02, 1.0968e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0763, 0.0182, 0.0782, 0.1866, 0.1624, 0.2515, 0.1162, 0.0853, 0.0252]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[2.3975e-08, 8.5826e-17, 1.3355e-08, 3.6971e-02, 6.5444e-06, 9.6302e-01,\n",
       "           1.5335e-06, 7.1980e-08, 1.2645e-14]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0852, 0.0153, 0.0892, 0.1575, 0.1917, 0.2471, 0.1004, 0.0880, 0.0256]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[7.5030e-03, 6.3248e-05, 1.1026e-02, 1.8980e-01, 7.3889e-02, 6.6412e-01,\n",
       "           2.9785e-02, 2.3600e-02, 2.1140e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[1.1595e-04, 1.8085e-08, 9.5959e-05, 1.0947e-01, 1.1182e-02, 8.7767e-01,\n",
       "           1.2545e-03, 2.1418e-04, 6.6928e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0770, 0.0061, 0.0607, 0.1781, 0.1608, 0.3532, 0.0801, 0.0736, 0.0104]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[4.4726e-04, 1.4753e-07, 9.9070e-04, 1.6566e-01, 1.4706e-02, 8.0544e-01,\n",
       "           1.0052e-02, 2.6990e-03, 2.6216e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[6.3584e-03, 2.0956e-05, 6.4765e-03, 2.1117e-01, 7.9289e-02, 6.5709e-01,\n",
       "           2.8121e-02, 1.1342e-02, 1.3634e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[9.4993e-04, 1.4701e-07, 9.3432e-04, 2.1226e-01, 1.5681e-02, 7.6726e-01,\n",
       "           2.1420e-03, 7.7323e-04, 8.5606e-07]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0800, 0.0063, 0.1000, 0.1383, 0.2353, 0.2372, 0.0949, 0.0929, 0.0151]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1103, 0.0313, 0.1097, 0.1252, 0.1997, 0.1782, 0.1139, 0.0931, 0.0386]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0674, 0.0024, 0.0701, 0.1442, 0.2412, 0.3390, 0.0799, 0.0502, 0.0055]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1084, 0.0321, 0.1118, 0.1134, 0.1867, 0.1789, 0.1122, 0.1115, 0.0450]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[5.0090e-03, 1.0226e-05, 4.5788e-03, 1.3396e-01, 4.4911e-02, 7.8960e-01,\n",
       "           1.7192e-02, 4.6910e-03, 4.8637e-05]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1116, 0.0213, 0.1091, 0.1105, 0.2228, 0.1866, 0.1214, 0.0901, 0.0266]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[1.0156e-02, 1.5859e-05, 8.4047e-03, 2.3805e-01, 1.0142e-01, 6.0195e-01,\n",
       "           2.3603e-02, 1.6301e-02, 1.0082e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1142, 0.0407, 0.1081, 0.1129, 0.1899, 0.1655, 0.1123, 0.1082, 0.0481]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0969, 0.0257, 0.0933, 0.1391, 0.1658, 0.2187, 0.1135, 0.1085, 0.0385]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.6885e-02, 8.0813e-05, 1.1198e-02, 2.2169e-01, 1.4521e-01, 5.3291e-01,\n",
       "           5.2568e-02, 1.9218e-02, 2.4215e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[0.1308, 0.0453, 0.1123, 0.1042, 0.1874, 0.1430, 0.1096, 0.1059, 0.0614]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " (tensor([[3.7562e-02, 4.0708e-04, 2.5219e-02, 1.6415e-01, 1.0293e-01, 5.9702e-01,\n",
       "           4.4986e-02, 2.6374e-02, 1.3527e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[1.6135e-02, 1.1230e-04, 9.9799e-03, 1.9414e-01, 3.9284e-02, 6.9766e-01,\n",
       "           2.8173e-02, 1.4284e-02, 2.2789e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[8.7449e-05, 1.0089e-09, 6.0170e-05, 4.6628e-02, 3.7353e-03, 9.4876e-01,\n",
       "           5.8848e-04, 1.4144e-04, 6.8248e-08]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0990, 0.0210, 0.1035, 0.1240, 0.2146, 0.1947, 0.1051, 0.1018, 0.0364]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[2.0682e-02, 1.5268e-04, 1.6666e-02, 2.2732e-01, 8.4266e-02, 5.7524e-01,\n",
       "           3.9840e-02, 3.5054e-02, 7.7801e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1152, 0.0510, 0.1121, 0.1078, 0.1682, 0.1541, 0.1177, 0.1125, 0.0612]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0787, 0.0067, 0.0843, 0.1309, 0.2153, 0.2919, 0.1072, 0.0717, 0.0135]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.1095, 0.0217, 0.0939, 0.1220, 0.1810, 0.2245, 0.1176, 0.0974, 0.0325]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[1.6393e-08, 2.0252e-17, 1.0312e-08, 4.4779e-03, 6.8078e-06, 9.9551e-01,\n",
       "           4.5188e-06, 8.9195e-08, 7.5604e-15]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[7.9107e-04, 2.9215e-07, 1.0523e-03, 2.1543e-01, 3.6378e-02, 7.3537e-01,\n",
       "           9.8032e-03, 1.1776e-03, 2.0309e-06]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.1165, 0.0279, 0.1145, 0.1044, 0.1898, 0.1823, 0.1165, 0.1069, 0.0411]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1025, 0.0170, 0.0985, 0.1038, 0.2261, 0.2123, 0.1038, 0.1042, 0.0318]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0672, 0.0062, 0.0688, 0.1384, 0.1629, 0.3662, 0.1097, 0.0665, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1223, 0.0398, 0.1158, 0.1046, 0.1719, 0.1648, 0.1119, 0.1110, 0.0580]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0720, 0.0063, 0.0712, 0.1394, 0.1626, 0.3484, 0.1112, 0.0758, 0.0131]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " (tensor([[0.0550, 0.0016, 0.0497, 0.1734, 0.1453, 0.4565, 0.0694, 0.0455, 0.0036]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1102, 0.0470, 0.1072, 0.1123, 0.1582, 0.1661, 0.1210, 0.1175, 0.0604]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.1042, 0.0129, 0.1061, 0.1161, 0.1834, 0.2613, 0.0972, 0.0955, 0.0234]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " (tensor([[0.0495, 0.0010, 0.0336, 0.1784, 0.1146, 0.5267, 0.0573, 0.0349, 0.0041]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " (tensor([[0.0601, 0.0048, 0.0557, 0.1652, 0.1365, 0.4045, 0.0998, 0.0636, 0.0099]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " (tensor([[0.1153, 0.0128, 0.1055, 0.1125, 0.1849, 0.2492, 0.0963, 0.1032, 0.0203]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " ...]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn = GCN()\n",
    "# gcn.load_state_dict(torch.load('gcn_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 321 verilog files \n",
    "* only 3 features             [type, operation_type, num_of_connections]\n",
    "* no edge attribute\n",
    "* 18 classes \n",
    "* 200 epochs \n",
    "* learning rate = 0.01\n",
    "* Dropoout = 0.4\n",
    "* Adam Optimizer\n",
    "* train 70, test 30 (on whole dataset, not each class)\n",
    "* time of training = seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train acc:  0.2902\n",
    "* Test Acc: 0.1959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Clean dataset (by removing unnecessay, uninformative or wrong code files)\n",
    "2) remove reduntant parsing (different files but same parsing)\n",
    "3) include more informative features\n",
    "4) improve encoding format\n",
    "5) try using less classes (most important ones, so that less classes but more balanced dataset)\n",
    "6) adding more files\n",
    "7) adjusting hyperparameters such as learning rate, dropout, ...etc\n",
    "8) splitting train, val, test\n",
    "9) using equal percentages of each class (adjusting splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "droput 0.4\n",
    "\n",
    "314 files\n",
    "\n",
    "17 features (node_type)\n",
    "\n",
    "16 classes\n",
    "\n",
    "conv relu conv relu conv relu conv linear\n",
    "\n",
    "train = 40, test = 27\n",
    "\n",
    "200 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as 5 but 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = 43, test = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "conv relu conv relu conv dropout linear\n",
    "\n",
    "9 classes\n",
    "\n",
    "164 file\n",
    "\n",
    "train = 34, test = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "9 classes\n",
    "\n",
    "conv relu conv relu conv dropout linear \n",
    "\n",
    "train = 64, test = 52\n",
    "\n",
    "164 \n",
    "\n",
    "17 features (node type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
