{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "%pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%pip install torch_geometric -q\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "from torch_geometric.utils import to_dense_adj, add_self_loops\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DATA_PATH = \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n",
      "['done\\\\adder10_synth.txt', 'done\\\\adder11_synth.txt', 'done\\\\adder12_synth.txt', 'done\\\\adder13_synth.txt', 'done\\\\adder14_synth.txt', 'done\\\\adder15_synth.txt', 'done\\\\adder16_synth.txt', 'done\\\\adder17_synth.txt', 'done\\\\adder18_synth.txt', 'done\\\\adder19_synth.txt', 'done\\\\adder1_synth.txt', 'done\\\\adder20_synth.txt', 'done\\\\adder21_synth.txt', 'done\\\\adder22_synth.txt', 'done\\\\adder23_synth.txt', 'done\\\\adder24_synth.txt', 'done\\\\adder25_synth.txt', 'done\\\\adder26_synth.txt', 'done\\\\adder27_synth.txt', 'done\\\\adder28_synth.txt', 'done\\\\adder2_synth.txt', 'done\\\\adder3_synth.txt', 'done\\\\adder4_synth.txt', 'done\\\\adder5_synth.txt', 'done\\\\adder6_synth.txt', 'done\\\\adder7_synth.txt', 'done\\\\adder8_synth.txt', 'done\\\\adder9_synth.txt', 'done\\\\and10_gate_synth.txt', 'done\\\\and11_gate_synth.txt', 'done\\\\and12_gate_synth.txt', 'done\\\\and13_synth.txt', 'done\\\\and14_synth.txt', 'done\\\\and15_synth.txt', 'done\\\\and16_synth.txt', 'done\\\\and17_synth.txt', 'done\\\\and18_gate_synth.txt', 'done\\\\and19_synth.txt', 'done\\\\and1_synth.txt', 'done\\\\and20_synth.txt', 'done\\\\and21_synth.txt', 'done\\\\and22_synth.txt', 'done\\\\and23_synth.txt', 'done\\\\and24_synth.txt', 'done\\\\and25_synth.txt', 'done\\\\and26_synth.txt', 'done\\\\and27_synth.txt', 'done\\\\and28_synth.txt', 'done\\\\and29_synth.txt', 'done\\\\and2_gate_synth.txt', 'done\\\\and30_synth.txt', 'done\\\\and3_gate_synth.txt', 'done\\\\and4_gate_synth.txt', 'done\\\\and5_gate_synth.txt', 'done\\\\and6_gate_synth.txt', 'done\\\\and7_gate_synth.txt', 'done\\\\and8_gate_synth.txt', 'done\\\\and9_gate_synth.txt', 'done\\\\comparator10_synth.txt', 'done\\\\comparator11_synth.txt', 'done\\\\comparator12_synth.txt', 'done\\\\comparator13_synth.txt', 'done\\\\comparator14_synth.txt', 'done\\\\comparator15_synth.txt', 'done\\\\comparator16_synth.txt', 'done\\\\comparator17_synth.txt', 'done\\\\comparator18_synth.txt', 'done\\\\comparator19_synth.txt', 'done\\\\comparator1_synth.txt', 'done\\\\comparator20_synth.txt', 'done\\\\comparator21_synth.txt', 'done\\\\comparator22_synth.txt', 'done\\\\comparator2_synth.txt', 'done\\\\comparator3_synth.txt', 'done\\\\comparator4_synth.txt', 'done\\\\comparator5_synth.txt', 'done\\\\comparator6_synth.txt', 'done\\\\comparator7_synth.txt', 'done\\\\comparator8_synth.txt', 'done\\\\comparator9_synth.txt', 'done\\\\decoder10_synth.txt', 'done\\\\decoder11_synth.txt', 'done\\\\decoder12_synth.txt', 'done\\\\decoder13_synth.txt', 'done\\\\decoder14_synth.txt', 'done\\\\decoder15_synth.txt', 'done\\\\decoder16_synth.txt', 'done\\\\decoder17_synth.txt', 'done\\\\decoder18_synth.txt', 'done\\\\decoder19_synth.txt', 'done\\\\decoder1_synth.txt', 'done\\\\decoder20_synth.txt', 'done\\\\decoder21_synth.txt', 'done\\\\decoder22_synth.txt', 'done\\\\decoder23_synth.txt', 'done\\\\decoder24_synth.txt', 'done\\\\decoder25_synth.txt', 'done\\\\decoder26_synth.txt', 'done\\\\decoder27_synth.txt', 'done\\\\decoder28_synth.txt', 'done\\\\decoder29_synth.txt', 'done\\\\decoder2_synth.txt', 'done\\\\decoder30_synth.txt', 'done\\\\decoder31_synth.txt', 'done\\\\decoder32_synth.txt', 'done\\\\decoder3_synth.txt', 'done\\\\decoder4_synth.txt', 'done\\\\decoder5_synth.txt', 'done\\\\decoder6_synth.txt', 'done\\\\decoder7_synth.txt', 'done\\\\decoder8_synth.txt', 'done\\\\decoder9_synth.txt', 'done\\\\encoder10_synth.txt', 'done\\\\encoder11_synth.txt', 'done\\\\encoder12_synth.txt', 'done\\\\encoder13_synth.txt', 'done\\\\encoder14_synth.txt', 'done\\\\encoder15_synth.txt', 'done\\\\encoder16_synth.txt', 'done\\\\encoder17_synth.txt', 'done\\\\encoder18_synth.txt', 'done\\\\encoder19_synth.txt', 'done\\\\encoder1_synth.txt', 'done\\\\encoder20_synth.txt', 'done\\\\encoder21_synth.txt', 'done\\\\encoder22_synth.txt', 'done\\\\encoder23_synth.txt', 'done\\\\encoder24_synth.txt', 'done\\\\encoder25_synth.txt', 'done\\\\encoder2_synth.txt', 'done\\\\encoder3_synth.txt', 'done\\\\encoder4_synth.txt', 'done\\\\encoder5_synth.txt', 'done\\\\encoder6_synth.txt', 'done\\\\encoder7_synth.txt', 'done\\\\encoder8_synth.txt', 'done\\\\encoder9_synth.txt', 'done\\\\mult10_synth.txt', 'done\\\\mult11_synth.txt', 'done\\\\mult12_synth.txt', 'done\\\\mult13_synth.txt', 'done\\\\mult14_synth.txt', 'done\\\\mult15_synth.txt', 'done\\\\mult16_synth.txt', 'done\\\\mult17_synth.txt', 'done\\\\mult18_synth.txt', 'done\\\\mult19_synth.txt', 'done\\\\mult1_synth.txt', 'done\\\\mult20_synth.txt', 'done\\\\mult21_synth.txt', 'done\\\\mult22_synth.txt', 'done\\\\mult23_synth.txt', 'done\\\\mult25_synth.txt', 'done\\\\mult26_synth.txt', 'done\\\\mult27_synth.txt', 'done\\\\mult28_synth.txt', 'done\\\\mult29_synth.txt', 'done\\\\mult2_synth.txt', 'done\\\\mult30_synth.txt', 'done\\\\mult3_synth.txt', 'done\\\\mult4_synth.txt', 'done\\\\mult5_synth.txt', 'done\\\\mult6_synth.txt', 'done\\\\mult7_synth.txt', 'done\\\\mult8_synth.txt', 'done\\\\mult9_synth.txt', 'done\\\\mux10_synth.txt', 'done\\\\mux11_synth.txt', 'done\\\\mux12_synth.txt', 'done\\\\mux13_synth.txt', 'done\\\\mux15_synth.txt', 'done\\\\mux16_synth.txt', 'done\\\\mux17_synth.txt', 'done\\\\mux18_synth.txt', 'done\\\\mux19_synth.txt', 'done\\\\mux1_synth.txt', 'done\\\\mux20_synth.txt', 'done\\\\mux21_synth.txt', 'done\\\\mux22_synth.txt', 'done\\\\mux23_synth.txt', 'done\\\\mux24_synth.txt', 'done\\\\mux25_synth.txt', 'done\\\\mux26_synth.txt', 'done\\\\mux27_synth.txt', 'done\\\\mux28_synth.txt', 'done\\\\mux2_synth.txt', 'done\\\\mux3_synth.txt', 'done\\\\mux4_synth.txt', 'done\\\\mux5_synth.txt', 'done\\\\mux6_synth.txt', 'done\\\\mux7_synth.txt', 'done\\\\mux8_synth.txt', 'done\\\\mux9_synth.txt', 'done\\\\nand10_synth.txt', 'done\\\\nand11_synth.txt', 'done\\\\nand12_gate_synth.txt', 'done\\\\nand13_synth.txt', 'done\\\\nand14_synth.txt', 'done\\\\nand15_synth.txt', 'done\\\\nand16_synth.txt', 'done\\\\nand17_synth.txt', 'done\\\\nand18_synth.txt', 'done\\\\nand19_synth.txt', 'done\\\\nand1_synth.txt', 'done\\\\nand20_gate_synth.txt', 'done\\\\nand21_synth.txt', 'done\\\\nand22_synth.txt', 'done\\\\nand23_synth.txt', 'done\\\\nand24_synth.txt', 'done\\\\nand25_synth.txt', 'done\\\\nand26_synth.txt', 'done\\\\nand27_synth.txt', 'done\\\\nand28_synth.txt', 'done\\\\nand29_synth.txt', 'done\\\\nand2_gate_synth.txt', 'done\\\\nand30_synth.txt', 'done\\\\nand31_synth.txt', 'done\\\\nand3_gate_synth.txt', 'done\\\\nand4_gate_synth.txt', 'done\\\\nand5_gate_synth.txt', 'done\\\\nand6_gate_synth.txt', 'done\\\\nand7_gate_synth.txt', 'done\\\\nand8_gate_synth.txt', 'done\\\\nand9_gate_synth.txt', 'done\\\\nor10_synth.txt', 'done\\\\nor11_synth.txt', 'done\\\\nor12_gate_synth.txt', 'done\\\\nor13_synth.txt', 'done\\\\nor14_synth.txt', 'done\\\\nor15_synth.txt', 'done\\\\nor16_synth.txt', 'done\\\\nor17_synth.txt', 'done\\\\nor18_synth.txt', 'done\\\\nor19_synth.txt', 'done\\\\nor1_synth.txt', 'done\\\\nor20_synth.txt', 'done\\\\nor21_synth.txt', 'done\\\\nor22_synth.txt', 'done\\\\nor23_synth.txt', 'done\\\\nor24_synth.txt', 'done\\\\nor25_synth.txt', 'done\\\\nor26_synth.txt', 'done\\\\nor27_gate_synth.txt', 'done\\\\nor28_synth.txt', 'done\\\\nor29_synth.txt', 'done\\\\nor2_gate_synth.txt', 'done\\\\nor30_synth.txt', 'done\\\\nor3_gate_synth.txt', 'done\\\\nor4_gate_synth.txt', 'done\\\\nor5_gate_synth.txt', 'done\\\\nor6_gate_synth.txt', 'done\\\\nor7_synth.txt', 'done\\\\nor8_gate_synth.txt', 'done\\\\nor9_synth.txt', 'done\\\\not10_synth.txt', 'done\\\\not11_synth.txt', 'done\\\\not12_synth.txt', 'done\\\\not13_synth.txt', 'done\\\\not14_synth.txt', 'done\\\\not15_synth.txt', 'done\\\\not16_synth.txt', 'done\\\\not1_synth.txt', 'done\\\\not2_synth.txt', 'done\\\\not3_synth.txt', 'done\\\\not4_synth.txt', 'done\\\\not5_synth.txt', 'done\\\\not6_synth.txt', 'done\\\\not7_synth.txt', 'done\\\\not8_synth.txt', 'done\\\\not9_synth.txt', 'done\\\\or11_synth.txt', 'done\\\\or13_synth.txt', 'done\\\\or14_synth.txt', 'done\\\\or15_synth.txt', 'done\\\\or16_synth.txt', 'done\\\\or17_gate_synth.txt', 'done\\\\or18_synth.txt', 'done\\\\or19_synth.txt', 'done\\\\or1_synth.txt', 'done\\\\or20_synth.txt', 'done\\\\or21_synth.txt', 'done\\\\or22_synth.txt', 'done\\\\or23_synth.txt', 'done\\\\or24_synth.txt', 'done\\\\or25_synth.txt', 'done\\\\or26_synth.txt', 'done\\\\or27_synth.txt', 'done\\\\or28_synth.txt', 'done\\\\or29_synth.txt', 'done\\\\or2_gate_synth.txt', 'done\\\\or3_gate_synth.txt', 'done\\\\or4_gate_synth.txt', 'done\\\\or5_gate_synth.txt', 'done\\\\or6_gate_synth.txt', 'done\\\\or7_synth.txt', 'done\\\\or8_gate_synth.txt', 'done\\\\or9_synth.txt', 'done\\\\pe10_synth.txt', 'done\\\\pe11_synth.txt', 'done\\\\pe12_synth.txt', 'done\\\\pe13_synth.txt', 'done\\\\pe14_synth.txt', 'done\\\\pe15_synth.txt', 'done\\\\pe16_synth.txt', 'done\\\\pe17_synth.txt', 'done\\\\pe18_synth.txt', 'done\\\\pe19_synth.txt', 'done\\\\pe1_synth.txt', 'done\\\\pe20_synth.txt', 'done\\\\pe21_synth.txt', 'done\\\\pe22_synth.txt', 'done\\\\pe2_synth.txt', 'done\\\\pe3_synth.txt', 'done\\\\pe4_synth.txt', 'done\\\\pe5_synth.txt', 'done\\\\pe6_synth.txt', 'done\\\\pe8_synth.txt', 'done\\\\pe9_synth.txt', 'done\\\\sub10_synth.txt', 'done\\\\sub11_synth.txt', 'done\\\\sub12_synth.txt', 'done\\\\sub1_synth.txt', 'done\\\\sub2_synth.txt', 'done\\\\sub4_synth.txt', 'done\\\\sub5_synth.txt', 'done\\\\sub6_synth.txt', 'done\\\\sub7_synth.txt', 'done\\\\sub8_synth.txt', 'done\\\\sub9_synth.txt', 'done\\\\xnor10_synth.txt', 'done\\\\xnor11_synth.txt', 'done\\\\xnor12_synth.txt', 'done\\\\xnor13_synth.txt', 'done\\\\xnor14_synth.txt', 'done\\\\xnor15_synth.txt', 'done\\\\xnor16_synth.txt', 'done\\\\xnor17_synth.txt', 'done\\\\xnor18_synth.txt', 'done\\\\xnor19_synth.txt', 'done\\\\xnor1_synth.txt', 'done\\\\xnor20_synth.txt', 'done\\\\xnor21_synth.txt', 'done\\\\xnor22_synth.txt', 'done\\\\xnor23_synth.txt', 'done\\\\xnor24_synth.txt', 'done\\\\xnor25_synth.txt', 'done\\\\xnor26_synth.txt', 'done\\\\xnor27_synth.txt', 'done\\\\xnor28_synth.txt', 'done\\\\xnor29_synth.txt', 'done\\\\xnor2_synth.txt', 'done\\\\xnor30_synth.txt', 'done\\\\xnor3_synth.txt', 'done\\\\xnor4_synth.txt', 'done\\\\xnor5_synth.txt', 'done\\\\xnor6_synth.txt', 'done\\\\xnor7_synth.txt', 'done\\\\xnor8_synth.txt', 'done\\\\xnor9_synth.txt', 'done\\\\xor10_synth.txt', 'done\\\\xor11_synth.txt', 'done\\\\xor12_synth.txt', 'done\\\\xor13_synth.txt', 'done\\\\xor14_synth.txt', 'done\\\\xor15_synth.txt', 'done\\\\xor16_synth.txt', 'done\\\\xor17_synth.txt', 'done\\\\xor18_synth.txt', 'done\\\\xor19_synth.txt', 'done\\\\xor1_synth.txt', 'done\\\\xor20_synth.txt', 'done\\\\xor21_synth.txt', 'done\\\\xor22_synth.txt', 'done\\\\xor23_synth.txt', 'done\\\\xor24_synth.txt', 'done\\\\xor25_synth.txt', 'done\\\\xor26_synth.txt', 'done\\\\xor27_synth.txt', 'done\\\\xor28_synth.txt', 'done\\\\xor29_synth.txt', 'done\\\\xor2_synth.txt', 'done\\\\xor3_synth.txt', 'done\\\\xor4_synth.txt', 'done\\\\xor5_synth.txt', 'done\\\\xor6_synth.txt', 'done\\\\xor7_synth.txt', 'done\\\\xor8_synth.txt']\n"
     ]
    }
   ],
   "source": [
    "def get_files_in_folder(input_folder):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'done'\n",
    "verilog_files = get_files_in_folder(folder_path)\n",
    "print(len(verilog_files))\n",
    "print(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                label = loaded_data[2]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                num_nodes = x.size(0)\n",
    "                \n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "# temp=extracting_attributes(\"./done/adder6.txt\")\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 387 Verilog files.\n",
      "387\n"
     ]
    }
   ],
   "source": [
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[54, 7], edge_index=[2, 72], y=[1, 15], batch=[54])\n",
      "done\\adder10_synth.txt\n",
      "done\\adder10_synth.txt\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(verilog_files[0])\n",
    "print(dataset.verilog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data objects are unique.\n"
     ]
    }
   ],
   "source": [
    "def are_all_data_objects_unique(dataset):\n",
    "    data_objects = []\n",
    "    for data in dataset:\n",
    "        if data in data_objects:\n",
    "            return False\n",
    "        data_objects.append(data)\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "is_unique = are_all_data_objects_unique(dataset)\n",
    "if is_unique:\n",
    "    print(\"All data objects are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate data objects found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done\\\\sub8_synth.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = random.randint(0, len(verilog_files))\n",
    "verilog_files[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset[num]\n",
    "data\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(range(data.num_nodes))\n",
    "edges = data.edge_index.t().tolist()\n",
    "# edge_attrs = {tuple(edge): attr.item() for edge, attr in zip(edges, data.edge_attr)}\n",
    "g.add_edges_from(edges)\n",
    "print(len(edges))\n",
    "\n",
    "# Draw the graph with edge attributes\n",
    "pos = nx.spring_layout(g)  # positions for all nodes\n",
    "nx.draw(g, pos, with_labels=True, node_color='skyblue', node_size=1500, edge_color='k', linewidths=1, font_size=15)\n",
    "nx.draw_networkx_edge_labels(g, pos, font_color='red', font_size=12)  # Add edge labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK THIS TANYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = []\n",
    "for data in dataset:\n",
    "    # print(data)\n",
    "    # print(data.y.tolist())\n",
    "    y_labels.append(np.argmax(data.y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y_labels, test_size=0.2, stratify = y_labels, random_state=41)\n",
    "train_loader = DataLoader(X_train, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(X_test, batch_size=16, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[57, 7], edge_index=[2, 69], y=[1, 15], batch=[57])\n"
     ]
    }
   ],
   "source": [
    "# len(train_loader.dataset)\n",
    "print(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "batch = next(loader_iter)\n",
    "# print(batch)\n",
    "# print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gcn1): GCNConv(7, 64)\n",
       "  (r1): ReLU()\n",
       "  (gcn2): GCNConv(64, 128)\n",
       "  (linear): Linear(in_features=128, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool, global_sort_pool\n",
    "from torch_geometric.nn import GCNConv, JumpingKnowledge, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GCN, self).__init__()\n",
    "        \n",
    "#         num_node_features = 7\n",
    "#         num_output_classes = 15\n",
    "        \n",
    "#         self.conv1 = GCNConv(num_node_features, 64)\n",
    "#         self.conv2 = GCNConv(64, 64)\n",
    "        \n",
    "#         self.jk = JumpingKnowledge(mode='cat')\n",
    "        \n",
    "#         self.conv3 = GCNConv(128, 128)\n",
    "#         self.linear = nn.Linear(128, num_output_classes)  # Adjust the input size based on JK mode\n",
    "        \n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x1 = F.relu(self.conv1(x, edge_index))\n",
    "#         x2 = F.relu(self.conv2(x1, edge_index))\n",
    "#         # x3 = F.relu(self.conv3(x2, edge_index))\n",
    "        \n",
    "#         # x = self.jk([x1, x2])\n",
    "#         x = F.dropout(x, p=0.2, training=self.training)\n",
    "#         x = self.conv3(x, edge_index)\n",
    "        \n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         x = global_mean_pool(x, batch)\n",
    "        \n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.linear(x)\n",
    "        \n",
    "#         probs = F.softmax(x, dim=-1)\n",
    "        \n",
    "#         return probs\n",
    "        \n",
    "        \n",
    "        \n",
    "        # KNN\n",
    "        # embeddings\n",
    "        # PCA\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        num_node_features = 7\n",
    "        num_output_classes = 15\n",
    "        \n",
    "        # num_channels = 32\n",
    "        \n",
    "        self.gcn1 = GCNConv(num_node_features, 64)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.gcn2 = GCNConv(64, 128)\n",
    "        # self.r2 = nn.ReLU()\n",
    "        # self.gcn3 = GCNConv(64, 128)\n",
    "        # self.r3 = nn.ReLU()\n",
    "        # self.gcn4 = GCNConv(128, 128)\n",
    "        self.linear = nn.Linear(in_features=128, out_features=num_output_classes)\n",
    "        self.embeddings = []\n",
    "    def forward(self, x, edge_index, batch):\n",
    "    \n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.r1(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        self.embeddings = x\n",
    "        # x = self.r2(x)\n",
    "        # x = self.gcn3(x, edge_index)\n",
    "        \n",
    "        # x = self.r3(x)\n",
    "        # x = self.gcn4(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p = 0.6, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        probs = F.softmax(x, dim=-1)\n",
    "        \n",
    "        return probs, self.embeddings\n",
    "    \n",
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.2039, Test Acc: 0.2051\n",
      "Epoch: 002, Train Acc: 0.1877, Test Acc: 0.1795\n",
      "Epoch: 003, Train Acc: 0.2362, Test Acc: 0.2308\n",
      "Epoch: 004, Train Acc: 0.2589, Test Acc: 0.2692\n",
      "Epoch: 005, Train Acc: 0.2427, Test Acc: 0.2308\n",
      "Epoch: 006, Train Acc: 0.3074, Test Acc: 0.2564\n",
      "Epoch: 007, Train Acc: 0.3172, Test Acc: 0.2179\n",
      "Epoch: 008, Train Acc: 0.3204, Test Acc: 0.2308\n",
      "Epoch: 009, Train Acc: 0.3463, Test Acc: 0.2308\n",
      "Epoch: 010, Train Acc: 0.3333, Test Acc: 0.2308\n",
      "Epoch: 011, Train Acc: 0.3236, Test Acc: 0.2821\n",
      "Epoch: 012, Train Acc: 0.3916, Test Acc: 0.3333\n",
      "Epoch: 013, Train Acc: 0.3948, Test Acc: 0.2949\n",
      "Epoch: 014, Train Acc: 0.3560, Test Acc: 0.2821\n",
      "Epoch: 015, Train Acc: 0.4304, Test Acc: 0.3205\n",
      "Epoch: 016, Train Acc: 0.4531, Test Acc: 0.3718\n",
      "Epoch: 017, Train Acc: 0.4822, Test Acc: 0.4103\n",
      "Epoch: 018, Train Acc: 0.4725, Test Acc: 0.4359\n",
      "Epoch: 019, Train Acc: 0.4757, Test Acc: 0.3846\n",
      "Epoch: 020, Train Acc: 0.4919, Test Acc: 0.4231\n",
      "Epoch: 021, Train Acc: 0.4951, Test Acc: 0.4359\n",
      "Epoch: 022, Train Acc: 0.5081, Test Acc: 0.4359\n",
      "Epoch: 023, Train Acc: 0.4854, Test Acc: 0.4103\n",
      "Epoch: 024, Train Acc: 0.5049, Test Acc: 0.5000\n",
      "Epoch: 025, Train Acc: 0.5049, Test Acc: 0.4744\n",
      "Epoch: 026, Train Acc: 0.5081, Test Acc: 0.4615\n",
      "Epoch: 027, Train Acc: 0.5081, Test Acc: 0.4872\n",
      "Epoch: 028, Train Acc: 0.5016, Test Acc: 0.4615\n",
      "Epoch: 029, Train Acc: 0.4822, Test Acc: 0.5000\n",
      "Epoch: 030, Train Acc: 0.4984, Test Acc: 0.5000\n",
      "Epoch: 031, Train Acc: 0.5081, Test Acc: 0.4872\n",
      "Epoch: 032, Train Acc: 0.5113, Test Acc: 0.5000\n",
      "Epoch: 033, Train Acc: 0.5016, Test Acc: 0.4744\n",
      "Epoch: 034, Train Acc: 0.5146, Test Acc: 0.4744\n",
      "Epoch: 035, Train Acc: 0.5307, Test Acc: 0.5128\n",
      "Epoch: 036, Train Acc: 0.5307, Test Acc: 0.4872\n",
      "Epoch: 037, Train Acc: 0.5307, Test Acc: 0.5128\n",
      "Epoch: 038, Train Acc: 0.5307, Test Acc: 0.5000\n",
      "Epoch: 039, Train Acc: 0.5081, Test Acc: 0.5256\n",
      "Epoch: 040, Train Acc: 0.5081, Test Acc: 0.5128\n",
      "Epoch: 041, Train Acc: 0.5049, Test Acc: 0.5128\n",
      "Epoch: 042, Train Acc: 0.5113, Test Acc: 0.4744\n",
      "Epoch: 043, Train Acc: 0.5113, Test Acc: 0.5000\n",
      "Epoch: 044, Train Acc: 0.5178, Test Acc: 0.4615\n",
      "Epoch: 045, Train Acc: 0.5113, Test Acc: 0.5128\n",
      "Epoch: 046, Train Acc: 0.5275, Test Acc: 0.5128\n",
      "Epoch: 047, Train Acc: 0.5016, Test Acc: 0.5000\n",
      "Epoch: 048, Train Acc: 0.5275, Test Acc: 0.5000\n",
      "Epoch: 049, Train Acc: 0.5340, Test Acc: 0.5000\n",
      "Epoch: 050, Train Acc: 0.5340, Test Acc: 0.5000\n",
      "Epoch: 051, Train Acc: 0.5372, Test Acc: 0.5256\n",
      "Epoch: 052, Train Acc: 0.5275, Test Acc: 0.5000\n",
      "Epoch: 053, Train Acc: 0.5243, Test Acc: 0.5128\n",
      "Epoch: 054, Train Acc: 0.5372, Test Acc: 0.5000\n",
      "Epoch: 055, Train Acc: 0.5243, Test Acc: 0.5128\n",
      "Epoch: 056, Train Acc: 0.5405, Test Acc: 0.5000\n",
      "Epoch: 057, Train Acc: 0.5469, Test Acc: 0.5000\n",
      "Epoch: 058, Train Acc: 0.5113, Test Acc: 0.5000\n",
      "Epoch: 059, Train Acc: 0.5275, Test Acc: 0.5256\n",
      "Epoch: 060, Train Acc: 0.5307, Test Acc: 0.5128\n",
      "Epoch: 061, Train Acc: 0.5340, Test Acc: 0.4872\n",
      "Epoch: 062, Train Acc: 0.5307, Test Acc: 0.4744\n",
      "Epoch: 063, Train Acc: 0.5372, Test Acc: 0.5000\n",
      "Epoch: 064, Train Acc: 0.5113, Test Acc: 0.4615\n",
      "Epoch: 065, Train Acc: 0.5275, Test Acc: 0.5256\n",
      "Epoch: 066, Train Acc: 0.5372, Test Acc: 0.5000\n",
      "Epoch: 067, Train Acc: 0.5307, Test Acc: 0.5000\n",
      "Epoch: 068, Train Acc: 0.5469, Test Acc: 0.5385\n",
      "Epoch: 069, Train Acc: 0.5372, Test Acc: 0.4872\n",
      "Epoch: 070, Train Acc: 0.5437, Test Acc: 0.5128\n",
      "Epoch: 071, Train Acc: 0.5340, Test Acc: 0.5128\n",
      "Epoch: 072, Train Acc: 0.5340, Test Acc: 0.5256\n",
      "Epoch: 073, Train Acc: 0.5210, Test Acc: 0.5128\n",
      "Epoch: 074, Train Acc: 0.5307, Test Acc: 0.5000\n",
      "Epoch: 075, Train Acc: 0.5372, Test Acc: 0.5256\n",
      "Epoch: 076, Train Acc: 0.5405, Test Acc: 0.5000\n",
      "Epoch: 077, Train Acc: 0.5469, Test Acc: 0.5256\n",
      "Epoch: 078, Train Acc: 0.5340, Test Acc: 0.5000\n",
      "Epoch: 079, Train Acc: 0.5340, Test Acc: 0.5000\n",
      "Epoch: 080, Train Acc: 0.5340, Test Acc: 0.5128\n",
      "Epoch: 081, Train Acc: 0.5437, Test Acc: 0.5256\n",
      "Epoch: 082, Train Acc: 0.5210, Test Acc: 0.5128\n",
      "Epoch: 083, Train Acc: 0.5502, Test Acc: 0.5256\n",
      "Epoch: 084, Train Acc: 0.5502, Test Acc: 0.5256\n",
      "Epoch: 085, Train Acc: 0.5372, Test Acc: 0.5000\n",
      "Epoch: 086, Train Acc: 0.5372, Test Acc: 0.5128\n",
      "Epoch: 087, Train Acc: 0.5340, Test Acc: 0.5128\n",
      "Epoch: 088, Train Acc: 0.5405, Test Acc: 0.5256\n",
      "Epoch: 089, Train Acc: 0.5437, Test Acc: 0.5385\n",
      "Epoch: 090, Train Acc: 0.5372, Test Acc: 0.5256\n",
      "Epoch: 091, Train Acc: 0.5340, Test Acc: 0.5128\n",
      "Epoch: 092, Train Acc: 0.5340, Test Acc: 0.5000\n",
      "Epoch: 093, Train Acc: 0.5372, Test Acc: 0.5000\n",
      "Epoch: 094, Train Acc: 0.5340, Test Acc: 0.5000\n",
      "Epoch: 095, Train Acc: 0.5405, Test Acc: 0.5000\n",
      "Epoch: 096, Train Acc: 0.5469, Test Acc: 0.5385\n",
      "Epoch: 097, Train Acc: 0.5372, Test Acc: 0.4872\n",
      "Epoch: 098, Train Acc: 0.5437, Test Acc: 0.5000\n",
      "Epoch: 099, Train Acc: 0.5469, Test Acc: 0.5000\n",
      "Epoch: 100, Train Acc: 0.5502, Test Acc: 0.5256\n",
      "Training duration: 373.19285559654236 seconds\n"
     ]
    }
   ],
   "source": [
    "out_prob = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gcn = GCN()\n",
    "gcn = gcn.to(device)\n",
    "# print(gcn.parameters())\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "# loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "out_labels = []\n",
    "\n",
    "# training_running_loss = 0.0\n",
    "global embeddings\n",
    "embeddings = []\n",
    "\n",
    "def train(train_loader):\n",
    "    \n",
    "    gcn.train()\n",
    "    # print(gcn.parameters())\n",
    "    for batch_data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        for data in batch_data:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            #forward pass\n",
    "            out, embeddings = gcn(data.x, data.edge_index, data.batch)\n",
    "            out_labels.append(out)\n",
    "            # print(out)\n",
    "            # calculate the loss\n",
    "            loss = criterion(out, data.y)\n",
    "            # zero the gradients of the weights so that the gradients are not accumulated\n",
    "            # calculate the gradients using backpropagation\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss\n",
    "            # training_running_loss += loss.detach().item()\n",
    "            \n",
    "            out_labels.append((out, data.y))\n",
    "        out_prob.append(out)\n",
    "        \n",
    "        \n",
    "\n",
    "testing_labels = []\n",
    "def test(loader):\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    for batch_data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        for data in batch_data:\n",
    "            out, _ = gcn(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            testing_labels.append(pred)\n",
    "            y_label = (data.y.tolist())\n",
    "            y_label = y_label[0].index(1.0)\n",
    "            pred_label = (pred.tolist())[0]\n",
    "            # print(pred_label)\n",
    "            # print(y_label)\n",
    "            if y_label == pred_label:\n",
    "                correct += 1            \n",
    "            # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 100\n",
    "# Your training code here\n",
    "for epoch in range(num_epochs):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch + 1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(\"Training duration:\", duration, \"seconds\")\n",
    "# with open(\"out_labels.txt\", \"w\") as output:\n",
    "#         output.write(str(out_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0610, 0.0609, 0.0730, 0.0788, 0.0735, 0.0757, 0.0534, 0.0705, 0.0718,\n",
       "          0.0557, 0.0578, 0.0669, 0.0585, 0.0766, 0.0660]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0610, 0.0609, 0.0730, 0.0788, 0.0735, 0.0757, 0.0534, 0.0705, 0.0718,\n",
       "           0.0557, 0.0578, 0.0669, 0.0585, 0.0766, 0.0660]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0581, 0.0768, 0.0746, 0.0700, 0.0805, 0.0720, 0.0584, 0.0674, 0.0596,\n",
       "          0.0603, 0.0577, 0.0622, 0.0707, 0.0670, 0.0646]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0581, 0.0768, 0.0746, 0.0700, 0.0805, 0.0720, 0.0584, 0.0674, 0.0596,\n",
       "           0.0603, 0.0577, 0.0622, 0.0707, 0.0670, 0.0646]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0634, 0.0758, 0.0669, 0.0638, 0.0643, 0.0798, 0.0591, 0.0650, 0.0597,\n",
       "          0.0584, 0.0603, 0.0685, 0.0723, 0.0764, 0.0663]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0634, 0.0758, 0.0669, 0.0638, 0.0643, 0.0798, 0.0591, 0.0650, 0.0597,\n",
       "           0.0584, 0.0603, 0.0685, 0.0723, 0.0764, 0.0663]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0463, 0.0735, 0.0734, 0.0723, 0.0739, 0.0680, 0.0668, 0.0657, 0.0641,\n",
       "          0.0604, 0.0640, 0.0669, 0.0710, 0.0666, 0.0671]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0463, 0.0735, 0.0734, 0.0723, 0.0739, 0.0680, 0.0668, 0.0657, 0.0641,\n",
       "           0.0604, 0.0640, 0.0669, 0.0710, 0.0666, 0.0671]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0562, 0.0696, 0.0680, 0.0647, 0.0769, 0.0778, 0.0624, 0.0663, 0.0650,\n",
       "          0.0556, 0.0577, 0.0664, 0.0690, 0.0745, 0.0698]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0562, 0.0696, 0.0680, 0.0647, 0.0769, 0.0778, 0.0624, 0.0663, 0.0650,\n",
       "           0.0556, 0.0577, 0.0664, 0.0690, 0.0745, 0.0698]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0534, 0.0740, 0.0715, 0.0797, 0.0821, 0.0698, 0.0655, 0.0538, 0.0632,\n",
       "          0.0603, 0.0611, 0.0634, 0.0716, 0.0682, 0.0624]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0534, 0.0740, 0.0715, 0.0797, 0.0821, 0.0698, 0.0655, 0.0538, 0.0632,\n",
       "           0.0603, 0.0611, 0.0634, 0.0716, 0.0682, 0.0624]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0550, 0.0733, 0.0783, 0.0687, 0.0761, 0.0653, 0.0674, 0.0594, 0.0622,\n",
       "          0.0718, 0.0610, 0.0592, 0.0621, 0.0724, 0.0677]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0550, 0.0733, 0.0783, 0.0687, 0.0761, 0.0653, 0.0674, 0.0594, 0.0622,\n",
       "           0.0718, 0.0610, 0.0592, 0.0621, 0.0724, 0.0677]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0617, 0.0804, 0.0676, 0.0750, 0.0796, 0.0690, 0.0541, 0.0640, 0.0663,\n",
       "          0.0720, 0.0563, 0.0553, 0.0670, 0.0728, 0.0587]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0617, 0.0804, 0.0676, 0.0750, 0.0796, 0.0690, 0.0541, 0.0640, 0.0663,\n",
       "           0.0720, 0.0563, 0.0553, 0.0670, 0.0728, 0.0587]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0562, 0.0627, 0.0711, 0.0729, 0.0895, 0.0668, 0.0636, 0.0647, 0.0604,\n",
       "          0.0548, 0.0694, 0.0739, 0.0656, 0.0669, 0.0617]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0562, 0.0627, 0.0711, 0.0729, 0.0895, 0.0668, 0.0636, 0.0647, 0.0604,\n",
       "           0.0548, 0.0694, 0.0739, 0.0656, 0.0669, 0.0617]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0659, 0.0807, 0.0704, 0.0711, 0.0747, 0.0671, 0.0632, 0.0604, 0.0592,\n",
       "          0.0567, 0.0634, 0.0694, 0.0616, 0.0728, 0.0634]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0659, 0.0807, 0.0704, 0.0711, 0.0747, 0.0671, 0.0632, 0.0604, 0.0592,\n",
       "           0.0567, 0.0634, 0.0694, 0.0616, 0.0728, 0.0634]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0613, 0.0674, 0.0727, 0.0830, 0.0779, 0.0604, 0.0623, 0.0654, 0.0582,\n",
       "          0.0658, 0.0574, 0.0595, 0.0634, 0.0799, 0.0655]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0613, 0.0674, 0.0727, 0.0830, 0.0779, 0.0604, 0.0623, 0.0654, 0.0582,\n",
       "           0.0658, 0.0574, 0.0595, 0.0634, 0.0799, 0.0655]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0579, 0.0695, 0.0737, 0.0749, 0.0764, 0.0647, 0.0650, 0.0594, 0.0708,\n",
       "          0.0677, 0.0571, 0.0704, 0.0685, 0.0612, 0.0628]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0579, 0.0695, 0.0737, 0.0749, 0.0764, 0.0647, 0.0650, 0.0594, 0.0708,\n",
       "           0.0677, 0.0571, 0.0704, 0.0685, 0.0612, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0585, 0.0713, 0.0670, 0.0745, 0.0755, 0.0701, 0.0676, 0.0549, 0.0757,\n",
       "          0.0610, 0.0599, 0.0699, 0.0613, 0.0697, 0.0630]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0585, 0.0713, 0.0670, 0.0745, 0.0755, 0.0701, 0.0676, 0.0549, 0.0757,\n",
       "           0.0610, 0.0599, 0.0699, 0.0613, 0.0697, 0.0630]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0660, 0.0765, 0.0717, 0.0839, 0.0784, 0.0694, 0.0701, 0.0573, 0.0659,\n",
       "          0.0581, 0.0526, 0.0644, 0.0585, 0.0656, 0.0615]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0660, 0.0765, 0.0717, 0.0839, 0.0784, 0.0694, 0.0701, 0.0573, 0.0659,\n",
       "           0.0581, 0.0526, 0.0644, 0.0585, 0.0656, 0.0615]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0638, 0.0666, 0.0725, 0.0760, 0.0760, 0.0685, 0.0666, 0.0599, 0.0621,\n",
       "          0.0590, 0.0640, 0.0659, 0.0688, 0.0659, 0.0643]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0638, 0.0666, 0.0725, 0.0760, 0.0760, 0.0685, 0.0666, 0.0599, 0.0621,\n",
       "           0.0590, 0.0640, 0.0659, 0.0688, 0.0659, 0.0643]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0563, 0.0866, 0.0607, 0.0852, 0.0783, 0.0649, 0.0651, 0.0641, 0.0517,\n",
       "          0.0738, 0.0580, 0.0645, 0.0623, 0.0688, 0.0597]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0563, 0.0866, 0.0607, 0.0852, 0.0783, 0.0649, 0.0651, 0.0641, 0.0517,\n",
       "           0.0738, 0.0580, 0.0645, 0.0623, 0.0688, 0.0597]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0658, 0.0665, 0.0666, 0.0831, 0.0854, 0.0638, 0.0686, 0.0530, 0.0619,\n",
       "          0.0598, 0.0517, 0.0798, 0.0696, 0.0657, 0.0588]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0658, 0.0665, 0.0666, 0.0831, 0.0854, 0.0638, 0.0686, 0.0530, 0.0619,\n",
       "           0.0598, 0.0517, 0.0798, 0.0696, 0.0657, 0.0588]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0661, 0.0683, 0.0647, 0.0773, 0.0676, 0.0732, 0.0741, 0.0613, 0.0643,\n",
       "          0.0599, 0.0540, 0.0580, 0.0754, 0.0695, 0.0664]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0661, 0.0683, 0.0647, 0.0773, 0.0676, 0.0732, 0.0741, 0.0613, 0.0643,\n",
       "           0.0599, 0.0540, 0.0580, 0.0754, 0.0695, 0.0664]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0586, 0.0698, 0.0636, 0.0768, 0.0792, 0.0662, 0.0673, 0.0687, 0.0536,\n",
       "          0.0654, 0.0556, 0.0662, 0.0616, 0.0831, 0.0645]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0586, 0.0698, 0.0636, 0.0768, 0.0792, 0.0662, 0.0673, 0.0687, 0.0536,\n",
       "           0.0654, 0.0556, 0.0662, 0.0616, 0.0831, 0.0645]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0720, 0.0638, 0.0723, 0.0847, 0.0794, 0.0686, 0.0725, 0.0623, 0.0537,\n",
       "          0.0547, 0.0611, 0.0637, 0.0602, 0.0649, 0.0661]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0720, 0.0638, 0.0723, 0.0847, 0.0794, 0.0686, 0.0725, 0.0623, 0.0537,\n",
       "           0.0547, 0.0611, 0.0637, 0.0602, 0.0649, 0.0661]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0629, 0.0629, 0.0785, 0.0878, 0.0838, 0.0721, 0.0739, 0.0660, 0.0664,\n",
       "          0.0534, 0.0556, 0.0622, 0.0593, 0.0566, 0.0586]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0629, 0.0629, 0.0785, 0.0878, 0.0838, 0.0721, 0.0739, 0.0660, 0.0664,\n",
       "           0.0534, 0.0556, 0.0622, 0.0593, 0.0566, 0.0586]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0736, 0.0669, 0.0700, 0.0739, 0.0852, 0.0696, 0.0680, 0.0632, 0.0688,\n",
       "          0.0598, 0.0506, 0.0634, 0.0530, 0.0697, 0.0641]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0736, 0.0669, 0.0700, 0.0739, 0.0852, 0.0696, 0.0680, 0.0632, 0.0688,\n",
       "           0.0598, 0.0506, 0.0634, 0.0530, 0.0697, 0.0641]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0670, 0.0650, 0.0679, 0.0824, 0.0876, 0.0643, 0.0808, 0.0602, 0.0620,\n",
       "          0.0635, 0.0554, 0.0670, 0.0532, 0.0667, 0.0570]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0670, 0.0650, 0.0679, 0.0824, 0.0876, 0.0643, 0.0808, 0.0602, 0.0620,\n",
       "           0.0635, 0.0554, 0.0670, 0.0532, 0.0667, 0.0570]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0542, 0.0647, 0.0667, 0.0855, 0.0879, 0.0639, 0.0765, 0.0595, 0.0589,\n",
       "          0.0581, 0.0586, 0.0702, 0.0696, 0.0629, 0.0629]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0542, 0.0647, 0.0667, 0.0855, 0.0879, 0.0639, 0.0765, 0.0595, 0.0589,\n",
       "           0.0581, 0.0586, 0.0702, 0.0696, 0.0629, 0.0629]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0630, 0.0601, 0.0740, 0.0911, 0.0753, 0.0643, 0.0831, 0.0593, 0.0594,\n",
       "          0.0564, 0.0571, 0.0721, 0.0599, 0.0664, 0.0585]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0630, 0.0601, 0.0740, 0.0911, 0.0753, 0.0643, 0.0831, 0.0593, 0.0594,\n",
       "           0.0564, 0.0571, 0.0721, 0.0599, 0.0664, 0.0585]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0625, 0.0680, 0.0643, 0.0794, 0.0763, 0.0767, 0.0732, 0.0589, 0.0683,\n",
       "          0.0619, 0.0555, 0.0625, 0.0671, 0.0642, 0.0612]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0625, 0.0680, 0.0643, 0.0794, 0.0763, 0.0767, 0.0732, 0.0589, 0.0683,\n",
       "           0.0619, 0.0555, 0.0625, 0.0671, 0.0642, 0.0612]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0677, 0.0686, 0.0624, 0.0814, 0.0865, 0.0653, 0.0774, 0.0623, 0.0553,\n",
       "          0.0676, 0.0550, 0.0661, 0.0573, 0.0638, 0.0635]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0677, 0.0686, 0.0624, 0.0814, 0.0865, 0.0653, 0.0774, 0.0623, 0.0553,\n",
       "           0.0676, 0.0550, 0.0661, 0.0573, 0.0638, 0.0635]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0588, 0.0702, 0.0762, 0.0945, 0.0921, 0.0673, 0.0822, 0.0554, 0.0628,\n",
       "          0.0635, 0.0520, 0.0651, 0.0561, 0.0550, 0.0488]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0588, 0.0702, 0.0762, 0.0945, 0.0921, 0.0673, 0.0822, 0.0554, 0.0628,\n",
       "           0.0635, 0.0520, 0.0651, 0.0561, 0.0550, 0.0488]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0654, 0.0661, 0.0790, 0.0786, 0.0842, 0.0621, 0.0682, 0.0543, 0.0626,\n",
       "          0.0644, 0.0556, 0.0704, 0.0634, 0.0657, 0.0599]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0654, 0.0661, 0.0790, 0.0786, 0.0842, 0.0621, 0.0682, 0.0543, 0.0626,\n",
       "           0.0644, 0.0556, 0.0704, 0.0634, 0.0657, 0.0599]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0658, 0.0677, 0.0852, 0.0822, 0.0803, 0.0600, 0.0795, 0.0472, 0.0691,\n",
       "          0.0550, 0.0557, 0.0790, 0.0614, 0.0587, 0.0532]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0658, 0.0677, 0.0852, 0.0822, 0.0803, 0.0600, 0.0795, 0.0472, 0.0691,\n",
       "           0.0550, 0.0557, 0.0790, 0.0614, 0.0587, 0.0532]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0665, 0.0666, 0.0676, 0.0764, 0.0761, 0.0746, 0.0799, 0.0582, 0.0630,\n",
       "          0.0599, 0.0518, 0.0643, 0.0640, 0.0664, 0.0647]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0665, 0.0666, 0.0676, 0.0764, 0.0761, 0.0746, 0.0799, 0.0582, 0.0630,\n",
       "           0.0599, 0.0518, 0.0643, 0.0640, 0.0664, 0.0647]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0595, 0.0704, 0.0713, 0.0791, 0.0825, 0.0646, 0.0803, 0.0536, 0.0609,\n",
       "          0.0714, 0.0525, 0.0719, 0.0578, 0.0657, 0.0582]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0595, 0.0704, 0.0713, 0.0791, 0.0825, 0.0646, 0.0803, 0.0536, 0.0609,\n",
       "           0.0714, 0.0525, 0.0719, 0.0578, 0.0657, 0.0582]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0695, 0.0670, 0.0728, 0.0880, 0.0787, 0.0715, 0.0778, 0.0517, 0.0644,\n",
       "          0.0649, 0.0490, 0.0648, 0.0563, 0.0678, 0.0559]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0695, 0.0670, 0.0728, 0.0880, 0.0787, 0.0715, 0.0778, 0.0517, 0.0644,\n",
       "           0.0649, 0.0490, 0.0648, 0.0563, 0.0678, 0.0559]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0517, 0.0570, 0.0843, 0.0900, 0.0911, 0.0694, 0.0703, 0.0596, 0.0639,\n",
       "          0.0632, 0.0527, 0.0618, 0.0686, 0.0627, 0.0535]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0517, 0.0570, 0.0843, 0.0900, 0.0911, 0.0694, 0.0703, 0.0596, 0.0639,\n",
       "           0.0632, 0.0527, 0.0618, 0.0686, 0.0627, 0.0535]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0658, 0.0658, 0.0734, 0.0775, 0.0778, 0.0691, 0.0757, 0.0541, 0.0585,\n",
       "          0.0686, 0.0565, 0.0628, 0.0716, 0.0639, 0.0588]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0658, 0.0658, 0.0734, 0.0775, 0.0778, 0.0691, 0.0757, 0.0541, 0.0585,\n",
       "           0.0686, 0.0565, 0.0628, 0.0716, 0.0639, 0.0588]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0670, 0.0617, 0.0673, 0.0932, 0.0776, 0.0626, 0.0709, 0.0546, 0.0545,\n",
       "          0.0655, 0.0542, 0.0723, 0.0677, 0.0694, 0.0616]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0670, 0.0617, 0.0673, 0.0932, 0.0776, 0.0626, 0.0709, 0.0546, 0.0545,\n",
       "           0.0655, 0.0542, 0.0723, 0.0677, 0.0694, 0.0616]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0713, 0.0682, 0.0650, 0.0711, 0.0800, 0.0691, 0.0848, 0.0501, 0.0569,\n",
       "          0.0719, 0.0582, 0.0619, 0.0679, 0.0648, 0.0587]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0713, 0.0682, 0.0650, 0.0711, 0.0800, 0.0691, 0.0848, 0.0501, 0.0569,\n",
       "           0.0719, 0.0582, 0.0619, 0.0679, 0.0648, 0.0587]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0670, 0.0672, 0.0654, 0.0973, 0.0827, 0.0654, 0.0754, 0.0486, 0.0614,\n",
       "          0.0654, 0.0495, 0.0711, 0.0664, 0.0621, 0.0551]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0670, 0.0672, 0.0654, 0.0973, 0.0827, 0.0654, 0.0754, 0.0486, 0.0614,\n",
       "           0.0654, 0.0495, 0.0711, 0.0664, 0.0621, 0.0551]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0592, 0.0615, 0.0761, 0.0758, 0.0742, 0.0718, 0.0883, 0.0539, 0.0607,\n",
       "          0.0718, 0.0536, 0.0605, 0.0699, 0.0614, 0.0611]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0592, 0.0615, 0.0761, 0.0758, 0.0742, 0.0718, 0.0883, 0.0539, 0.0607,\n",
       "           0.0718, 0.0536, 0.0605, 0.0699, 0.0614, 0.0611]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0594, 0.0602, 0.0837, 0.0773, 0.0939, 0.0690, 0.0789, 0.0520, 0.0545,\n",
       "          0.0709, 0.0514, 0.0748, 0.0574, 0.0637, 0.0529]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0594, 0.0602, 0.0837, 0.0773, 0.0939, 0.0690, 0.0789, 0.0520, 0.0545,\n",
       "           0.0709, 0.0514, 0.0748, 0.0574, 0.0637, 0.0529]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0639, 0.0606, 0.0728, 0.0820, 0.0817, 0.0707, 0.0773, 0.0542, 0.0596,\n",
       "          0.0667, 0.0517, 0.0623, 0.0681, 0.0687, 0.0597]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0639, 0.0606, 0.0728, 0.0820, 0.0817, 0.0707, 0.0773, 0.0542, 0.0596,\n",
       "           0.0667, 0.0517, 0.0623, 0.0681, 0.0687, 0.0597]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0638, 0.0653, 0.0724, 0.0846, 0.0913, 0.0641, 0.0711, 0.0625, 0.0573,\n",
       "          0.0615, 0.0534, 0.0667, 0.0562, 0.0668, 0.0630]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0638, 0.0653, 0.0724, 0.0846, 0.0913, 0.0641, 0.0711, 0.0625, 0.0573,\n",
       "           0.0615, 0.0534, 0.0667, 0.0562, 0.0668, 0.0630]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0623, 0.0613, 0.0813, 0.0908, 0.0845, 0.0741, 0.0784, 0.0479, 0.0664,\n",
       "          0.0596, 0.0442, 0.0664, 0.0604, 0.0694, 0.0528]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0623, 0.0613, 0.0813, 0.0908, 0.0845, 0.0741, 0.0784, 0.0479, 0.0664,\n",
       "           0.0596, 0.0442, 0.0664, 0.0604, 0.0694, 0.0528]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0591, 0.0671, 0.0870, 0.0959, 0.0797, 0.0790, 0.0768, 0.0452, 0.0611,\n",
       "          0.0632, 0.0446, 0.0776, 0.0653, 0.0559, 0.0424]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0591, 0.0671, 0.0870, 0.0959, 0.0797, 0.0790, 0.0768, 0.0452, 0.0611,\n",
       "           0.0632, 0.0446, 0.0776, 0.0653, 0.0559, 0.0424]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0581, 0.0632, 0.0739, 0.0805, 0.0844, 0.0662, 0.0955, 0.0525, 0.0524,\n",
       "          0.0680, 0.0546, 0.0724, 0.0644, 0.0564, 0.0576]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0581, 0.0632, 0.0739, 0.0805, 0.0844, 0.0662, 0.0955, 0.0525, 0.0524,\n",
       "           0.0680, 0.0546, 0.0724, 0.0644, 0.0564, 0.0576]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0682, 0.0556, 0.0744, 0.0885, 0.0864, 0.0683, 0.0825, 0.0470, 0.0615,\n",
       "          0.0648, 0.0490, 0.0682, 0.0611, 0.0668, 0.0577]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0682, 0.0556, 0.0744, 0.0885, 0.0864, 0.0683, 0.0825, 0.0470, 0.0615,\n",
       "           0.0648, 0.0490, 0.0682, 0.0611, 0.0668, 0.0577]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0635, 0.0660, 0.0677, 0.0811, 0.0849, 0.0730, 0.0716, 0.0531, 0.0658,\n",
       "          0.0714, 0.0482, 0.0645, 0.0558, 0.0746, 0.0588]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0635, 0.0660, 0.0677, 0.0811, 0.0849, 0.0730, 0.0716, 0.0531, 0.0658,\n",
       "           0.0714, 0.0482, 0.0645, 0.0558, 0.0746, 0.0588]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0569, 0.0552, 0.0712, 0.0937, 0.0876, 0.0710, 0.0946, 0.0481, 0.0593,\n",
       "          0.0683, 0.0485, 0.0681, 0.0570, 0.0640, 0.0565]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0569, 0.0552, 0.0712, 0.0937, 0.0876, 0.0710, 0.0946, 0.0481, 0.0593,\n",
       "           0.0683, 0.0485, 0.0681, 0.0570, 0.0640, 0.0565]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0625, 0.0699, 0.0693, 0.1005, 0.0831, 0.0682, 0.0791, 0.0502, 0.0664,\n",
       "          0.0644, 0.0448, 0.0715, 0.0574, 0.0620, 0.0507]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0625, 0.0699, 0.0693, 0.1005, 0.0831, 0.0682, 0.0791, 0.0502, 0.0664,\n",
       "           0.0644, 0.0448, 0.0715, 0.0574, 0.0620, 0.0507]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0663, 0.0647, 0.0760, 0.0837, 0.0799, 0.0716, 0.0674, 0.0550, 0.0589,\n",
       "          0.0649, 0.0494, 0.0688, 0.0611, 0.0667, 0.0657]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0663, 0.0647, 0.0760, 0.0837, 0.0799, 0.0716, 0.0674, 0.0550, 0.0589,\n",
       "           0.0649, 0.0494, 0.0688, 0.0611, 0.0667, 0.0657]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0525, 0.0631, 0.0847, 0.1001, 0.0903, 0.0612, 0.0866, 0.0440, 0.0689,\n",
       "          0.0719, 0.0364, 0.0726, 0.0505, 0.0698, 0.0474]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0525, 0.0631, 0.0847, 0.1001, 0.0903, 0.0612, 0.0866, 0.0440, 0.0689,\n",
       "           0.0719, 0.0364, 0.0726, 0.0505, 0.0698, 0.0474]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0662, 0.0583, 0.0762, 0.0907, 0.0720, 0.0679, 0.0801, 0.0479, 0.0670,\n",
       "          0.0677, 0.0445, 0.0683, 0.0556, 0.0768, 0.0607]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0662, 0.0583, 0.0762, 0.0907, 0.0720, 0.0679, 0.0801, 0.0479, 0.0670,\n",
       "           0.0677, 0.0445, 0.0683, 0.0556, 0.0768, 0.0607]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0658, 0.0564, 0.0754, 0.1040, 0.0741, 0.0771, 0.0782, 0.0505, 0.0648,\n",
       "          0.0640, 0.0463, 0.0651, 0.0545, 0.0672, 0.0567]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0658, 0.0564, 0.0754, 0.1040, 0.0741, 0.0771, 0.0782, 0.0505, 0.0648,\n",
       "           0.0640, 0.0463, 0.0651, 0.0545, 0.0672, 0.0567]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0594, 0.0611, 0.0899, 0.0844, 0.0726, 0.0646, 0.0747, 0.0525, 0.0569,\n",
       "          0.0650, 0.0493, 0.0788, 0.0682, 0.0655, 0.0570]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0594, 0.0611, 0.0899, 0.0844, 0.0726, 0.0646, 0.0747, 0.0525, 0.0569,\n",
       "           0.0650, 0.0493, 0.0788, 0.0682, 0.0655, 0.0570]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0540, 0.0547, 0.0932, 0.0977, 0.0825, 0.0692, 0.0779, 0.0475, 0.0523,\n",
       "          0.0655, 0.0468, 0.0817, 0.0595, 0.0651, 0.0524]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0540, 0.0547, 0.0932, 0.0977, 0.0825, 0.0692, 0.0779, 0.0475, 0.0523,\n",
       "           0.0655, 0.0468, 0.0817, 0.0595, 0.0651, 0.0524]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0657, 0.0633, 0.0731, 0.0958, 0.0702, 0.0625, 0.0764, 0.0476, 0.0628,\n",
       "          0.0637, 0.0469, 0.0765, 0.0556, 0.0734, 0.0662]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0657, 0.0633, 0.0731, 0.0958, 0.0702, 0.0625, 0.0764, 0.0476, 0.0628,\n",
       "           0.0637, 0.0469, 0.0765, 0.0556, 0.0734, 0.0662]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0641, 0.0618, 0.0713, 0.1014, 0.0732, 0.0699, 0.0716, 0.0506, 0.0616,\n",
       "          0.0665, 0.0446, 0.0708, 0.0591, 0.0715, 0.0620]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0641, 0.0618, 0.0713, 0.1014, 0.0732, 0.0699, 0.0716, 0.0506, 0.0616,\n",
       "           0.0665, 0.0446, 0.0708, 0.0591, 0.0715, 0.0620]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0627, 0.0569, 0.0666, 0.1049, 0.0859, 0.0653, 0.0848, 0.0450, 0.0596,\n",
       "          0.0660, 0.0403, 0.0805, 0.0557, 0.0695, 0.0563]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0627, 0.0569, 0.0666, 0.1049, 0.0859, 0.0653, 0.0848, 0.0450, 0.0596,\n",
       "           0.0660, 0.0403, 0.0805, 0.0557, 0.0695, 0.0563]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0566, 0.0576, 0.0805, 0.0899, 0.0829, 0.0643, 0.0733, 0.0544, 0.0558,\n",
       "          0.0680, 0.0481, 0.0737, 0.0641, 0.0681, 0.0629]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0566, 0.0576, 0.0805, 0.0899, 0.0829, 0.0643, 0.0733, 0.0544, 0.0558,\n",
       "           0.0680, 0.0481, 0.0737, 0.0641, 0.0681, 0.0629]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0616, 0.0549, 0.0762, 0.0952, 0.0845, 0.0740, 0.0671, 0.0541, 0.0627,\n",
       "          0.0549, 0.0455, 0.0774, 0.0629, 0.0687, 0.0602]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0616, 0.0549, 0.0762, 0.0952, 0.0845, 0.0740, 0.0671, 0.0541, 0.0627,\n",
       "           0.0549, 0.0455, 0.0774, 0.0629, 0.0687, 0.0602]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0605, 0.0613, 0.0693, 0.0973, 0.0924, 0.0590, 0.0716, 0.0496, 0.0523,\n",
       "          0.0675, 0.0547, 0.0803, 0.0560, 0.0628, 0.0653]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0605, 0.0613, 0.0693, 0.0973, 0.0924, 0.0590, 0.0716, 0.0496, 0.0523,\n",
       "           0.0675, 0.0547, 0.0803, 0.0560, 0.0628, 0.0653]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0664, 0.0544, 0.0744, 0.0860, 0.0771, 0.0709, 0.0659, 0.0564, 0.0526,\n",
       "          0.0672, 0.0526, 0.0773, 0.0626, 0.0703, 0.0660]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0664, 0.0544, 0.0744, 0.0860, 0.0771, 0.0709, 0.0659, 0.0564, 0.0526,\n",
       "           0.0672, 0.0526, 0.0773, 0.0626, 0.0703, 0.0660]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0624, 0.0528, 0.0663, 0.1036, 0.0880, 0.0668, 0.0666, 0.0535, 0.0589,\n",
       "          0.0538, 0.0444, 0.0867, 0.0577, 0.0731, 0.0653]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0624, 0.0528, 0.0663, 0.1036, 0.0880, 0.0668, 0.0666, 0.0535, 0.0589,\n",
       "           0.0538, 0.0444, 0.0867, 0.0577, 0.0731, 0.0653]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0535, 0.0496, 0.0918, 0.0812, 0.0970, 0.0653, 0.0668, 0.0564, 0.0503,\n",
       "          0.0643, 0.0537, 0.0797, 0.0626, 0.0624, 0.0655]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0535, 0.0496, 0.0918, 0.0812, 0.0970, 0.0653, 0.0668, 0.0564, 0.0503,\n",
       "           0.0643, 0.0537, 0.0797, 0.0626, 0.0624, 0.0655]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0693, 0.0486, 0.0756, 0.1017, 0.0731, 0.0677, 0.0765, 0.0423, 0.0543,\n",
       "          0.0639, 0.0445, 0.0888, 0.0668, 0.0681, 0.0590]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0693, 0.0486, 0.0756, 0.1017, 0.0731, 0.0677, 0.0765, 0.0423, 0.0543,\n",
       "           0.0639, 0.0445, 0.0888, 0.0668, 0.0681, 0.0590]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0665, 0.0482, 0.0733, 0.0997, 0.0863, 0.0688, 0.0745, 0.0521, 0.0526,\n",
       "          0.0566, 0.0518, 0.0808, 0.0635, 0.0607, 0.0647]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0665, 0.0482, 0.0733, 0.0997, 0.0863, 0.0688, 0.0745, 0.0521, 0.0526,\n",
       "           0.0566, 0.0518, 0.0808, 0.0635, 0.0607, 0.0647]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0595, 0.0568, 0.0802, 0.0923, 0.0982, 0.0689, 0.0719, 0.0512, 0.0536,\n",
       "          0.0624, 0.0481, 0.0671, 0.0647, 0.0637, 0.0613]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0595, 0.0568, 0.0802, 0.0923, 0.0982, 0.0689, 0.0719, 0.0512, 0.0536,\n",
       "           0.0624, 0.0481, 0.0671, 0.0647, 0.0637, 0.0613]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0573, 0.0567, 0.0831, 0.0901, 0.0831, 0.0676, 0.0783, 0.0494, 0.0504,\n",
       "          0.0611, 0.0498, 0.0784, 0.0598, 0.0685, 0.0663]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0573, 0.0567, 0.0831, 0.0901, 0.0831, 0.0676, 0.0783, 0.0494, 0.0504,\n",
       "           0.0611, 0.0498, 0.0784, 0.0598, 0.0685, 0.0663]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0658, 0.0492, 0.0755, 0.1015, 0.1005, 0.0598, 0.0841, 0.0402, 0.0441,\n",
       "          0.0574, 0.0491, 0.0737, 0.0692, 0.0631, 0.0668]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0658, 0.0492, 0.0755, 0.1015, 0.1005, 0.0598, 0.0841, 0.0402, 0.0441,\n",
       "           0.0574, 0.0491, 0.0737, 0.0692, 0.0631, 0.0668]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0627, 0.0536, 0.0806, 0.0980, 0.0953, 0.0635, 0.0694, 0.0489, 0.0538,\n",
       "          0.0646, 0.0499, 0.0810, 0.0570, 0.0632, 0.0585]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0627, 0.0536, 0.0806, 0.0980, 0.0953, 0.0635, 0.0694, 0.0489, 0.0538,\n",
       "           0.0646, 0.0499, 0.0810, 0.0570, 0.0632, 0.0585]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0696, 0.0532, 0.0608, 0.0948, 0.1060, 0.0630, 0.0660, 0.0502, 0.0602,\n",
       "          0.0544, 0.0395, 0.0834, 0.0657, 0.0678, 0.0653]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0696, 0.0532, 0.0608, 0.0948, 0.1060, 0.0630, 0.0660, 0.0502, 0.0602,\n",
       "           0.0544, 0.0395, 0.0834, 0.0657, 0.0678, 0.0653]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0675, 0.0454, 0.0703, 0.0933, 0.1218, 0.0559, 0.0783, 0.0383, 0.0443,\n",
       "          0.0534, 0.0481, 0.0902, 0.0561, 0.0612, 0.0761]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0675, 0.0454, 0.0703, 0.0933, 0.1218, 0.0559, 0.0783, 0.0383, 0.0443,\n",
       "           0.0534, 0.0481, 0.0902, 0.0561, 0.0612, 0.0761]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0677, 0.0596, 0.0697, 0.0993, 0.0781, 0.0594, 0.0713, 0.0523, 0.0541,\n",
       "          0.0683, 0.0443, 0.0825, 0.0564, 0.0680, 0.0689]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0677, 0.0596, 0.0697, 0.0993, 0.0781, 0.0594, 0.0713, 0.0523, 0.0541,\n",
       "           0.0683, 0.0443, 0.0825, 0.0564, 0.0680, 0.0689]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0616, 0.0556, 0.0782, 0.0983, 0.0841, 0.0582, 0.0757, 0.0453, 0.0483,\n",
       "          0.0729, 0.0496, 0.0784, 0.0701, 0.0671, 0.0567]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0616, 0.0556, 0.0782, 0.0983, 0.0841, 0.0582, 0.0757, 0.0453, 0.0483,\n",
       "           0.0729, 0.0496, 0.0784, 0.0701, 0.0671, 0.0567]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0624, 0.0462, 0.0783, 0.0910, 0.1283, 0.0647, 0.0695, 0.0524, 0.0554,\n",
       "          0.0505, 0.0501, 0.0694, 0.0571, 0.0542, 0.0707]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0624, 0.0462, 0.0783, 0.0910, 0.1283, 0.0647, 0.0695, 0.0524, 0.0554,\n",
       "           0.0505, 0.0501, 0.0694, 0.0571, 0.0542, 0.0707]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0688, 0.0529, 0.0786, 0.1113, 0.0956, 0.0681, 0.0723, 0.0376, 0.0508,\n",
       "          0.0524, 0.0464, 0.0787, 0.0549, 0.0585, 0.0731]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0688, 0.0529, 0.0786, 0.1113, 0.0956, 0.0681, 0.0723, 0.0376, 0.0508,\n",
       "           0.0524, 0.0464, 0.0787, 0.0549, 0.0585, 0.0731]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0596, 0.0547, 0.0822, 0.0897, 0.1018, 0.0619, 0.0673, 0.0516, 0.0489,\n",
       "          0.0628, 0.0561, 0.0710, 0.0592, 0.0658, 0.0674]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0596, 0.0547, 0.0822, 0.0897, 0.1018, 0.0619, 0.0673, 0.0516, 0.0489,\n",
       "           0.0628, 0.0561, 0.0710, 0.0592, 0.0658, 0.0674]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0651, 0.0468, 0.0662, 0.1198, 0.0793, 0.0660, 0.0745, 0.0471, 0.0507,\n",
       "          0.0557, 0.0459, 0.0802, 0.0637, 0.0633, 0.0756]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0651, 0.0468, 0.0662, 0.1198, 0.0793, 0.0660, 0.0745, 0.0471, 0.0507,\n",
       "           0.0557, 0.0459, 0.0802, 0.0637, 0.0633, 0.0756]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0689, 0.0459, 0.0675, 0.1085, 0.0969, 0.0686, 0.0790, 0.0411, 0.0613,\n",
       "          0.0513, 0.0383, 0.0885, 0.0549, 0.0667, 0.0626]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0689, 0.0459, 0.0675, 0.1085, 0.0969, 0.0686, 0.0790, 0.0411, 0.0613,\n",
       "           0.0513, 0.0383, 0.0885, 0.0549, 0.0667, 0.0626]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0681, 0.0448, 0.0754, 0.1023, 0.0810, 0.0604, 0.0781, 0.0423, 0.0440,\n",
       "          0.0589, 0.0453, 0.0884, 0.0604, 0.0714, 0.0793]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0681, 0.0448, 0.0754, 0.1023, 0.0810, 0.0604, 0.0781, 0.0423, 0.0440,\n",
       "           0.0589, 0.0453, 0.0884, 0.0604, 0.0714, 0.0793]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0595, 0.0444, 0.0725, 0.1113, 0.1055, 0.0647, 0.0784, 0.0431, 0.0457,\n",
       "          0.0509, 0.0447, 0.0798, 0.0603, 0.0640, 0.0753]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0595, 0.0444, 0.0725, 0.1113, 0.1055, 0.0647, 0.0784, 0.0431, 0.0457,\n",
       "           0.0509, 0.0447, 0.0798, 0.0603, 0.0640, 0.0753]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0641, 0.0557, 0.0728, 0.0980, 0.0942, 0.0759, 0.0735, 0.0483, 0.0475,\n",
       "          0.0610, 0.0518, 0.0672, 0.0634, 0.0580, 0.0686]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0641, 0.0557, 0.0728, 0.0980, 0.0942, 0.0759, 0.0735, 0.0483, 0.0475,\n",
       "           0.0610, 0.0518, 0.0672, 0.0634, 0.0580, 0.0686]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0674, 0.0468, 0.0676, 0.1062, 0.0812, 0.0781, 0.0938, 0.0390, 0.0418,\n",
       "          0.0509, 0.0431, 0.0826, 0.0640, 0.0599, 0.0776]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0674, 0.0468, 0.0676, 0.1062, 0.0812, 0.0781, 0.0938, 0.0390, 0.0418,\n",
       "           0.0509, 0.0431, 0.0826, 0.0640, 0.0599, 0.0776]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0689, 0.0489, 0.0868, 0.0964, 0.0765, 0.0660, 0.0688, 0.0465, 0.0506,\n",
       "          0.0571, 0.0465, 0.0912, 0.0593, 0.0685, 0.0679]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0689, 0.0489, 0.0868, 0.0964, 0.0765, 0.0660, 0.0688, 0.0465, 0.0506,\n",
       "           0.0571, 0.0465, 0.0912, 0.0593, 0.0685, 0.0679]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0704, 0.0480, 0.0681, 0.0955, 0.0886, 0.0848, 0.0682, 0.0489, 0.0496,\n",
       "          0.0554, 0.0423, 0.0783, 0.0595, 0.0639, 0.0786]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0704, 0.0480, 0.0681, 0.0955, 0.0886, 0.0848, 0.0682, 0.0489, 0.0496,\n",
       "           0.0554, 0.0423, 0.0783, 0.0595, 0.0639, 0.0786]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0695, 0.0473, 0.0751, 0.1115, 0.1004, 0.0578, 0.0721, 0.0420, 0.0477,\n",
       "          0.0530, 0.0430, 0.1036, 0.0440, 0.0681, 0.0649]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0695, 0.0473, 0.0751, 0.1115, 0.1004, 0.0578, 0.0721, 0.0420, 0.0477,\n",
       "           0.0530, 0.0430, 0.1036, 0.0440, 0.0681, 0.0649]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0626, 0.0504, 0.0867, 0.0951, 0.1050, 0.0655, 0.0697, 0.0481, 0.0435,\n",
       "          0.0594, 0.0459, 0.0801, 0.0587, 0.0589, 0.0704]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0626, 0.0504, 0.0867, 0.0951, 0.1050, 0.0655, 0.0697, 0.0481, 0.0435,\n",
       "           0.0594, 0.0459, 0.0801, 0.0587, 0.0589, 0.0704]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0698, 0.0554, 0.0868, 0.0968, 0.1018, 0.0664, 0.0641, 0.0508, 0.0523,\n",
       "          0.0574, 0.0437, 0.0692, 0.0558, 0.0608, 0.0690]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0698, 0.0554, 0.0868, 0.0968, 0.1018, 0.0664, 0.0641, 0.0508, 0.0523,\n",
       "           0.0574, 0.0437, 0.0692, 0.0558, 0.0608, 0.0690]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0655, 0.0465, 0.0928, 0.0997, 0.1066, 0.0733, 0.0750, 0.0447, 0.0420,\n",
       "          0.0541, 0.0452, 0.0636, 0.0627, 0.0578, 0.0705]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0655, 0.0465, 0.0928, 0.0997, 0.1066, 0.0733, 0.0750, 0.0447, 0.0420,\n",
       "           0.0541, 0.0452, 0.0636, 0.0627, 0.0578, 0.0705]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0675, 0.0482, 0.0750, 0.0895, 0.0796, 0.0753, 0.0830, 0.0518, 0.0497,\n",
       "          0.0576, 0.0517, 0.0804, 0.0623, 0.0578, 0.0706]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0675, 0.0482, 0.0750, 0.0895, 0.0796, 0.0753, 0.0830, 0.0518, 0.0497,\n",
       "           0.0576, 0.0517, 0.0804, 0.0623, 0.0578, 0.0706]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0694, 0.0457, 0.0722, 0.0929, 0.0828, 0.0788, 0.0729, 0.0493, 0.0483,\n",
       "          0.0606, 0.0420, 0.0813, 0.0609, 0.0627, 0.0804]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0694, 0.0457, 0.0722, 0.0929, 0.0828, 0.0788, 0.0729, 0.0493, 0.0483,\n",
       "           0.0606, 0.0420, 0.0813, 0.0609, 0.0627, 0.0804]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0692, 0.0418, 0.0709, 0.0902, 0.0881, 0.0917, 0.0775, 0.0392, 0.0454,\n",
       "          0.0648, 0.0385, 0.0833, 0.0596, 0.0706, 0.0694]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0692, 0.0418, 0.0709, 0.0902, 0.0881, 0.0917, 0.0775, 0.0392, 0.0454,\n",
       "           0.0648, 0.0385, 0.0833, 0.0596, 0.0706, 0.0694]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0718, 0.0450, 0.0849, 0.0874, 0.0811, 0.0777, 0.0737, 0.0501, 0.0501,\n",
       "          0.0559, 0.0435, 0.0712, 0.0644, 0.0673, 0.0759]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0718, 0.0450, 0.0849, 0.0874, 0.0811, 0.0777, 0.0737, 0.0501, 0.0501,\n",
       "           0.0559, 0.0435, 0.0712, 0.0644, 0.0673, 0.0759]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0713, 0.0452, 0.0773, 0.1031, 0.0909, 0.0766, 0.0698, 0.0536, 0.0482,\n",
       "          0.0492, 0.0415, 0.0630, 0.0722, 0.0660, 0.0720]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0713, 0.0452, 0.0773, 0.1031, 0.0909, 0.0766, 0.0698, 0.0536, 0.0482,\n",
       "           0.0492, 0.0415, 0.0630, 0.0722, 0.0660, 0.0720]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0675, 0.0510, 0.0910, 0.0746, 0.0904, 0.0758, 0.0695, 0.0531, 0.0457,\n",
       "          0.0626, 0.0452, 0.0782, 0.0641, 0.0650, 0.0665]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0675, 0.0510, 0.0910, 0.0746, 0.0904, 0.0758, 0.0695, 0.0531, 0.0457,\n",
       "           0.0626, 0.0452, 0.0782, 0.0641, 0.0650, 0.0665]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0672, 0.0466, 0.0746, 0.0872, 0.1036, 0.0810, 0.0704, 0.0525, 0.0527,\n",
       "          0.0522, 0.0410, 0.0785, 0.0559, 0.0710, 0.0653]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0672, 0.0466, 0.0746, 0.0872, 0.1036, 0.0810, 0.0704, 0.0525, 0.0527,\n",
       "           0.0522, 0.0410, 0.0785, 0.0559, 0.0710, 0.0653]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0726, 0.0483, 0.0795, 0.0780, 0.1270, 0.0759, 0.0619, 0.0518, 0.0403,\n",
       "          0.0521, 0.0421, 0.0708, 0.0595, 0.0683, 0.0721]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0726, 0.0483, 0.0795, 0.0780, 0.1270, 0.0759, 0.0619, 0.0518, 0.0403,\n",
       "           0.0521, 0.0421, 0.0708, 0.0595, 0.0683, 0.0721]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0687, 0.0542, 0.0701, 0.0762, 0.1044, 0.0668, 0.0705, 0.0476, 0.0515,\n",
       "          0.0601, 0.0467, 0.0805, 0.0696, 0.0611, 0.0721]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0687, 0.0542, 0.0701, 0.0762, 0.1044, 0.0668, 0.0705, 0.0476, 0.0515,\n",
       "           0.0601, 0.0467, 0.0805, 0.0696, 0.0611, 0.0721]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0758, 0.0423, 0.0858, 0.0938, 0.1213, 0.0711, 0.0696, 0.0522, 0.0493,\n",
       "          0.0509, 0.0460, 0.0641, 0.0471, 0.0600, 0.0707]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0758, 0.0423, 0.0858, 0.0938, 0.1213, 0.0711, 0.0696, 0.0522, 0.0493,\n",
       "           0.0509, 0.0460, 0.0641, 0.0471, 0.0600, 0.0707]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0760, 0.0473, 0.0876, 0.0995, 0.1119, 0.0713, 0.0736, 0.0420, 0.0481,\n",
       "          0.0548, 0.0401, 0.0608, 0.0531, 0.0616, 0.0723]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0760, 0.0473, 0.0876, 0.0995, 0.1119, 0.0713, 0.0736, 0.0420, 0.0481,\n",
       "           0.0548, 0.0401, 0.0608, 0.0531, 0.0616, 0.0723]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0775, 0.0452, 0.0750, 0.0926, 0.0804, 0.0796, 0.0689, 0.0556, 0.0525,\n",
       "          0.0522, 0.0463, 0.0770, 0.0602, 0.0631, 0.0738]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0775, 0.0452, 0.0750, 0.0926, 0.0804, 0.0796, 0.0689, 0.0556, 0.0525,\n",
       "           0.0522, 0.0463, 0.0770, 0.0602, 0.0631, 0.0738]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0731, 0.0395, 0.0760, 0.0837, 0.1131, 0.0927, 0.0705, 0.0425, 0.0465,\n",
       "          0.0496, 0.0362, 0.0840, 0.0559, 0.0608, 0.0760]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0731, 0.0395, 0.0760, 0.0837, 0.1131, 0.0927, 0.0705, 0.0425, 0.0465,\n",
       "           0.0496, 0.0362, 0.0840, 0.0559, 0.0608, 0.0760]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0717, 0.0511, 0.0851, 0.0935, 0.0999, 0.0780, 0.0723, 0.0518, 0.0530,\n",
       "          0.0553, 0.0482, 0.0616, 0.0518, 0.0602, 0.0666]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0717, 0.0511, 0.0851, 0.0935, 0.0999, 0.0780, 0.0723, 0.0518, 0.0530,\n",
       "           0.0553, 0.0482, 0.0616, 0.0518, 0.0602, 0.0666]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0703, 0.0449, 0.0814, 0.0867, 0.1274, 0.0729, 0.0685, 0.0533, 0.0510,\n",
       "          0.0520, 0.0383, 0.0726, 0.0520, 0.0576, 0.0713]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0703, 0.0449, 0.0814, 0.0867, 0.1274, 0.0729, 0.0685, 0.0533, 0.0510,\n",
       "           0.0520, 0.0383, 0.0726, 0.0520, 0.0576, 0.0713]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0836, 0.0449, 0.0903, 0.0868, 0.1055, 0.0806, 0.0810, 0.0426, 0.0497,\n",
       "          0.0424, 0.0377, 0.0766, 0.0514, 0.0519, 0.0750]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0836, 0.0449, 0.0903, 0.0868, 0.1055, 0.0806, 0.0810, 0.0426, 0.0497,\n",
       "           0.0424, 0.0377, 0.0766, 0.0514, 0.0519, 0.0750]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0630, 0.0507, 0.0891, 0.0943, 0.1014, 0.0839, 0.0716, 0.0424, 0.0492,\n",
       "          0.0491, 0.0456, 0.0697, 0.0542, 0.0667, 0.0690]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0630, 0.0507, 0.0891, 0.0943, 0.1014, 0.0839, 0.0716, 0.0424, 0.0492,\n",
       "           0.0491, 0.0456, 0.0697, 0.0542, 0.0667, 0.0690]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0745, 0.0423, 0.0906, 0.1034, 0.1169, 0.0824, 0.0691, 0.0369, 0.0474,\n",
       "          0.0506, 0.0333, 0.0636, 0.0513, 0.0632, 0.0745]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0745, 0.0423, 0.0906, 0.1034, 0.1169, 0.0824, 0.0691, 0.0369, 0.0474,\n",
       "           0.0506, 0.0333, 0.0636, 0.0513, 0.0632, 0.0745]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0770, 0.0537, 0.0888, 0.0798, 0.0984, 0.0711, 0.0761, 0.0461, 0.0561,\n",
       "          0.0553, 0.0460, 0.0658, 0.0559, 0.0572, 0.0726]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0770, 0.0537, 0.0888, 0.0798, 0.0984, 0.0711, 0.0761, 0.0461, 0.0561,\n",
       "           0.0553, 0.0460, 0.0658, 0.0559, 0.0572, 0.0726]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0735, 0.0457, 0.0729, 0.0868, 0.0959, 0.0997, 0.0730, 0.0483, 0.0563,\n",
       "          0.0590, 0.0384, 0.0660, 0.0580, 0.0590, 0.0675]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0735, 0.0457, 0.0729, 0.0868, 0.0959, 0.0997, 0.0730, 0.0483, 0.0563,\n",
       "           0.0590, 0.0384, 0.0660, 0.0580, 0.0590, 0.0675]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0771, 0.0528, 0.0756, 0.0760, 0.0993, 0.0719, 0.0649, 0.0456, 0.0524,\n",
       "          0.0593, 0.0479, 0.0771, 0.0652, 0.0631, 0.0719]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0771, 0.0528, 0.0756, 0.0760, 0.0993, 0.0719, 0.0649, 0.0456, 0.0524,\n",
       "           0.0593, 0.0479, 0.0771, 0.0652, 0.0631, 0.0719]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0587, 0.0522, 0.0810, 0.0794, 0.1008, 0.0735, 0.0607, 0.0513, 0.0625,\n",
       "          0.0621, 0.0453, 0.0668, 0.0697, 0.0666, 0.0694]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0587, 0.0522, 0.0810, 0.0794, 0.1008, 0.0735, 0.0607, 0.0513, 0.0625,\n",
       "           0.0621, 0.0453, 0.0668, 0.0697, 0.0666, 0.0694]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0730, 0.0441, 0.0805, 0.0838, 0.0949, 0.0782, 0.0566, 0.0521, 0.0523,\n",
       "          0.0504, 0.0386, 0.0860, 0.0724, 0.0618, 0.0753]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0730, 0.0441, 0.0805, 0.0838, 0.0949, 0.0782, 0.0566, 0.0521, 0.0523,\n",
       "           0.0504, 0.0386, 0.0860, 0.0724, 0.0618, 0.0753]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0918, 0.0401, 0.0859, 0.0999, 0.1223, 0.0815, 0.0761, 0.0396, 0.0491,\n",
       "          0.0460, 0.0314, 0.0718, 0.0470, 0.0555, 0.0620]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0918, 0.0401, 0.0859, 0.0999, 0.1223, 0.0815, 0.0761, 0.0396, 0.0491,\n",
       "           0.0460, 0.0314, 0.0718, 0.0470, 0.0555, 0.0620]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0731, 0.0399, 0.0700, 0.1017, 0.1271, 0.1009, 0.0646, 0.0463, 0.0476,\n",
       "          0.0475, 0.0308, 0.0651, 0.0564, 0.0602, 0.0690]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0731, 0.0399, 0.0700, 0.1017, 0.1271, 0.1009, 0.0646, 0.0463, 0.0476,\n",
       "           0.0475, 0.0308, 0.0651, 0.0564, 0.0602, 0.0690]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0768, 0.0330, 0.0778, 0.0835, 0.1216, 0.1006, 0.0791, 0.0454, 0.0486,\n",
       "          0.0384, 0.0373, 0.0754, 0.0504, 0.0568, 0.0754]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0768, 0.0330, 0.0778, 0.0835, 0.1216, 0.1006, 0.0791, 0.0454, 0.0486,\n",
       "           0.0384, 0.0373, 0.0754, 0.0504, 0.0568, 0.0754]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0759, 0.0490, 0.0855, 0.0783, 0.0897, 0.0788, 0.0675, 0.0498, 0.0580,\n",
       "          0.0483, 0.0426, 0.0770, 0.0638, 0.0608, 0.0749]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0759, 0.0490, 0.0855, 0.0783, 0.0897, 0.0788, 0.0675, 0.0498, 0.0580,\n",
       "           0.0483, 0.0426, 0.0770, 0.0638, 0.0608, 0.0749]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0719, 0.0436, 0.0894, 0.0888, 0.1168, 0.0829, 0.0643, 0.0487, 0.0496,\n",
       "          0.0518, 0.0442, 0.0663, 0.0553, 0.0549, 0.0715]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0719, 0.0436, 0.0894, 0.0888, 0.1168, 0.0829, 0.0643, 0.0487, 0.0496,\n",
       "           0.0518, 0.0442, 0.0663, 0.0553, 0.0549, 0.0715]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0762, 0.0476, 0.0751, 0.0963, 0.1150, 0.0781, 0.0684, 0.0421, 0.0524,\n",
       "          0.0417, 0.0389, 0.0802, 0.0504, 0.0676, 0.0700]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0762, 0.0476, 0.0751, 0.0963, 0.1150, 0.0781, 0.0684, 0.0421, 0.0524,\n",
       "           0.0417, 0.0389, 0.0802, 0.0504, 0.0676, 0.0700]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0800, 0.0349, 0.0675, 0.0985, 0.1608, 0.0737, 0.0643, 0.0382, 0.0432,\n",
       "          0.0467, 0.0304, 0.0715, 0.0456, 0.0642, 0.0804]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0800, 0.0349, 0.0675, 0.0985, 0.1608, 0.0737, 0.0643, 0.0382, 0.0432,\n",
       "           0.0467, 0.0304, 0.0715, 0.0456, 0.0642, 0.0804]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0612, 0.0553, 0.0751, 0.0821, 0.0892, 0.0766, 0.0693, 0.0548, 0.0550,\n",
       "          0.0632, 0.0546, 0.0669, 0.0637, 0.0604, 0.0727]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0612, 0.0553, 0.0751, 0.0821, 0.0892, 0.0766, 0.0693, 0.0548, 0.0550,\n",
       "           0.0632, 0.0546, 0.0669, 0.0637, 0.0604, 0.0727]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0734, 0.0420, 0.0908, 0.0902, 0.0819, 0.0720, 0.0637, 0.0483, 0.0527,\n",
       "          0.0516, 0.0439, 0.0815, 0.0567, 0.0663, 0.0850]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0734, 0.0420, 0.0908, 0.0902, 0.0819, 0.0720, 0.0637, 0.0483, 0.0527,\n",
       "           0.0516, 0.0439, 0.0815, 0.0567, 0.0663, 0.0850]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0671, 0.0494, 0.0858, 0.0759, 0.1059, 0.0826, 0.0760, 0.0420, 0.0493,\n",
       "          0.0468, 0.0425, 0.0890, 0.0551, 0.0547, 0.0781]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0671, 0.0494, 0.0858, 0.0759, 0.1059, 0.0826, 0.0760, 0.0420, 0.0493,\n",
       "           0.0468, 0.0425, 0.0890, 0.0551, 0.0547, 0.0781]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0734, 0.0399, 0.0832, 0.0817, 0.1454, 0.0830, 0.0682, 0.0388, 0.0413,\n",
       "          0.0497, 0.0377, 0.0671, 0.0501, 0.0615, 0.0790]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0734, 0.0399, 0.0832, 0.0817, 0.1454, 0.0830, 0.0682, 0.0388, 0.0413,\n",
       "           0.0497, 0.0377, 0.0671, 0.0501, 0.0615, 0.0790]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0755, 0.0459, 0.0862, 0.0854, 0.0929, 0.0843, 0.0645, 0.0439, 0.0504,\n",
       "          0.0508, 0.0410, 0.0782, 0.0489, 0.0674, 0.0846]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0755, 0.0459, 0.0862, 0.0854, 0.0929, 0.0843, 0.0645, 0.0439, 0.0504,\n",
       "           0.0508, 0.0410, 0.0782, 0.0489, 0.0674, 0.0846]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0890, 0.0467, 0.0809, 0.0843, 0.0866, 0.0783, 0.0649, 0.0517, 0.0488,\n",
       "          0.0467, 0.0425, 0.0758, 0.0549, 0.0669, 0.0819]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0890, 0.0467, 0.0809, 0.0843, 0.0866, 0.0783, 0.0649, 0.0517, 0.0488,\n",
       "           0.0467, 0.0425, 0.0758, 0.0549, 0.0669, 0.0819]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0682, 0.0549, 0.0729, 0.1000, 0.1119, 0.0765, 0.0560, 0.0489, 0.0573,\n",
       "          0.0520, 0.0393, 0.0728, 0.0513, 0.0654, 0.0726]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0682, 0.0549, 0.0729, 0.1000, 0.1119, 0.0765, 0.0560, 0.0489, 0.0573,\n",
       "           0.0520, 0.0393, 0.0728, 0.0513, 0.0654, 0.0726]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0849, 0.0431, 0.0750, 0.0731, 0.0854, 0.0799, 0.0670, 0.0427, 0.0580,\n",
       "          0.0438, 0.0421, 0.0973, 0.0625, 0.0587, 0.0865]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0849, 0.0431, 0.0750, 0.0731, 0.0854, 0.0799, 0.0670, 0.0427, 0.0580,\n",
       "           0.0438, 0.0421, 0.0973, 0.0625, 0.0587, 0.0865]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0754, 0.0475, 0.0837, 0.0802, 0.0900, 0.0818, 0.0603, 0.0468, 0.0568,\n",
       "          0.0527, 0.0414, 0.0782, 0.0604, 0.0748, 0.0701]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0754, 0.0475, 0.0837, 0.0802, 0.0900, 0.0818, 0.0603, 0.0468, 0.0568,\n",
       "           0.0527, 0.0414, 0.0782, 0.0604, 0.0748, 0.0701]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0752, 0.0479, 0.0745, 0.0818, 0.1181, 0.0742, 0.0609, 0.0448, 0.0502,\n",
       "          0.0496, 0.0383, 0.0798, 0.0528, 0.0757, 0.0762]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0752, 0.0479, 0.0745, 0.0818, 0.1181, 0.0742, 0.0609, 0.0448, 0.0502,\n",
       "           0.0496, 0.0383, 0.0798, 0.0528, 0.0757, 0.0762]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0826, 0.0467, 0.0978, 0.0857, 0.1120, 0.0840, 0.0629, 0.0419, 0.0529,\n",
       "          0.0346, 0.0365, 0.0761, 0.0425, 0.0635, 0.0802]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0826, 0.0467, 0.0978, 0.0857, 0.1120, 0.0840, 0.0629, 0.0419, 0.0529,\n",
       "           0.0346, 0.0365, 0.0761, 0.0425, 0.0635, 0.0802]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0717, 0.0539, 0.0788, 0.0727, 0.0902, 0.0838, 0.0667, 0.0418, 0.0492,\n",
       "          0.0523, 0.0488, 0.0824, 0.0629, 0.0693, 0.0754]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0717, 0.0539, 0.0788, 0.0727, 0.0902, 0.0838, 0.0667, 0.0418, 0.0492,\n",
       "           0.0523, 0.0488, 0.0824, 0.0629, 0.0693, 0.0754]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0775, 0.0482, 0.0924, 0.0793, 0.1290, 0.0795, 0.0682, 0.0454, 0.0453,\n",
       "          0.0402, 0.0358, 0.0786, 0.0505, 0.0543, 0.0757]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0775, 0.0482, 0.0924, 0.0793, 0.1290, 0.0795, 0.0682, 0.0454, 0.0453,\n",
       "           0.0402, 0.0358, 0.0786, 0.0505, 0.0543, 0.0757]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0805, 0.0430, 0.0957, 0.0767, 0.1132, 0.0833, 0.0673, 0.0363, 0.0447,\n",
       "          0.0370, 0.0401, 0.0750, 0.0477, 0.0666, 0.0929]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0805, 0.0430, 0.0957, 0.0767, 0.1132, 0.0833, 0.0673, 0.0363, 0.0447,\n",
       "           0.0370, 0.0401, 0.0750, 0.0477, 0.0666, 0.0929]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0810, 0.0432, 0.0777, 0.0800, 0.1122, 0.0793, 0.0782, 0.0409, 0.0565,\n",
       "          0.0426, 0.0383, 0.0760, 0.0606, 0.0548, 0.0785]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0810, 0.0432, 0.0777, 0.0800, 0.1122, 0.0793, 0.0782, 0.0409, 0.0565,\n",
       "           0.0426, 0.0383, 0.0760, 0.0606, 0.0548, 0.0785]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0789, 0.0525, 0.0828, 0.0805, 0.1431, 0.0848, 0.0559, 0.0419, 0.0417,\n",
       "          0.0407, 0.0350, 0.0733, 0.0441, 0.0626, 0.0822]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0789, 0.0525, 0.0828, 0.0805, 0.1431, 0.0848, 0.0559, 0.0419, 0.0417,\n",
       "           0.0407, 0.0350, 0.0733, 0.0441, 0.0626, 0.0822]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0711, 0.0412, 0.0919, 0.0811, 0.0999, 0.0807, 0.0699, 0.0414, 0.0440,\n",
       "          0.0527, 0.0402, 0.0743, 0.0577, 0.0679, 0.0860]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0711, 0.0412, 0.0919, 0.0811, 0.0999, 0.0807, 0.0699, 0.0414, 0.0440,\n",
       "           0.0527, 0.0402, 0.0743, 0.0577, 0.0679, 0.0860]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0781, 0.0545, 0.0845, 0.0867, 0.1122, 0.0712, 0.0612, 0.0424, 0.0516,\n",
       "          0.0497, 0.0464, 0.0715, 0.0503, 0.0651, 0.0747]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0781, 0.0545, 0.0845, 0.0867, 0.1122, 0.0712, 0.0612, 0.0424, 0.0516,\n",
       "           0.0497, 0.0464, 0.0715, 0.0503, 0.0651, 0.0747]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0768, 0.0579, 0.0741, 0.0818, 0.0867, 0.0793, 0.0668, 0.0445, 0.0586,\n",
       "          0.0529, 0.0475, 0.0716, 0.0566, 0.0666, 0.0784]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0768, 0.0579, 0.0741, 0.0818, 0.0867, 0.0793, 0.0668, 0.0445, 0.0586,\n",
       "           0.0529, 0.0475, 0.0716, 0.0566, 0.0666, 0.0784]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0784, 0.0503, 0.0771, 0.0754, 0.1168, 0.0978, 0.0731, 0.0366, 0.0488,\n",
       "          0.0381, 0.0403, 0.0805, 0.0466, 0.0644, 0.0758]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0784, 0.0503, 0.0771, 0.0754, 0.1168, 0.0978, 0.0731, 0.0366, 0.0488,\n",
       "           0.0381, 0.0403, 0.0805, 0.0466, 0.0644, 0.0758]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0816, 0.0481, 0.0903, 0.0745, 0.1183, 0.0809, 0.0642, 0.0337, 0.0396,\n",
       "          0.0431, 0.0324, 0.0838, 0.0446, 0.0693, 0.0956]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0816, 0.0481, 0.0903, 0.0745, 0.1183, 0.0809, 0.0642, 0.0337, 0.0396,\n",
       "           0.0431, 0.0324, 0.0838, 0.0446, 0.0693, 0.0956]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0699, 0.0514, 0.0795, 0.0887, 0.1146, 0.0773, 0.0589, 0.0423, 0.0562,\n",
       "          0.0468, 0.0373, 0.0724, 0.0502, 0.0695, 0.0851]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0699, 0.0514, 0.0795, 0.0887, 0.1146, 0.0773, 0.0589, 0.0423, 0.0562,\n",
       "           0.0468, 0.0373, 0.0724, 0.0502, 0.0695, 0.0851]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0798, 0.0597, 0.0744, 0.0763, 0.1084, 0.0890, 0.0711, 0.0400, 0.0581,\n",
       "          0.0476, 0.0431, 0.0692, 0.0568, 0.0530, 0.0734]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0798, 0.0597, 0.0744, 0.0763, 0.1084, 0.0890, 0.0711, 0.0400, 0.0581,\n",
       "           0.0476, 0.0431, 0.0692, 0.0568, 0.0530, 0.0734]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0772, 0.0532, 0.0776, 0.0754, 0.1111, 0.0812, 0.0629, 0.0368, 0.0483,\n",
       "          0.0462, 0.0353, 0.0942, 0.0507, 0.0682, 0.0817]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0772, 0.0532, 0.0776, 0.0754, 0.1111, 0.0812, 0.0629, 0.0368, 0.0483,\n",
       "           0.0462, 0.0353, 0.0942, 0.0507, 0.0682, 0.0817]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0758, 0.0615, 0.0833, 0.0780, 0.1191, 0.0834, 0.0644, 0.0402, 0.0554,\n",
       "          0.0515, 0.0427, 0.0684, 0.0474, 0.0625, 0.0664]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0758, 0.0615, 0.0833, 0.0780, 0.1191, 0.0834, 0.0644, 0.0402, 0.0554,\n",
       "           0.0515, 0.0427, 0.0684, 0.0474, 0.0625, 0.0664]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0749, 0.0584, 0.0888, 0.0818, 0.0958, 0.0746, 0.0557, 0.0412, 0.0451,\n",
       "          0.0476, 0.0466, 0.0872, 0.0616, 0.0614, 0.0794]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0749, 0.0584, 0.0888, 0.0818, 0.0958, 0.0746, 0.0557, 0.0412, 0.0451,\n",
       "           0.0476, 0.0466, 0.0872, 0.0616, 0.0614, 0.0794]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0729, 0.0476, 0.0886, 0.0847, 0.1275, 0.0968, 0.0596, 0.0298, 0.0421,\n",
       "          0.0417, 0.0367, 0.0762, 0.0561, 0.0629, 0.0769]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0729, 0.0476, 0.0886, 0.0847, 0.1275, 0.0968, 0.0596, 0.0298, 0.0421,\n",
       "           0.0417, 0.0367, 0.0762, 0.0561, 0.0629, 0.0769]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0648, 0.0560, 0.0762, 0.0765, 0.1007, 0.0746, 0.0582, 0.0458, 0.0577,\n",
       "          0.0600, 0.0440, 0.0774, 0.0522, 0.0797, 0.0762]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0648, 0.0560, 0.0762, 0.0765, 0.1007, 0.0746, 0.0582, 0.0458, 0.0577,\n",
       "           0.0600, 0.0440, 0.0774, 0.0522, 0.0797, 0.0762]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0681, 0.0449, 0.0993, 0.0759, 0.1629, 0.0832, 0.0609, 0.0346, 0.0419,\n",
       "          0.0358, 0.0299, 0.0723, 0.0487, 0.0688, 0.0728]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0681, 0.0449, 0.0993, 0.0759, 0.1629, 0.0832, 0.0609, 0.0346, 0.0419,\n",
       "           0.0358, 0.0299, 0.0723, 0.0487, 0.0688, 0.0728]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0921, 0.0499, 0.0843, 0.0943, 0.1190, 0.0917, 0.0518, 0.0353, 0.0491,\n",
       "          0.0339, 0.0332, 0.0739, 0.0464, 0.0678, 0.0773]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0921, 0.0499, 0.0843, 0.0943, 0.1190, 0.0917, 0.0518, 0.0353, 0.0491,\n",
       "           0.0339, 0.0332, 0.0739, 0.0464, 0.0678, 0.0773]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0714, 0.0512, 0.0835, 0.0834, 0.1150, 0.0833, 0.0605, 0.0409, 0.0437,\n",
       "          0.0426, 0.0374, 0.0779, 0.0560, 0.0704, 0.0829]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0714, 0.0512, 0.0835, 0.0834, 0.1150, 0.0833, 0.0605, 0.0409, 0.0437,\n",
       "           0.0426, 0.0374, 0.0779, 0.0560, 0.0704, 0.0829]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0789, 0.0513, 0.1046, 0.0755, 0.1462, 0.0753, 0.0560, 0.0353, 0.0551,\n",
       "          0.0376, 0.0314, 0.0710, 0.0423, 0.0691, 0.0703]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0789, 0.0513, 0.1046, 0.0755, 0.1462, 0.0753, 0.0560, 0.0353, 0.0551,\n",
       "           0.0376, 0.0314, 0.0710, 0.0423, 0.0691, 0.0703]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0762, 0.0565, 0.0874, 0.0714, 0.1288, 0.0769, 0.0605, 0.0369, 0.0516,\n",
       "          0.0419, 0.0369, 0.0767, 0.0488, 0.0668, 0.0827]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0762, 0.0565, 0.0874, 0.0714, 0.1288, 0.0769, 0.0605, 0.0369, 0.0516,\n",
       "           0.0419, 0.0369, 0.0767, 0.0488, 0.0668, 0.0827]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0801, 0.0509, 0.0818, 0.0791, 0.0888, 0.0821, 0.0509, 0.0368, 0.0488,\n",
       "          0.0485, 0.0412, 0.0943, 0.0677, 0.0728, 0.0761]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0801, 0.0509, 0.0818, 0.0791, 0.0888, 0.0821, 0.0509, 0.0368, 0.0488,\n",
       "           0.0485, 0.0412, 0.0943, 0.0677, 0.0728, 0.0761]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0760, 0.0553, 0.0819, 0.0962, 0.1412, 0.0703, 0.0548, 0.0417, 0.0360,\n",
       "          0.0425, 0.0329, 0.0730, 0.0457, 0.0788, 0.0738]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0760, 0.0553, 0.0819, 0.0962, 0.1412, 0.0703, 0.0548, 0.0417, 0.0360,\n",
       "           0.0425, 0.0329, 0.0730, 0.0457, 0.0788, 0.0738]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0856, 0.0521, 0.0859, 0.0719, 0.1215, 0.0809, 0.0696, 0.0331, 0.0500,\n",
       "          0.0428, 0.0372, 0.0766, 0.0464, 0.0753, 0.0711]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0856, 0.0521, 0.0859, 0.0719, 0.1215, 0.0809, 0.0696, 0.0331, 0.0500,\n",
       "           0.0428, 0.0372, 0.0766, 0.0464, 0.0753, 0.0711]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0717, 0.0498, 0.0962, 0.0759, 0.1417, 0.0797, 0.0680, 0.0270, 0.0507,\n",
       "          0.0405, 0.0356, 0.0721, 0.0430, 0.0749, 0.0732]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0717, 0.0498, 0.0962, 0.0759, 0.1417, 0.0797, 0.0680, 0.0270, 0.0507,\n",
       "           0.0405, 0.0356, 0.0721, 0.0430, 0.0749, 0.0732]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0810, 0.0603, 0.0844, 0.0873, 0.1283, 0.0838, 0.0597, 0.0363, 0.0552,\n",
       "          0.0389, 0.0315, 0.0700, 0.0495, 0.0649, 0.0690]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0810, 0.0603, 0.0844, 0.0873, 0.1283, 0.0838, 0.0597, 0.0363, 0.0552,\n",
       "           0.0389, 0.0315, 0.0700, 0.0495, 0.0649, 0.0690]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0785, 0.0462, 0.0878, 0.0768, 0.1131, 0.0938, 0.0629, 0.0321, 0.0490,\n",
       "          0.0415, 0.0347, 0.0732, 0.0523, 0.0725, 0.0856]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0785, 0.0462, 0.0878, 0.0768, 0.1131, 0.0938, 0.0629, 0.0321, 0.0490,\n",
       "           0.0415, 0.0347, 0.0732, 0.0523, 0.0725, 0.0856]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0772, 0.0555, 0.0901, 0.0683, 0.1257, 0.0879, 0.0546, 0.0325, 0.0472,\n",
       "          0.0417, 0.0323, 0.0730, 0.0535, 0.0839, 0.0764]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0772, 0.0555, 0.0901, 0.0683, 0.1257, 0.0879, 0.0546, 0.0325, 0.0472,\n",
       "           0.0417, 0.0323, 0.0730, 0.0535, 0.0839, 0.0764]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0666, 0.0548, 0.0903, 0.0756, 0.0912, 0.0757, 0.0666, 0.0401, 0.0515,\n",
       "          0.0465, 0.0489, 0.0729, 0.0667, 0.0693, 0.0833]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0666, 0.0548, 0.0903, 0.0756, 0.0912, 0.0757, 0.0666, 0.0401, 0.0515,\n",
       "           0.0465, 0.0489, 0.0729, 0.0667, 0.0693, 0.0833]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0775, 0.0458, 0.0880, 0.0829, 0.1411, 0.0837, 0.0598, 0.0357, 0.0439,\n",
       "          0.0395, 0.0292, 0.0687, 0.0396, 0.0763, 0.0883]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0775, 0.0458, 0.0880, 0.0829, 0.1411, 0.0837, 0.0598, 0.0357, 0.0439,\n",
       "           0.0395, 0.0292, 0.0687, 0.0396, 0.0763, 0.0883]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0807, 0.0628, 0.0848, 0.0611, 0.0742, 0.0804, 0.0696, 0.0420, 0.0605,\n",
       "          0.0456, 0.0468, 0.0862, 0.0601, 0.0725, 0.0726]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0807, 0.0628, 0.0848, 0.0611, 0.0742, 0.0804, 0.0696, 0.0420, 0.0605,\n",
       "           0.0456, 0.0468, 0.0862, 0.0601, 0.0725, 0.0726]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0736, 0.0620, 0.0837, 0.0918, 0.0953, 0.0711, 0.0614, 0.0345, 0.0474,\n",
       "          0.0412, 0.0380, 0.0874, 0.0593, 0.0773, 0.0762]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0736, 0.0620, 0.0837, 0.0918, 0.0953, 0.0711, 0.0614, 0.0345, 0.0474,\n",
       "           0.0412, 0.0380, 0.0874, 0.0593, 0.0773, 0.0762]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0725, 0.0532, 0.0931, 0.0729, 0.1034, 0.0812, 0.0517, 0.0312, 0.0439,\n",
       "          0.0469, 0.0343, 0.0887, 0.0586, 0.0792, 0.0891]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0725, 0.0532, 0.0931, 0.0729, 0.1034, 0.0812, 0.0517, 0.0312, 0.0439,\n",
       "           0.0469, 0.0343, 0.0887, 0.0586, 0.0792, 0.0891]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0652, 0.0648, 0.0813, 0.0742, 0.0878, 0.0744, 0.0528, 0.0443, 0.0602,\n",
       "          0.0541, 0.0487, 0.0774, 0.0623, 0.0718, 0.0808]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0652, 0.0648, 0.0813, 0.0742, 0.0878, 0.0744, 0.0528, 0.0443, 0.0602,\n",
       "           0.0541, 0.0487, 0.0774, 0.0623, 0.0718, 0.0808]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0700, 0.0445, 0.1073, 0.0840, 0.1293, 0.0759, 0.0465, 0.0311, 0.0402,\n",
       "          0.0413, 0.0291, 0.0711, 0.0501, 0.0901, 0.0896]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0700, 0.0445, 0.1073, 0.0840, 0.1293, 0.0759, 0.0465, 0.0311, 0.0402,\n",
       "           0.0413, 0.0291, 0.0711, 0.0501, 0.0901, 0.0896]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0849, 0.0504, 0.0832, 0.0679, 0.0831, 0.0823, 0.0627, 0.0400, 0.0538,\n",
       "          0.0463, 0.0382, 0.0831, 0.0582, 0.0798, 0.0863]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0849, 0.0504, 0.0832, 0.0679, 0.0831, 0.0823, 0.0627, 0.0400, 0.0538,\n",
       "           0.0463, 0.0382, 0.0831, 0.0582, 0.0798, 0.0863]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0710, 0.0512, 0.0820, 0.0672, 0.0894, 0.0694, 0.0727, 0.0349, 0.0595,\n",
       "          0.0449, 0.0410, 0.0795, 0.0456, 0.0989, 0.0928]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0710, 0.0512, 0.0820, 0.0672, 0.0894, 0.0694, 0.0727, 0.0349, 0.0595,\n",
       "           0.0449, 0.0410, 0.0795, 0.0456, 0.0989, 0.0928]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0729, 0.0455, 0.0919, 0.0612, 0.1314, 0.0722, 0.0488, 0.0364, 0.0576,\n",
       "          0.0416, 0.0332, 0.0789, 0.0506, 0.0874, 0.0903]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0729, 0.0455, 0.0919, 0.0612, 0.1314, 0.0722, 0.0488, 0.0364, 0.0576,\n",
       "           0.0416, 0.0332, 0.0789, 0.0506, 0.0874, 0.0903]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0818, 0.0519, 0.0989, 0.0721, 0.1331, 0.0793, 0.0503, 0.0279, 0.0403,\n",
       "          0.0355, 0.0274, 0.0741, 0.0535, 0.0881, 0.0858]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0818, 0.0519, 0.0989, 0.0721, 0.1331, 0.0793, 0.0503, 0.0279, 0.0403,\n",
       "           0.0355, 0.0274, 0.0741, 0.0535, 0.0881, 0.0858]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0729, 0.0517, 0.0899, 0.0669, 0.0782, 0.0743, 0.0596, 0.0415, 0.0624,\n",
       "          0.0452, 0.0357, 0.0859, 0.0564, 0.0918, 0.0877]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0729, 0.0517, 0.0899, 0.0669, 0.0782, 0.0743, 0.0596, 0.0415, 0.0624,\n",
       "           0.0452, 0.0357, 0.0859, 0.0564, 0.0918, 0.0877]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0650, 0.0480, 0.0951, 0.0775, 0.0882, 0.0660, 0.0534, 0.0326, 0.0570,\n",
       "          0.0481, 0.0403, 0.0892, 0.0619, 0.1004, 0.0773]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0650, 0.0480, 0.0951, 0.0775, 0.0882, 0.0660, 0.0534, 0.0326, 0.0570,\n",
       "           0.0481, 0.0403, 0.0892, 0.0619, 0.1004, 0.0773]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0621, 0.0505, 0.0862, 0.0678, 0.0904, 0.0688, 0.0592, 0.0381, 0.0641,\n",
       "          0.0674, 0.0373, 0.0802, 0.0612, 0.0940, 0.0726]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0621, 0.0505, 0.0862, 0.0678, 0.0904, 0.0688, 0.0592, 0.0381, 0.0641,\n",
       "           0.0674, 0.0373, 0.0802, 0.0612, 0.0940, 0.0726]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0750, 0.0596, 0.0781, 0.0787, 0.0947, 0.0704, 0.0600, 0.0352, 0.0583,\n",
       "          0.0415, 0.0359, 0.0974, 0.0624, 0.0806, 0.0723]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0750, 0.0596, 0.0781, 0.0787, 0.0947, 0.0704, 0.0600, 0.0352, 0.0583,\n",
       "           0.0415, 0.0359, 0.0974, 0.0624, 0.0806, 0.0723]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0838, 0.0476, 0.0965, 0.0728, 0.1162, 0.0812, 0.0537, 0.0365, 0.0480,\n",
       "          0.0426, 0.0282, 0.0725, 0.0462, 0.0896, 0.0846]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0838, 0.0476, 0.0965, 0.0728, 0.1162, 0.0812, 0.0537, 0.0365, 0.0480,\n",
       "           0.0426, 0.0282, 0.0725, 0.0462, 0.0896, 0.0846]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0723, 0.0488, 0.0854, 0.0710, 0.0826, 0.0706, 0.0603, 0.0357, 0.0631,\n",
       "          0.0475, 0.0315, 0.0970, 0.0606, 0.0980, 0.0755]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0723, 0.0488, 0.0854, 0.0710, 0.0826, 0.0706, 0.0603, 0.0357, 0.0631,\n",
       "           0.0475, 0.0315, 0.0970, 0.0606, 0.0980, 0.0755]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0775, 0.0507, 0.0853, 0.0711, 0.1157, 0.0743, 0.0501, 0.0381, 0.0623,\n",
       "          0.0407, 0.0338, 0.0773, 0.0562, 0.0903, 0.0768]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0775, 0.0507, 0.0853, 0.0711, 0.1157, 0.0743, 0.0501, 0.0381, 0.0623,\n",
       "           0.0407, 0.0338, 0.0773, 0.0562, 0.0903, 0.0768]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0769, 0.0469, 0.0849, 0.0685, 0.1265, 0.0938, 0.0598, 0.0340, 0.0427,\n",
       "          0.0264, 0.0276, 0.0842, 0.0494, 0.0970, 0.0814]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0769, 0.0469, 0.0849, 0.0685, 0.1265, 0.0938, 0.0598, 0.0340, 0.0427,\n",
       "           0.0264, 0.0276, 0.0842, 0.0494, 0.0970, 0.0814]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0656, 0.0552, 0.0944, 0.0800, 0.1083, 0.0755, 0.0672, 0.0300, 0.0693,\n",
       "          0.0390, 0.0306, 0.0843, 0.0526, 0.0778, 0.0701]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0656, 0.0552, 0.0944, 0.0800, 0.1083, 0.0755, 0.0672, 0.0300, 0.0693,\n",
       "           0.0390, 0.0306, 0.0843, 0.0526, 0.0778, 0.0701]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0749, 0.0563, 0.0796, 0.0672, 0.0808, 0.0741, 0.0662, 0.0392, 0.0517,\n",
       "          0.0597, 0.0403, 0.0809, 0.0600, 0.0983, 0.0708]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0749, 0.0563, 0.0796, 0.0672, 0.0808, 0.0741, 0.0662, 0.0392, 0.0517,\n",
       "           0.0597, 0.0403, 0.0809, 0.0600, 0.0983, 0.0708]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0635, 0.0713, 0.0781, 0.0787, 0.0907, 0.0705, 0.0563, 0.0479, 0.0642,\n",
       "          0.0615, 0.0423, 0.0747, 0.0550, 0.0748, 0.0707]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0635, 0.0713, 0.0781, 0.0787, 0.0907, 0.0705, 0.0563, 0.0479, 0.0642,\n",
       "           0.0615, 0.0423, 0.0747, 0.0550, 0.0748, 0.0707]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0702, 0.0528, 0.0885, 0.0747, 0.0741, 0.0768, 0.0568, 0.0495, 0.0676,\n",
       "          0.0547, 0.0409, 0.0834, 0.0601, 0.0875, 0.0623]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0702, 0.0528, 0.0885, 0.0747, 0.0741, 0.0768, 0.0568, 0.0495, 0.0676,\n",
       "           0.0547, 0.0409, 0.0834, 0.0601, 0.0875, 0.0623]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0891, 0.0506, 0.0838, 0.0798, 0.0870, 0.0755, 0.0550, 0.0425, 0.0629,\n",
       "          0.0423, 0.0330, 0.0801, 0.0495, 0.0893, 0.0795]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0891, 0.0506, 0.0838, 0.0798, 0.0870, 0.0755, 0.0550, 0.0425, 0.0629,\n",
       "           0.0423, 0.0330, 0.0801, 0.0495, 0.0893, 0.0795]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0676, 0.0561, 0.0955, 0.0630, 0.1021, 0.0793, 0.0567, 0.0359, 0.0669,\n",
       "          0.0443, 0.0319, 0.0807, 0.0571, 0.0816, 0.0815]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0676, 0.0561, 0.0955, 0.0630, 0.1021, 0.0793, 0.0567, 0.0359, 0.0669,\n",
       "           0.0443, 0.0319, 0.0807, 0.0571, 0.0816, 0.0815]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0687, 0.0549, 0.0736, 0.0723, 0.1111, 0.0776, 0.0613, 0.0375, 0.0580,\n",
       "          0.0428, 0.0297, 0.0755, 0.0555, 0.0916, 0.0900]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0687, 0.0549, 0.0736, 0.0723, 0.1111, 0.0776, 0.0613, 0.0375, 0.0580,\n",
       "           0.0428, 0.0297, 0.0755, 0.0555, 0.0916, 0.0900]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0710, 0.0608, 0.1010, 0.0695, 0.1077, 0.0784, 0.0460, 0.0365, 0.0658,\n",
       "          0.0445, 0.0278, 0.0894, 0.0567, 0.0814, 0.0635]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0710, 0.0608, 0.1010, 0.0695, 0.1077, 0.0784, 0.0460, 0.0365, 0.0658,\n",
       "           0.0445, 0.0278, 0.0894, 0.0567, 0.0814, 0.0635]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0720, 0.0637, 0.0737, 0.0668, 0.0789, 0.0692, 0.0648, 0.0440, 0.0692,\n",
       "          0.0504, 0.0490, 0.0917, 0.0599, 0.0792, 0.0676]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0720, 0.0637, 0.0737, 0.0668, 0.0789, 0.0692, 0.0648, 0.0440, 0.0692,\n",
       "           0.0504, 0.0490, 0.0917, 0.0599, 0.0792, 0.0676]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0743, 0.0615, 0.0711, 0.0807, 0.0833, 0.0736, 0.0580, 0.0389, 0.0678,\n",
       "          0.0513, 0.0413, 0.0755, 0.0605, 0.0855, 0.0766]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0743, 0.0615, 0.0711, 0.0807, 0.0833, 0.0736, 0.0580, 0.0389, 0.0678,\n",
       "           0.0513, 0.0413, 0.0755, 0.0605, 0.0855, 0.0766]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0649, 0.0625, 0.0907, 0.0790, 0.0704, 0.0676, 0.0535, 0.0434, 0.0721,\n",
       "          0.0527, 0.0465, 0.0818, 0.0634, 0.0796, 0.0718]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0649, 0.0625, 0.0907, 0.0790, 0.0704, 0.0676, 0.0535, 0.0434, 0.0721,\n",
       "           0.0527, 0.0465, 0.0818, 0.0634, 0.0796, 0.0718]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0632, 0.0490, 0.0873, 0.0894, 0.0904, 0.0840, 0.0508, 0.0428, 0.0540,\n",
       "          0.0477, 0.0397, 0.0723, 0.0563, 0.0984, 0.0747]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0632, 0.0490, 0.0873, 0.0894, 0.0904, 0.0840, 0.0508, 0.0428, 0.0540,\n",
       "           0.0477, 0.0397, 0.0723, 0.0563, 0.0984, 0.0747]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0679, 0.0562, 0.0864, 0.0684, 0.1202, 0.0681, 0.0538, 0.0370, 0.0495,\n",
       "          0.0422, 0.0296, 0.0764, 0.0555, 0.1006, 0.0882]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0679, 0.0562, 0.0864, 0.0684, 0.1202, 0.0681, 0.0538, 0.0370, 0.0495,\n",
       "           0.0422, 0.0296, 0.0764, 0.0555, 0.1006, 0.0882]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0669, 0.0638, 0.0917, 0.0663, 0.0805, 0.0640, 0.0568, 0.0448, 0.0563,\n",
       "          0.0501, 0.0432, 0.0906, 0.0698, 0.0809, 0.0745]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0669, 0.0638, 0.0917, 0.0663, 0.0805, 0.0640, 0.0568, 0.0448, 0.0563,\n",
       "           0.0501, 0.0432, 0.0906, 0.0698, 0.0809, 0.0745]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0694, 0.0580, 0.0847, 0.0797, 0.0829, 0.0819, 0.0556, 0.0407, 0.0576,\n",
       "          0.0481, 0.0334, 0.0822, 0.0606, 0.0867, 0.0784]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0694, 0.0580, 0.0847, 0.0797, 0.0829, 0.0819, 0.0556, 0.0407, 0.0576,\n",
       "           0.0481, 0.0334, 0.0822, 0.0606, 0.0867, 0.0784]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0654, 0.0626, 0.0833, 0.0705, 0.0709, 0.0710, 0.0591, 0.0426, 0.0604,\n",
       "          0.0589, 0.0445, 0.0825, 0.0725, 0.0834, 0.0724]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0654, 0.0626, 0.0833, 0.0705, 0.0709, 0.0710, 0.0591, 0.0426, 0.0604,\n",
       "           0.0589, 0.0445, 0.0825, 0.0725, 0.0834, 0.0724]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0622, 0.0642, 0.0866, 0.0698, 0.0740, 0.0627, 0.0582, 0.0432, 0.0649,\n",
       "          0.0654, 0.0506, 0.0812, 0.0706, 0.0792, 0.0669]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0622, 0.0642, 0.0866, 0.0698, 0.0740, 0.0627, 0.0582, 0.0432, 0.0649,\n",
       "           0.0654, 0.0506, 0.0812, 0.0706, 0.0792, 0.0669]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0698, 0.0579, 0.0806, 0.0678, 0.0932, 0.0788, 0.0639, 0.0409, 0.0643,\n",
       "          0.0557, 0.0434, 0.0675, 0.0624, 0.0760, 0.0780]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0698, 0.0579, 0.0806, 0.0678, 0.0932, 0.0788, 0.0639, 0.0409, 0.0643,\n",
       "           0.0557, 0.0434, 0.0675, 0.0624, 0.0760, 0.0780]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0713, 0.0504, 0.1011, 0.0915, 0.0937, 0.0798, 0.0521, 0.0422, 0.0549,\n",
       "          0.0444, 0.0322, 0.0669, 0.0578, 0.0861, 0.0756]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0713, 0.0504, 0.1011, 0.0915, 0.0937, 0.0798, 0.0521, 0.0422, 0.0549,\n",
       "           0.0444, 0.0322, 0.0669, 0.0578, 0.0861, 0.0756]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0630, 0.0602, 0.0780, 0.0744, 0.0743, 0.0665, 0.0591, 0.0433, 0.0738,\n",
       "          0.0618, 0.0420, 0.0846, 0.0619, 0.0949, 0.0621]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0630, 0.0602, 0.0780, 0.0744, 0.0743, 0.0665, 0.0591, 0.0433, 0.0738,\n",
       "           0.0618, 0.0420, 0.0846, 0.0619, 0.0949, 0.0621]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0768, 0.0532, 0.0867, 0.0649, 0.1044, 0.0789, 0.0550, 0.0409, 0.0499,\n",
       "          0.0489, 0.0254, 0.0789, 0.0634, 0.0952, 0.0776]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0768, 0.0532, 0.0867, 0.0649, 0.1044, 0.0789, 0.0550, 0.0409, 0.0499,\n",
       "           0.0489, 0.0254, 0.0789, 0.0634, 0.0952, 0.0776]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0777, 0.0529, 0.0897, 0.0820, 0.0980, 0.0784, 0.0609, 0.0336, 0.0465,\n",
       "          0.0509, 0.0322, 0.0693, 0.0568, 0.0856, 0.0855]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0777, 0.0529, 0.0897, 0.0820, 0.0980, 0.0784, 0.0609, 0.0336, 0.0465,\n",
       "           0.0509, 0.0322, 0.0693, 0.0568, 0.0856, 0.0855]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0704, 0.0563, 0.0660, 0.0807, 0.0803, 0.0780, 0.0647, 0.0378, 0.0628,\n",
       "          0.0535, 0.0382, 0.0736, 0.0632, 0.0889, 0.0858]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0704, 0.0563, 0.0660, 0.0807, 0.0803, 0.0780, 0.0647, 0.0378, 0.0628,\n",
       "           0.0535, 0.0382, 0.0736, 0.0632, 0.0889, 0.0858]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0712, 0.0571, 0.0816, 0.0649, 0.0787, 0.0652, 0.0657, 0.0428, 0.0667,\n",
       "          0.0622, 0.0452, 0.0770, 0.0686, 0.0800, 0.0731]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0712, 0.0571, 0.0816, 0.0649, 0.0787, 0.0652, 0.0657, 0.0428, 0.0667,\n",
       "           0.0622, 0.0452, 0.0770, 0.0686, 0.0800, 0.0731]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0761, 0.0473, 0.0960, 0.0725, 0.0802, 0.0905, 0.0518, 0.0386, 0.0674,\n",
       "          0.0348, 0.0370, 0.0858, 0.0580, 0.0850, 0.0790]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0761, 0.0473, 0.0960, 0.0725, 0.0802, 0.0905, 0.0518, 0.0386, 0.0674,\n",
       "           0.0348, 0.0370, 0.0858, 0.0580, 0.0850, 0.0790]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0777, 0.0541, 0.0942, 0.0694, 0.0838, 0.1074, 0.0534, 0.0376, 0.0440,\n",
       "          0.0391, 0.0339, 0.0727, 0.0588, 0.0888, 0.0852]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0777, 0.0541, 0.0942, 0.0694, 0.0838, 0.1074, 0.0534, 0.0376, 0.0440,\n",
       "           0.0391, 0.0339, 0.0727, 0.0588, 0.0888, 0.0852]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0837, 0.0524, 0.0894, 0.0843, 0.0750, 0.0818, 0.0600, 0.0409, 0.0604,\n",
       "          0.0389, 0.0377, 0.0756, 0.0507, 0.0904, 0.0790]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0837, 0.0524, 0.0894, 0.0843, 0.0750, 0.0818, 0.0600, 0.0409, 0.0604,\n",
       "           0.0389, 0.0377, 0.0756, 0.0507, 0.0904, 0.0790]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0775, 0.0490, 0.0862, 0.0815, 0.0613, 0.0771, 0.0630, 0.0360, 0.0855,\n",
       "          0.0541, 0.0386, 0.0808, 0.0535, 0.0861, 0.0699]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0775, 0.0490, 0.0862, 0.0815, 0.0613, 0.0771, 0.0630, 0.0360, 0.0855,\n",
       "           0.0541, 0.0386, 0.0808, 0.0535, 0.0861, 0.0699]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0663, 0.0583, 0.0845, 0.0793, 0.0670, 0.0716, 0.0526, 0.0455, 0.0602,\n",
       "          0.0666, 0.0379, 0.0848, 0.0584, 0.0956, 0.0713]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0663, 0.0583, 0.0845, 0.0793, 0.0670, 0.0716, 0.0526, 0.0455, 0.0602,\n",
       "           0.0666, 0.0379, 0.0848, 0.0584, 0.0956, 0.0713]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0721, 0.0541, 0.0937, 0.0751, 0.0911, 0.0888, 0.0587, 0.0352, 0.0500,\n",
       "          0.0412, 0.0381, 0.0750, 0.0653, 0.0944, 0.0674]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0721, 0.0541, 0.0937, 0.0751, 0.0911, 0.0888, 0.0587, 0.0352, 0.0500,\n",
       "           0.0412, 0.0381, 0.0750, 0.0653, 0.0944, 0.0674]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0719, 0.0560, 0.0774, 0.0739, 0.0953, 0.0850, 0.0564, 0.0410, 0.0643,\n",
       "          0.0535, 0.0359, 0.0664, 0.0603, 0.0873, 0.0754]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0719, 0.0560, 0.0774, 0.0739, 0.0953, 0.0850, 0.0564, 0.0410, 0.0643,\n",
       "           0.0535, 0.0359, 0.0664, 0.0603, 0.0873, 0.0754]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0898, 0.0620, 0.0813, 0.0918, 0.0849, 0.0750, 0.0558, 0.0316, 0.0493,\n",
       "          0.0466, 0.0288, 0.0712, 0.0590, 0.0994, 0.0736]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0898, 0.0620, 0.0813, 0.0918, 0.0849, 0.0750, 0.0558, 0.0316, 0.0493,\n",
       "           0.0466, 0.0288, 0.0712, 0.0590, 0.0994, 0.0736]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0621, 0.0605, 0.0830, 0.0679, 0.0710, 0.0676, 0.0644, 0.0406, 0.0574,\n",
       "          0.0530, 0.0519, 0.0925, 0.0721, 0.0827, 0.0732]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0621, 0.0605, 0.0830, 0.0679, 0.0710, 0.0676, 0.0644, 0.0406, 0.0574,\n",
       "           0.0530, 0.0519, 0.0925, 0.0721, 0.0827, 0.0732]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0728, 0.0571, 0.0931, 0.0722, 0.0736, 0.0764, 0.0591, 0.0315, 0.0575,\n",
       "          0.0448, 0.0359, 0.0940, 0.0584, 0.0975, 0.0762]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0728, 0.0571, 0.0931, 0.0722, 0.0736, 0.0764, 0.0591, 0.0315, 0.0575,\n",
       "           0.0448, 0.0359, 0.0940, 0.0584, 0.0975, 0.0762]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0657, 0.0589, 0.0815, 0.0727, 0.0702, 0.0631, 0.0625, 0.0411, 0.0549,\n",
       "          0.0595, 0.0538, 0.0806, 0.0758, 0.0853, 0.0744]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0657, 0.0589, 0.0815, 0.0727, 0.0702, 0.0631, 0.0625, 0.0411, 0.0549,\n",
       "           0.0595, 0.0538, 0.0806, 0.0758, 0.0853, 0.0744]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0896, 0.0591, 0.0820, 0.0762, 0.0949, 0.0783, 0.0438, 0.0396, 0.0644,\n",
       "          0.0390, 0.0346, 0.0723, 0.0593, 0.0823, 0.0845]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0896, 0.0591, 0.0820, 0.0762, 0.0949, 0.0783, 0.0438, 0.0396, 0.0644,\n",
       "           0.0390, 0.0346, 0.0723, 0.0593, 0.0823, 0.0845]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0872, 0.0411, 0.0751, 0.0729, 0.0983, 0.0993, 0.0509, 0.0331, 0.0461,\n",
       "          0.0435, 0.0291, 0.0690, 0.0651, 0.1010, 0.0883]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0872, 0.0411, 0.0751, 0.0729, 0.0983, 0.0993, 0.0509, 0.0331, 0.0461,\n",
       "           0.0435, 0.0291, 0.0690, 0.0651, 0.1010, 0.0883]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0723, 0.0571, 0.0867, 0.0710, 0.0895, 0.0711, 0.0584, 0.0357, 0.0622,\n",
       "          0.0530, 0.0428, 0.0720, 0.0611, 0.0865, 0.0806]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0723, 0.0571, 0.0867, 0.0710, 0.0895, 0.0711, 0.0584, 0.0357, 0.0622,\n",
       "           0.0530, 0.0428, 0.0720, 0.0611, 0.0865, 0.0806]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0660, 0.0561, 0.1017, 0.0848, 0.0856, 0.0595, 0.0542, 0.0322, 0.0605,\n",
       "          0.0502, 0.0409, 0.0779, 0.0521, 0.1018, 0.0765]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0660, 0.0561, 0.1017, 0.0848, 0.0856, 0.0595, 0.0542, 0.0322, 0.0605,\n",
       "           0.0502, 0.0409, 0.0779, 0.0521, 0.1018, 0.0765]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0751, 0.0616, 0.1044, 0.0797, 0.0858, 0.0728, 0.0575, 0.0310, 0.0594,\n",
       "          0.0385, 0.0308, 0.0777, 0.0505, 0.0941, 0.0810]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0751, 0.0616, 0.1044, 0.0797, 0.0858, 0.0728, 0.0575, 0.0310, 0.0594,\n",
       "           0.0385, 0.0308, 0.0777, 0.0505, 0.0941, 0.0810]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0845, 0.0560, 0.0826, 0.0709, 0.0980, 0.0814, 0.0670, 0.0335, 0.0590,\n",
       "          0.0445, 0.0338, 0.0764, 0.0558, 0.0747, 0.0818]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0845, 0.0560, 0.0826, 0.0709, 0.0980, 0.0814, 0.0670, 0.0335, 0.0590,\n",
       "           0.0445, 0.0338, 0.0764, 0.0558, 0.0747, 0.0818]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0838, 0.0503, 0.0848, 0.0861, 0.0860, 0.0590, 0.0586, 0.0308, 0.0518,\n",
       "          0.0478, 0.0318, 0.0954, 0.0574, 0.1036, 0.0728]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0838, 0.0503, 0.0848, 0.0861, 0.0860, 0.0590, 0.0586, 0.0308, 0.0518,\n",
       "           0.0478, 0.0318, 0.0954, 0.0574, 0.1036, 0.0728]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0638, 0.0658, 0.0725, 0.0686, 0.0732, 0.0660, 0.0602, 0.0400, 0.0671,\n",
       "          0.0676, 0.0465, 0.0850, 0.0721, 0.0757, 0.0759]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0638, 0.0658, 0.0725, 0.0686, 0.0732, 0.0660, 0.0602, 0.0400, 0.0671,\n",
       "           0.0676, 0.0465, 0.0850, 0.0721, 0.0757, 0.0759]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0903, 0.0552, 0.0857, 0.0858, 0.0844, 0.0729, 0.0472, 0.0419, 0.0630,\n",
       "          0.0354, 0.0277, 0.0862, 0.0577, 0.0884, 0.0781]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0903, 0.0552, 0.0857, 0.0858, 0.0844, 0.0729, 0.0472, 0.0419, 0.0630,\n",
       "           0.0354, 0.0277, 0.0862, 0.0577, 0.0884, 0.0781]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0857, 0.0609, 0.0804, 0.0787, 0.0698, 0.0781, 0.0518, 0.0326, 0.0499,\n",
       "          0.0496, 0.0341, 0.0769, 0.0621, 0.1023, 0.0871]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0857, 0.0609, 0.0804, 0.0787, 0.0698, 0.0781, 0.0518, 0.0326, 0.0499,\n",
       "           0.0496, 0.0341, 0.0769, 0.0621, 0.1023, 0.0871]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0752, 0.0586, 0.0701, 0.0732, 0.0591, 0.0546, 0.0590, 0.0311, 0.0584,\n",
       "          0.0494, 0.0413, 0.1221, 0.0853, 0.0894, 0.0731]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0752, 0.0586, 0.0701, 0.0732, 0.0591, 0.0546, 0.0590, 0.0311, 0.0584,\n",
       "           0.0494, 0.0413, 0.1221, 0.0853, 0.0894, 0.0731]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0880, 0.0535, 0.0805, 0.0872, 0.0972, 0.0812, 0.0453, 0.0372, 0.0520,\n",
       "          0.0380, 0.0298, 0.0740, 0.0553, 0.1001, 0.0808]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0880, 0.0535, 0.0805, 0.0872, 0.0972, 0.0812, 0.0453, 0.0372, 0.0520,\n",
       "           0.0380, 0.0298, 0.0740, 0.0553, 0.1001, 0.0808]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0793, 0.0553, 0.1061, 0.0672, 0.0838, 0.0667, 0.0614, 0.0273, 0.0627,\n",
       "          0.0397, 0.0288, 0.0912, 0.0478, 0.1068, 0.0760]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0793, 0.0553, 0.1061, 0.0672, 0.0838, 0.0667, 0.0614, 0.0273, 0.0627,\n",
       "           0.0397, 0.0288, 0.0912, 0.0478, 0.1068, 0.0760]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0801, 0.0557, 0.0935, 0.0942, 0.0842, 0.0779, 0.0456, 0.0355, 0.0603,\n",
       "          0.0330, 0.0301, 0.0805, 0.0484, 0.0958, 0.0851]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0801, 0.0557, 0.0935, 0.0942, 0.0842, 0.0779, 0.0456, 0.0355, 0.0603,\n",
       "           0.0330, 0.0301, 0.0805, 0.0484, 0.0958, 0.0851]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0606, 0.0684, 0.0737, 0.0727, 0.0618, 0.0630, 0.0488, 0.0403, 0.0829,\n",
       "          0.0721, 0.0453, 0.0888, 0.0748, 0.0751, 0.0715]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0606, 0.0684, 0.0737, 0.0727, 0.0618, 0.0630, 0.0488, 0.0403, 0.0829,\n",
       "           0.0721, 0.0453, 0.0888, 0.0748, 0.0751, 0.0715]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0616, 0.0737, 0.0721, 0.0674, 0.0605, 0.0708, 0.0577, 0.0478, 0.0657,\n",
       "          0.0672, 0.0593, 0.0794, 0.0744, 0.0735, 0.0690]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0616, 0.0737, 0.0721, 0.0674, 0.0605, 0.0708, 0.0577, 0.0478, 0.0657,\n",
       "           0.0672, 0.0593, 0.0794, 0.0744, 0.0735, 0.0690]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0618, 0.0678, 0.0942, 0.0765, 0.0662, 0.0692, 0.0569, 0.0309, 0.0601,\n",
       "          0.0567, 0.0420, 0.0832, 0.0621, 0.0879, 0.0845]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0618, 0.0678, 0.0942, 0.0765, 0.0662, 0.0692, 0.0569, 0.0309, 0.0601,\n",
       "           0.0567, 0.0420, 0.0832, 0.0621, 0.0879, 0.0845]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0714, 0.0649, 0.0798, 0.0694, 0.0644, 0.0643, 0.0598, 0.0390, 0.0561,\n",
       "          0.0544, 0.0456, 0.0963, 0.0737, 0.0812, 0.0797]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0714, 0.0649, 0.0798, 0.0694, 0.0644, 0.0643, 0.0598, 0.0390, 0.0561,\n",
       "           0.0544, 0.0456, 0.0963, 0.0737, 0.0812, 0.0797]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.1066, 0.0493, 0.0882, 0.0637, 0.0891, 0.0729, 0.0618, 0.0298, 0.0542,\n",
       "          0.0389, 0.0224, 0.0813, 0.0611, 0.0982, 0.0826]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1066, 0.0493, 0.0882, 0.0637, 0.0891, 0.0729, 0.0618, 0.0298, 0.0542,\n",
       "           0.0389, 0.0224, 0.0813, 0.0611, 0.0982, 0.0826]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0847, 0.0566, 0.0757, 0.0771, 0.0999, 0.0791, 0.0618, 0.0402, 0.0511,\n",
       "          0.0399, 0.0293, 0.0629, 0.0572, 0.0876, 0.0969]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0847, 0.0566, 0.0757, 0.0771, 0.0999, 0.0791, 0.0618, 0.0402, 0.0511,\n",
       "           0.0399, 0.0293, 0.0629, 0.0572, 0.0876, 0.0969]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0782, 0.0664, 0.0937, 0.0716, 0.0795, 0.0781, 0.0516, 0.0389, 0.0635,\n",
       "          0.0520, 0.0357, 0.0683, 0.0713, 0.0800, 0.0713]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0782, 0.0664, 0.0937, 0.0716, 0.0795, 0.0781, 0.0516, 0.0389, 0.0635,\n",
       "           0.0520, 0.0357, 0.0683, 0.0713, 0.0800, 0.0713]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0719, 0.0603, 0.0886, 0.0717, 0.0799, 0.0746, 0.0627, 0.0326, 0.0680,\n",
       "          0.0546, 0.0411, 0.0772, 0.0569, 0.0867, 0.0732]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0719, 0.0603, 0.0886, 0.0717, 0.0799, 0.0746, 0.0627, 0.0326, 0.0680,\n",
       "           0.0546, 0.0411, 0.0772, 0.0569, 0.0867, 0.0732]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0713, 0.0608, 0.0795, 0.0599, 0.0690, 0.0758, 0.0526, 0.0431, 0.0569,\n",
       "          0.0623, 0.0413, 0.0873, 0.0820, 0.0843, 0.0738]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0713, 0.0608, 0.0795, 0.0599, 0.0690, 0.0758, 0.0526, 0.0431, 0.0569,\n",
       "           0.0623, 0.0413, 0.0873, 0.0820, 0.0843, 0.0738]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0954, 0.0489, 0.0779, 0.0723, 0.0761, 0.0839, 0.0577, 0.0410, 0.0596,\n",
       "          0.0505, 0.0301, 0.0704, 0.0620, 0.0953, 0.0790]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0954, 0.0489, 0.0779, 0.0723, 0.0761, 0.0839, 0.0577, 0.0410, 0.0596,\n",
       "           0.0505, 0.0301, 0.0704, 0.0620, 0.0953, 0.0790]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0650, 0.0837, 0.0712, 0.0636, 0.0773, 0.0582, 0.0592, 0.0446, 0.0705,\n",
       "          0.0662, 0.0493, 0.0845, 0.0751, 0.0727, 0.0590]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0650, 0.0837, 0.0712, 0.0636, 0.0773, 0.0582, 0.0592, 0.0446, 0.0705,\n",
       "           0.0662, 0.0493, 0.0845, 0.0751, 0.0727, 0.0590]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0818, 0.0585, 0.0787, 0.0774, 0.0645, 0.0703, 0.0609, 0.0350, 0.0706,\n",
       "          0.0548, 0.0389, 0.0916, 0.0668, 0.0799, 0.0704]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0818, 0.0585, 0.0787, 0.0774, 0.0645, 0.0703, 0.0609, 0.0350, 0.0706,\n",
       "           0.0548, 0.0389, 0.0916, 0.0668, 0.0799, 0.0704]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0731, 0.0602, 0.0913, 0.0714, 0.0754, 0.0749, 0.0552, 0.0399, 0.0572,\n",
       "          0.0482, 0.0375, 0.0774, 0.0629, 0.1069, 0.0685]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0731, 0.0602, 0.0913, 0.0714, 0.0754, 0.0749, 0.0552, 0.0399, 0.0572,\n",
       "           0.0482, 0.0375, 0.0774, 0.0629, 0.1069, 0.0685]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0530, 0.0685, 0.0723, 0.0653, 0.0535, 0.0546, 0.0548, 0.0378, 0.0925,\n",
       "          0.0904, 0.0398, 0.1095, 0.0706, 0.0794, 0.0580]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0530, 0.0685, 0.0723, 0.0653, 0.0535, 0.0546, 0.0548, 0.0378, 0.0925,\n",
       "           0.0904, 0.0398, 0.1095, 0.0706, 0.0794, 0.0580]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0687, 0.0638, 0.0808, 0.0664, 0.0700, 0.0753, 0.0571, 0.0398, 0.0604,\n",
       "          0.0566, 0.0436, 0.0911, 0.0660, 0.0846, 0.0758]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0687, 0.0638, 0.0808, 0.0664, 0.0700, 0.0753, 0.0571, 0.0398, 0.0604,\n",
       "           0.0566, 0.0436, 0.0911, 0.0660, 0.0846, 0.0758]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.1051, 0.0521, 0.0803, 0.0723, 0.0887, 0.0743, 0.0632, 0.0314, 0.0668,\n",
       "          0.0372, 0.0252, 0.0930, 0.0615, 0.0827, 0.0661]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1051, 0.0521, 0.0803, 0.0723, 0.0887, 0.0743, 0.0632, 0.0314, 0.0668,\n",
       "           0.0372, 0.0252, 0.0930, 0.0615, 0.0827, 0.0661]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0913, 0.0563, 0.0917, 0.0925, 0.1112, 0.0860, 0.0441, 0.0326, 0.0567,\n",
       "          0.0412, 0.0211, 0.0573, 0.0560, 0.0803, 0.0818]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0913, 0.0563, 0.0917, 0.0925, 0.1112, 0.0860, 0.0441, 0.0326, 0.0567,\n",
       "           0.0412, 0.0211, 0.0573, 0.0560, 0.0803, 0.0818]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0658, 0.0673, 0.0914, 0.0747, 0.0774, 0.0711, 0.0584, 0.0351, 0.0798,\n",
       "          0.0569, 0.0357, 0.0785, 0.0639, 0.0741, 0.0700]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0658, 0.0673, 0.0914, 0.0747, 0.0774, 0.0711, 0.0584, 0.0351, 0.0798,\n",
       "           0.0569, 0.0357, 0.0785, 0.0639, 0.0741, 0.0700]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0644, 0.0640, 0.0705, 0.0624, 0.0624, 0.0706, 0.0574, 0.0399, 0.0737,\n",
       "          0.0718, 0.0415, 0.0952, 0.0731, 0.0898, 0.0633]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0644, 0.0640, 0.0705, 0.0624, 0.0624, 0.0706, 0.0574, 0.0399, 0.0737,\n",
       "           0.0718, 0.0415, 0.0952, 0.0731, 0.0898, 0.0633]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0775, 0.0585, 0.0878, 0.0780, 0.0651, 0.0714, 0.0600, 0.0339, 0.0566,\n",
       "          0.0597, 0.0415, 0.0806, 0.0705, 0.0884, 0.0705]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0775, 0.0585, 0.0878, 0.0780, 0.0651, 0.0714, 0.0600, 0.0339, 0.0566,\n",
       "           0.0597, 0.0415, 0.0806, 0.0705, 0.0884, 0.0705]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.1000, 0.0520, 0.0844, 0.0817, 0.0886, 0.0754, 0.0502, 0.0317, 0.0522,\n",
       "          0.0451, 0.0270, 0.0842, 0.0553, 0.0984, 0.0739]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1000, 0.0520, 0.0844, 0.0817, 0.0886, 0.0754, 0.0502, 0.0317, 0.0522,\n",
       "           0.0451, 0.0270, 0.0842, 0.0553, 0.0984, 0.0739]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0705, 0.0545, 0.0782, 0.0644, 0.0609, 0.0629, 0.0605, 0.0376, 0.0685,\n",
       "          0.0681, 0.0488, 0.1007, 0.0783, 0.0825, 0.0636]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0705, 0.0545, 0.0782, 0.0644, 0.0609, 0.0629, 0.0605, 0.0376, 0.0685,\n",
       "           0.0681, 0.0488, 0.1007, 0.0783, 0.0825, 0.0636]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0500, 0.0755, 0.0775, 0.0661, 0.0435, 0.0645, 0.0629, 0.0381, 0.0898,\n",
       "          0.0770, 0.0531, 0.1011, 0.0692, 0.0726, 0.0589]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0500, 0.0755, 0.0775, 0.0661, 0.0435, 0.0645, 0.0629, 0.0381, 0.0898,\n",
       "           0.0770, 0.0531, 0.1011, 0.0692, 0.0726, 0.0589]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0848, 0.0595, 0.0929, 0.0746, 0.0712, 0.0687, 0.0611, 0.0304, 0.0645,\n",
       "          0.0615, 0.0306, 0.0805, 0.0742, 0.0815, 0.0639]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0848, 0.0595, 0.0929, 0.0746, 0.0712, 0.0687, 0.0611, 0.0304, 0.0645,\n",
       "           0.0615, 0.0306, 0.0805, 0.0742, 0.0815, 0.0639]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0817, 0.0511, 0.1029, 0.0641, 0.0767, 0.0961, 0.0526, 0.0300, 0.0494,\n",
       "          0.0450, 0.0226, 0.0938, 0.0629, 0.0939, 0.0774]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0817, 0.0511, 0.1029, 0.0641, 0.0767, 0.0961, 0.0526, 0.0300, 0.0494,\n",
       "           0.0450, 0.0226, 0.0938, 0.0629, 0.0939, 0.0774]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0684, 0.0629, 0.1097, 0.0773, 0.0899, 0.0669, 0.0643, 0.0306, 0.0716,\n",
       "          0.0492, 0.0411, 0.0648, 0.0616, 0.0720, 0.0697]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0684, 0.0629, 0.1097, 0.0773, 0.0899, 0.0669, 0.0643, 0.0306, 0.0716,\n",
       "           0.0492, 0.0411, 0.0648, 0.0616, 0.0720, 0.0697]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0622, 0.0597, 0.0642, 0.0731, 0.0614, 0.0647, 0.0659, 0.0421, 0.0739,\n",
       "          0.1058, 0.0441, 0.0808, 0.0796, 0.0676, 0.0549]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0622, 0.0597, 0.0642, 0.0731, 0.0614, 0.0647, 0.0659, 0.0421, 0.0739,\n",
       "           0.1058, 0.0441, 0.0808, 0.0796, 0.0676, 0.0549]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0888, 0.0530, 0.0916, 0.0685, 0.0840, 0.0952, 0.0557, 0.0300, 0.0401,\n",
       "          0.0434, 0.0345, 0.0723, 0.0604, 0.1022, 0.0803]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0888, 0.0530, 0.0916, 0.0685, 0.0840, 0.0952, 0.0557, 0.0300, 0.0401,\n",
       "           0.0434, 0.0345, 0.0723, 0.0604, 0.1022, 0.0803]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0758, 0.0599, 0.0980, 0.0749, 0.0754, 0.0624, 0.0551, 0.0311, 0.0516,\n",
       "          0.0533, 0.0378, 0.0856, 0.0722, 0.0927, 0.0743]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0758, 0.0599, 0.0980, 0.0749, 0.0754, 0.0624, 0.0551, 0.0311, 0.0516,\n",
       "           0.0533, 0.0378, 0.0856, 0.0722, 0.0927, 0.0743]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0611, 0.0589, 0.0891, 0.0652, 0.0589, 0.0706, 0.0624, 0.0384, 0.0576,\n",
       "          0.0678, 0.0465, 0.0899, 0.0849, 0.0674, 0.0814]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0611, 0.0589, 0.0891, 0.0652, 0.0589, 0.0706, 0.0624, 0.0384, 0.0576,\n",
       "           0.0678, 0.0465, 0.0899, 0.0849, 0.0674, 0.0814]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0678, 0.0617, 0.0813, 0.0733, 0.0542, 0.0653, 0.0594, 0.0408, 0.0713,\n",
       "          0.0786, 0.0470, 0.0950, 0.0735, 0.0681, 0.0627]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0678, 0.0617, 0.0813, 0.0733, 0.0542, 0.0653, 0.0594, 0.0408, 0.0713,\n",
       "           0.0786, 0.0470, 0.0950, 0.0735, 0.0681, 0.0627]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0761, 0.0532, 0.1151, 0.0723, 0.0831, 0.0774, 0.0650, 0.0240, 0.0484,\n",
       "          0.0407, 0.0289, 0.0854, 0.0559, 0.0905, 0.0840]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0761, 0.0532, 0.1151, 0.0723, 0.0831, 0.0774, 0.0650, 0.0240, 0.0484,\n",
       "           0.0407, 0.0289, 0.0854, 0.0559, 0.0905, 0.0840]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0795, 0.0492, 0.0985, 0.0729, 0.0776, 0.0785, 0.0627, 0.0313, 0.0575,\n",
       "          0.0518, 0.0326, 0.0851, 0.0604, 0.0845, 0.0780]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0795, 0.0492, 0.0985, 0.0729, 0.0776, 0.0785, 0.0627, 0.0313, 0.0575,\n",
       "           0.0518, 0.0326, 0.0851, 0.0604, 0.0845, 0.0780]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0812, 0.0539, 0.1085, 0.0712, 0.0730, 0.0882, 0.0558, 0.0239, 0.0588,\n",
       "          0.0430, 0.0256, 0.0847, 0.0592, 0.0868, 0.0861]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0812, 0.0539, 0.1085, 0.0712, 0.0730, 0.0882, 0.0558, 0.0239, 0.0588,\n",
       "           0.0430, 0.0256, 0.0847, 0.0592, 0.0868, 0.0861]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0871, 0.0499, 0.1316, 0.0916, 0.0699, 0.0851, 0.0519, 0.0244, 0.0480,\n",
       "          0.0414, 0.0253, 0.0719, 0.0631, 0.0915, 0.0676]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0871, 0.0499, 0.1316, 0.0916, 0.0699, 0.0851, 0.0519, 0.0244, 0.0480,\n",
       "           0.0414, 0.0253, 0.0719, 0.0631, 0.0915, 0.0676]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0880, 0.0569, 0.0894, 0.0869, 0.0633, 0.0782, 0.0562, 0.0325, 0.0682,\n",
       "          0.0492, 0.0290, 0.0795, 0.0690, 0.0811, 0.0726]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0880, 0.0569, 0.0894, 0.0869, 0.0633, 0.0782, 0.0562, 0.0325, 0.0682,\n",
       "           0.0492, 0.0290, 0.0795, 0.0690, 0.0811, 0.0726]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0754, 0.0579, 0.0782, 0.0763, 0.0714, 0.0759, 0.0589, 0.0475, 0.0718,\n",
       "          0.0651, 0.0477, 0.0630, 0.0642, 0.0753, 0.0714]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0754, 0.0579, 0.0782, 0.0763, 0.0714, 0.0759, 0.0589, 0.0475, 0.0718,\n",
       "           0.0651, 0.0477, 0.0630, 0.0642, 0.0753, 0.0714]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0726, 0.0477, 0.0921, 0.0701, 0.0592, 0.0671, 0.0614, 0.0369, 0.0593,\n",
       "          0.0630, 0.0445, 0.0959, 0.0837, 0.0753, 0.0711]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0726, 0.0477, 0.0921, 0.0701, 0.0592, 0.0671, 0.0614, 0.0369, 0.0593,\n",
       "           0.0630, 0.0445, 0.0959, 0.0837, 0.0753, 0.0711]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0987, 0.0674, 0.0853, 0.0951, 0.0865, 0.0761, 0.0581, 0.0304, 0.0576,\n",
       "          0.0588, 0.0272, 0.0628, 0.0493, 0.0818, 0.0649]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0987, 0.0674, 0.0853, 0.0951, 0.0865, 0.0761, 0.0581, 0.0304, 0.0576,\n",
       "           0.0588, 0.0272, 0.0628, 0.0493, 0.0818, 0.0649]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0995, 0.0540, 0.0788, 0.0902, 0.0644, 0.0713, 0.0636, 0.0270, 0.0568,\n",
       "          0.0577, 0.0324, 0.0853, 0.0687, 0.0853, 0.0650]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0995, 0.0540, 0.0788, 0.0902, 0.0644, 0.0713, 0.0636, 0.0270, 0.0568,\n",
       "           0.0577, 0.0324, 0.0853, 0.0687, 0.0853, 0.0650]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0745, 0.0732, 0.0983, 0.0918, 0.0735, 0.0958, 0.0495, 0.0295, 0.0540,\n",
       "          0.0551, 0.0323, 0.0591, 0.0598, 0.0861, 0.0674]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0745, 0.0732, 0.0983, 0.0918, 0.0735, 0.0958, 0.0495, 0.0295, 0.0540,\n",
       "           0.0551, 0.0323, 0.0591, 0.0598, 0.0861, 0.0674]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0653, 0.0607, 0.1039, 0.0831, 0.0589, 0.0618, 0.0595, 0.0266, 0.0629,\n",
       "          0.0689, 0.0417, 0.0980, 0.0660, 0.0870, 0.0558]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0653, 0.0607, 0.1039, 0.0831, 0.0589, 0.0618, 0.0595, 0.0266, 0.0629,\n",
       "           0.0689, 0.0417, 0.0980, 0.0660, 0.0870, 0.0558]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0757, 0.0672, 0.1046, 0.0814, 0.1123, 0.0799, 0.0692, 0.0320, 0.0486,\n",
       "          0.0448, 0.0300, 0.0507, 0.0520, 0.0868, 0.0649]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0757, 0.0672, 0.1046, 0.0814, 0.1123, 0.0799, 0.0692, 0.0320, 0.0486,\n",
       "           0.0448, 0.0300, 0.0507, 0.0520, 0.0868, 0.0649]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0624, 0.0542, 0.1123, 0.0632, 0.0650, 0.0734, 0.0531, 0.0372, 0.0549,\n",
       "          0.0717, 0.0414, 0.0777, 0.0794, 0.0818, 0.0724]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0624, 0.0542, 0.1123, 0.0632, 0.0650, 0.0734, 0.0531, 0.0372, 0.0549,\n",
       "           0.0717, 0.0414, 0.0777, 0.0794, 0.0818, 0.0724]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0549, 0.0576, 0.0854, 0.0626, 0.0577, 0.0612, 0.0579, 0.0373, 0.0712,\n",
       "          0.0899, 0.0518, 0.0890, 0.0833, 0.0705, 0.0698]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0549, 0.0576, 0.0854, 0.0626, 0.0577, 0.0612, 0.0579, 0.0373, 0.0712,\n",
       "           0.0899, 0.0518, 0.0890, 0.0833, 0.0705, 0.0698]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0599, 0.0616, 0.0777, 0.0694, 0.0540, 0.0682, 0.0574, 0.0355, 0.0765,\n",
       "          0.0817, 0.0497, 0.0867, 0.0870, 0.0768, 0.0580]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0599, 0.0616, 0.0777, 0.0694, 0.0540, 0.0682, 0.0574, 0.0355, 0.0765,\n",
       "           0.0817, 0.0497, 0.0867, 0.0870, 0.0768, 0.0580]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0635, 0.0566, 0.0789, 0.0600, 0.0627, 0.0655, 0.0722, 0.0362, 0.0829,\n",
       "          0.0942, 0.0524, 0.0772, 0.0769, 0.0695, 0.0512]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0635, 0.0566, 0.0789, 0.0600, 0.0627, 0.0655, 0.0722, 0.0362, 0.0829,\n",
       "           0.0942, 0.0524, 0.0772, 0.0769, 0.0695, 0.0512]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0836, 0.0547, 0.1080, 0.0860, 0.0647, 0.0793, 0.0672, 0.0245, 0.0612,\n",
       "          0.0539, 0.0354, 0.0644, 0.0699, 0.0829, 0.0644]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0836, 0.0547, 0.1080, 0.0860, 0.0647, 0.0793, 0.0672, 0.0245, 0.0612,\n",
       "           0.0539, 0.0354, 0.0644, 0.0699, 0.0829, 0.0644]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0743, 0.0517, 0.1068, 0.0745, 0.0798, 0.0812, 0.0559, 0.0200, 0.0671,\n",
       "          0.0484, 0.0238, 0.0756, 0.0523, 0.1151, 0.0736]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0743, 0.0517, 0.1068, 0.0745, 0.0798, 0.0812, 0.0559, 0.0200, 0.0671,\n",
       "           0.0484, 0.0238, 0.0756, 0.0523, 0.1151, 0.0736]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0917, 0.0450, 0.1068, 0.0677, 0.0676, 0.0978, 0.0542, 0.0297, 0.0637,\n",
       "          0.0505, 0.0290, 0.0666, 0.0727, 0.0945, 0.0622]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0917, 0.0450, 0.1068, 0.0677, 0.0676, 0.0978, 0.0542, 0.0297, 0.0637,\n",
       "           0.0505, 0.0290, 0.0666, 0.0727, 0.0945, 0.0622]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0849, 0.0490, 0.0816, 0.0707, 0.0611, 0.0950, 0.0625, 0.0335, 0.0754,\n",
       "          0.0583, 0.0356, 0.0738, 0.0543, 0.0843, 0.0799]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0849, 0.0490, 0.0816, 0.0707, 0.0611, 0.0950, 0.0625, 0.0335, 0.0754,\n",
       "           0.0583, 0.0356, 0.0738, 0.0543, 0.0843, 0.0799]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0553, 0.0451, 0.0802, 0.0682, 0.0480, 0.0597, 0.0637, 0.0239, 0.0870,\n",
       "          0.1313, 0.0458, 0.0782, 0.0765, 0.0829, 0.0542]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0553, 0.0451, 0.0802, 0.0682, 0.0480, 0.0597, 0.0637, 0.0239, 0.0870,\n",
       "           0.1313, 0.0458, 0.0782, 0.0765, 0.0829, 0.0542]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0703, 0.0472, 0.1165, 0.0640, 0.0500, 0.0622, 0.0649, 0.0275, 0.0726,\n",
       "          0.1021, 0.0408, 0.0891, 0.0570, 0.0830, 0.0528]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0703, 0.0472, 0.1165, 0.0640, 0.0500, 0.0622, 0.0649, 0.0275, 0.0726,\n",
       "           0.1021, 0.0408, 0.0891, 0.0570, 0.0830, 0.0528]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1021, 0.0431, 0.1238, 0.0842, 0.0564, 0.0801, 0.0464, 0.0182, 0.0497,\n",
       "          0.0456, 0.0225, 0.0810, 0.0685, 0.1115, 0.0668]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1021, 0.0431, 0.1238, 0.0842, 0.0564, 0.0801, 0.0464, 0.0182, 0.0497,\n",
       "           0.0456, 0.0225, 0.0810, 0.0685, 0.1115, 0.0668]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0693, 0.0561, 0.1031, 0.0792, 0.0715, 0.0673, 0.0593, 0.0300, 0.0600,\n",
       "          0.0624, 0.0368, 0.0796, 0.0630, 0.0878, 0.0746]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0693, 0.0561, 0.1031, 0.0792, 0.0715, 0.0673, 0.0593, 0.0300, 0.0600,\n",
       "           0.0624, 0.0368, 0.0796, 0.0630, 0.0878, 0.0746]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0892, 0.0470, 0.1081, 0.0777, 0.0703, 0.0846, 0.0653, 0.0366, 0.0579,\n",
       "          0.0480, 0.0373, 0.0683, 0.0574, 0.0887, 0.0637]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0892, 0.0470, 0.1081, 0.0777, 0.0703, 0.0846, 0.0653, 0.0366, 0.0579,\n",
       "           0.0480, 0.0373, 0.0683, 0.0574, 0.0887, 0.0637]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0660, 0.0501, 0.0835, 0.0723, 0.0503, 0.0611, 0.0608, 0.0312, 0.0540,\n",
       "          0.0940, 0.0455, 0.1078, 0.0863, 0.0839, 0.0533]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0660, 0.0501, 0.0835, 0.0723, 0.0503, 0.0611, 0.0608, 0.0312, 0.0540,\n",
       "           0.0940, 0.0455, 0.1078, 0.0863, 0.0839, 0.0533]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0672, 0.0447, 0.0964, 0.0711, 0.0629, 0.0574, 0.0696, 0.0304, 0.0859,\n",
       "          0.0886, 0.0336, 0.0708, 0.0634, 0.0886, 0.0694]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0672, 0.0447, 0.0964, 0.0711, 0.0629, 0.0574, 0.0696, 0.0304, 0.0859,\n",
       "           0.0886, 0.0336, 0.0708, 0.0634, 0.0886, 0.0694]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0531, 0.0458, 0.0868, 0.0640, 0.0421, 0.0641, 0.0453, 0.0292, 0.0558,\n",
       "          0.0510, 0.0407, 0.1593, 0.1223, 0.0847, 0.0559]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0531, 0.0458, 0.0868, 0.0640, 0.0421, 0.0641, 0.0453, 0.0292, 0.0558,\n",
       "           0.0510, 0.0407, 0.1593, 0.1223, 0.0847, 0.0559]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0677, 0.0483, 0.1000, 0.0663, 0.0530, 0.0662, 0.0498, 0.0298, 0.0628,\n",
       "          0.0663, 0.0463, 0.1082, 0.0890, 0.0935, 0.0529]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0677, 0.0483, 0.1000, 0.0663, 0.0530, 0.0662, 0.0498, 0.0298, 0.0628,\n",
       "           0.0663, 0.0463, 0.1082, 0.0890, 0.0935, 0.0529]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0517, 0.0462, 0.1018, 0.0509, 0.0522, 0.0584, 0.0621, 0.0287, 0.0611,\n",
       "          0.0854, 0.0468, 0.1007, 0.1092, 0.0851, 0.0599]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0517, 0.0462, 0.1018, 0.0509, 0.0522, 0.0584, 0.0621, 0.0287, 0.0611,\n",
       "           0.0854, 0.0468, 0.1007, 0.1092, 0.0851, 0.0599]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0872, 0.0490, 0.1051, 0.0628, 0.0776, 0.0836, 0.0573, 0.0351, 0.0662,\n",
       "          0.0591, 0.0244, 0.0682, 0.0573, 0.0949, 0.0721]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0872, 0.0490, 0.1051, 0.0628, 0.0776, 0.0836, 0.0573, 0.0351, 0.0662,\n",
       "           0.0591, 0.0244, 0.0682, 0.0573, 0.0949, 0.0721]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0737, 0.0541, 0.1012, 0.0809, 0.0881, 0.0753, 0.0588, 0.0239, 0.0604,\n",
       "          0.0634, 0.0299, 0.0623, 0.0706, 0.0926, 0.0649]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0737, 0.0541, 0.1012, 0.0809, 0.0881, 0.0753, 0.0588, 0.0239, 0.0604,\n",
       "           0.0634, 0.0299, 0.0623, 0.0706, 0.0926, 0.0649]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0577, 0.0545, 0.0792, 0.0727, 0.0599, 0.0538, 0.0558, 0.0344, 0.0816,\n",
       "          0.1180, 0.0492, 0.0731, 0.0786, 0.0767, 0.0547]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0577, 0.0545, 0.0792, 0.0727, 0.0599, 0.0538, 0.0558, 0.0344, 0.0816,\n",
       "           0.1180, 0.0492, 0.0731, 0.0786, 0.0767, 0.0547]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0780, 0.0381, 0.1639, 0.0912, 0.0684, 0.0736, 0.0563, 0.0150, 0.0501,\n",
       "          0.0402, 0.0193, 0.0716, 0.0542, 0.1205, 0.0595]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0780, 0.0381, 0.1639, 0.0912, 0.0684, 0.0736, 0.0563, 0.0150, 0.0501,\n",
       "           0.0402, 0.0193, 0.0716, 0.0542, 0.1205, 0.0595]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0831, 0.0432, 0.0932, 0.0836, 0.0579, 0.0849, 0.0602, 0.0305, 0.0588,\n",
       "          0.0776, 0.0361, 0.0712, 0.0772, 0.0910, 0.0515]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0831, 0.0432, 0.0932, 0.0836, 0.0579, 0.0849, 0.0602, 0.0305, 0.0588,\n",
       "           0.0776, 0.0361, 0.0712, 0.0772, 0.0910, 0.0515]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0763, 0.0604, 0.0941, 0.1001, 0.0681, 0.0988, 0.0472, 0.0317, 0.0537,\n",
       "          0.0452, 0.0298, 0.0699, 0.0777, 0.0800, 0.0670]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0763, 0.0604, 0.0941, 0.1001, 0.0681, 0.0988, 0.0472, 0.0317, 0.0537,\n",
       "           0.0452, 0.0298, 0.0699, 0.0777, 0.0800, 0.0670]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0668, 0.0611, 0.0884, 0.0691, 0.0726, 0.0851, 0.0512, 0.0306, 0.0726,\n",
       "          0.0566, 0.0393, 0.0667, 0.0927, 0.0813, 0.0661]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0668, 0.0611, 0.0884, 0.0691, 0.0726, 0.0851, 0.0512, 0.0306, 0.0726,\n",
       "           0.0566, 0.0393, 0.0667, 0.0927, 0.0813, 0.0661]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0575, 0.0565, 0.0906, 0.0735, 0.0558, 0.0610, 0.0575, 0.0356, 0.0644,\n",
       "          0.0695, 0.0479, 0.1066, 0.0993, 0.0722, 0.0521]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0575, 0.0565, 0.0906, 0.0735, 0.0558, 0.0610, 0.0575, 0.0356, 0.0644,\n",
       "           0.0695, 0.0479, 0.1066, 0.0993, 0.0722, 0.0521]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0562, 0.0468, 0.0940, 0.0690, 0.0464, 0.0516, 0.0701, 0.0297, 0.0752,\n",
       "          0.1013, 0.0533, 0.0917, 0.0822, 0.0825, 0.0500]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0562, 0.0468, 0.0940, 0.0690, 0.0464, 0.0516, 0.0701, 0.0297, 0.0752,\n",
       "           0.1013, 0.0533, 0.0917, 0.0822, 0.0825, 0.0500]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0511, 0.0571, 0.0710, 0.0561, 0.0481, 0.0610, 0.0578, 0.0377, 0.0904,\n",
       "          0.1170, 0.0665, 0.0740, 0.1090, 0.0571, 0.0462]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0511, 0.0571, 0.0710, 0.0561, 0.0481, 0.0610, 0.0578, 0.0377, 0.0904,\n",
       "           0.1170, 0.0665, 0.0740, 0.1090, 0.0571, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0666, 0.0461, 0.1084, 0.0657, 0.0935, 0.0727, 0.0525, 0.0194, 0.0759,\n",
       "          0.0514, 0.0291, 0.0927, 0.0701, 0.0976, 0.0583]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0666, 0.0461, 0.1084, 0.0657, 0.0935, 0.0727, 0.0525, 0.0194, 0.0759,\n",
       "           0.0514, 0.0291, 0.0927, 0.0701, 0.0976, 0.0583]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0472, 0.0537, 0.0774, 0.0641, 0.0437, 0.0486, 0.0529, 0.0293, 0.0811,\n",
       "          0.1560, 0.0464, 0.0957, 0.1046, 0.0621, 0.0371]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0472, 0.0537, 0.0774, 0.0641, 0.0437, 0.0486, 0.0529, 0.0293, 0.0811,\n",
       "           0.1560, 0.0464, 0.0957, 0.1046, 0.0621, 0.0371]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0622, 0.0474, 0.1019, 0.0643, 0.0587, 0.0760, 0.0578, 0.0294, 0.0578,\n",
       "          0.0680, 0.0443, 0.0989, 0.0897, 0.0862, 0.0573]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0622, 0.0474, 0.1019, 0.0643, 0.0587, 0.0760, 0.0578, 0.0294, 0.0578,\n",
       "           0.0680, 0.0443, 0.0989, 0.0897, 0.0862, 0.0573]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0411, 0.0563, 0.0685, 0.0555, 0.0549, 0.0441, 0.0495, 0.0438, 0.1177,\n",
       "          0.1281, 0.0572, 0.0643, 0.1019, 0.0619, 0.0552]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0411, 0.0563, 0.0685, 0.0555, 0.0549, 0.0441, 0.0495, 0.0438, 0.1177,\n",
       "           0.1281, 0.0572, 0.0643, 0.1019, 0.0619, 0.0552]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0735, 0.0357, 0.1185, 0.0753, 0.0755, 0.0891, 0.0575, 0.0294, 0.0579,\n",
       "          0.0561, 0.0308, 0.0681, 0.0945, 0.0817, 0.0562]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0735, 0.0357, 0.1185, 0.0753, 0.0755, 0.0891, 0.0575, 0.0294, 0.0579,\n",
       "           0.0561, 0.0308, 0.0681, 0.0945, 0.0817, 0.0562]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0511, 0.0453, 0.0751, 0.0525, 0.0471, 0.0653, 0.0649, 0.0297, 0.0987,\n",
       "          0.1627, 0.0502, 0.0749, 0.0946, 0.0423, 0.0456]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0511, 0.0453, 0.0751, 0.0525, 0.0471, 0.0653, 0.0649, 0.0297, 0.0987,\n",
       "           0.1627, 0.0502, 0.0749, 0.0946, 0.0423, 0.0456]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0478, 0.0393, 0.0917, 0.0545, 0.0492, 0.0501, 0.0503, 0.0247, 0.0738,\n",
       "          0.1054, 0.0422, 0.1113, 0.1501, 0.0675, 0.0420]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0478, 0.0393, 0.0917, 0.0545, 0.0492, 0.0501, 0.0503, 0.0247, 0.0738,\n",
       "           0.1054, 0.0422, 0.1113, 0.1501, 0.0675, 0.0420]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0790, 0.0320, 0.1319, 0.0875, 0.0755, 0.0881, 0.0465, 0.0199, 0.0673,\n",
       "          0.0535, 0.0192, 0.0694, 0.0786, 0.1008, 0.0509]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0790, 0.0320, 0.1319, 0.0875, 0.0755, 0.0881, 0.0465, 0.0199, 0.0673,\n",
       "           0.0535, 0.0192, 0.0694, 0.0786, 0.1008, 0.0509]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0603, 0.0597, 0.0862, 0.0656, 0.0587, 0.0643, 0.0594, 0.0429, 0.0725,\n",
       "          0.0811, 0.0644, 0.0788, 0.0829, 0.0676, 0.0555]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0603, 0.0597, 0.0862, 0.0656, 0.0587, 0.0643, 0.0594, 0.0429, 0.0725,\n",
       "           0.0811, 0.0644, 0.0788, 0.0829, 0.0676, 0.0555]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0793, 0.0399, 0.1065, 0.0835, 0.0708, 0.0902, 0.0513, 0.0311, 0.0618,\n",
       "          0.0589, 0.0227, 0.0703, 0.0908, 0.0809, 0.0620]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0793, 0.0399, 0.1065, 0.0835, 0.0708, 0.0902, 0.0513, 0.0311, 0.0618,\n",
       "           0.0589, 0.0227, 0.0703, 0.0908, 0.0809, 0.0620]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0417, 0.0506, 0.1020, 0.0493, 0.0528, 0.0445, 0.0422, 0.0317, 0.1179,\n",
       "          0.1748, 0.0525, 0.0761, 0.0742, 0.0538, 0.0361]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0417, 0.0506, 0.1020, 0.0493, 0.0528, 0.0445, 0.0422, 0.0317, 0.1179,\n",
       "           0.1748, 0.0525, 0.0761, 0.0742, 0.0538, 0.0361]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0568, 0.0510, 0.1088, 0.0761, 0.0733, 0.0684, 0.0754, 0.0295, 0.0759,\n",
       "          0.0710, 0.0352, 0.0736, 0.0828, 0.0689, 0.0532]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0568, 0.0510, 0.1088, 0.0761, 0.0733, 0.0684, 0.0754, 0.0295, 0.0759,\n",
       "           0.0710, 0.0352, 0.0736, 0.0828, 0.0689, 0.0532]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0559, 0.0551, 0.0907, 0.0496, 0.0539, 0.0543, 0.0546, 0.0291, 0.0747,\n",
       "          0.1008, 0.0481, 0.1256, 0.0927, 0.0651, 0.0497]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0559, 0.0551, 0.0907, 0.0496, 0.0539, 0.0543, 0.0546, 0.0291, 0.0747,\n",
       "           0.1008, 0.0481, 0.1256, 0.0927, 0.0651, 0.0497]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0738, 0.0405, 0.1605, 0.0638, 0.0742, 0.0749, 0.0573, 0.0275, 0.0740,\n",
       "          0.0538, 0.0274, 0.0603, 0.0542, 0.0899, 0.0680]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0738, 0.0405, 0.1605, 0.0638, 0.0742, 0.0749, 0.0573, 0.0275, 0.0740,\n",
       "           0.0538, 0.0274, 0.0603, 0.0542, 0.0899, 0.0680]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0843, 0.0345, 0.1396, 0.0728, 0.0833, 0.0777, 0.0457, 0.0215, 0.0555,\n",
       "          0.0432, 0.0278, 0.0678, 0.0679, 0.0965, 0.0818]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0843, 0.0345, 0.1396, 0.0728, 0.0833, 0.0777, 0.0457, 0.0215, 0.0555,\n",
       "           0.0432, 0.0278, 0.0678, 0.0679, 0.0965, 0.0818]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0632, 0.0432, 0.0806, 0.0531, 0.0547, 0.0585, 0.0690, 0.0242, 0.0859,\n",
       "          0.1241, 0.0360, 0.0879, 0.0961, 0.0645, 0.0590]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0632, 0.0432, 0.0806, 0.0531, 0.0547, 0.0585, 0.0690, 0.0242, 0.0859,\n",
       "           0.1241, 0.0360, 0.0879, 0.0961, 0.0645, 0.0590]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0683, 0.0405, 0.0884, 0.0661, 0.0678, 0.0642, 0.0638, 0.0247, 0.0862,\n",
       "          0.0982, 0.0312, 0.0964, 0.0865, 0.0633, 0.0544]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0683, 0.0405, 0.0884, 0.0661, 0.0678, 0.0642, 0.0638, 0.0247, 0.0862,\n",
       "           0.0982, 0.0312, 0.0964, 0.0865, 0.0633, 0.0544]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0461, 0.0548, 0.1007, 0.0686, 0.0668, 0.0546, 0.0582, 0.0247, 0.0909,\n",
       "          0.1295, 0.0418, 0.0601, 0.0737, 0.0740, 0.0555]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0461, 0.0548, 0.1007, 0.0686, 0.0668, 0.0546, 0.0582, 0.0247, 0.0909,\n",
       "           0.1295, 0.0418, 0.0601, 0.0737, 0.0740, 0.0555]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0430, 0.0605, 0.0676, 0.0412, 0.0404, 0.0475, 0.0419, 0.0260, 0.0745,\n",
       "          0.1220, 0.0576, 0.1241, 0.1585, 0.0583, 0.0367]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0430, 0.0605, 0.0676, 0.0412, 0.0404, 0.0475, 0.0419, 0.0260, 0.0745,\n",
       "           0.1220, 0.0576, 0.1241, 0.1585, 0.0583, 0.0367]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0500, 0.0581, 0.1211, 0.0651, 0.0912, 0.0857, 0.0489, 0.0317, 0.0721,\n",
       "          0.0759, 0.0319, 0.0731, 0.0747, 0.0639, 0.0565]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0500, 0.0581, 0.1211, 0.0651, 0.0912, 0.0857, 0.0489, 0.0317, 0.0721,\n",
       "           0.0759, 0.0319, 0.0731, 0.0747, 0.0639, 0.0565]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0613, 0.0349, 0.1543, 0.0676, 0.0688, 0.0656, 0.0523, 0.0250, 0.0790,\n",
       "          0.0810, 0.0283, 0.0756, 0.0858, 0.0678, 0.0527]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0613, 0.0349, 0.1543, 0.0676, 0.0688, 0.0656, 0.0523, 0.0250, 0.0790,\n",
       "           0.0810, 0.0283, 0.0756, 0.0858, 0.0678, 0.0527]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0441, 0.0349, 0.0705, 0.0532, 0.0399, 0.0451, 0.0497, 0.0189, 0.1170,\n",
       "          0.2372, 0.0385, 0.0912, 0.0918, 0.0421, 0.0259]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0441, 0.0349, 0.0705, 0.0532, 0.0399, 0.0451, 0.0497, 0.0189, 0.1170,\n",
       "           0.2372, 0.0385, 0.0912, 0.0918, 0.0421, 0.0259]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0456, 0.0446, 0.0681, 0.0603, 0.0335, 0.0427, 0.0504, 0.0253, 0.1123,\n",
       "          0.1955, 0.0525, 0.0948, 0.0913, 0.0506, 0.0327]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0456, 0.0446, 0.0681, 0.0603, 0.0335, 0.0427, 0.0504, 0.0253, 0.1123,\n",
       "           0.1955, 0.0525, 0.0948, 0.0913, 0.0506, 0.0327]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0358, 0.0275, 0.0540, 0.0453, 0.0356, 0.0385, 0.0333, 0.0187, 0.1293,\n",
       "          0.2886, 0.0357, 0.0795, 0.1115, 0.0351, 0.0316]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0358, 0.0275, 0.0540, 0.0453, 0.0356, 0.0385, 0.0333, 0.0187, 0.1293,\n",
       "           0.2886, 0.0357, 0.0795, 0.1115, 0.0351, 0.0316]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0522, 0.0442, 0.0900, 0.0647, 0.0537, 0.0765, 0.0522, 0.0300, 0.0927,\n",
       "          0.0923, 0.0423, 0.0842, 0.1056, 0.0637, 0.0557]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0522, 0.0442, 0.0900, 0.0647, 0.0537, 0.0765, 0.0522, 0.0300, 0.0927,\n",
       "           0.0923, 0.0423, 0.0842, 0.1056, 0.0637, 0.0557]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0392, 0.0357, 0.0997, 0.0479, 0.0494, 0.0463, 0.0403, 0.0284, 0.0643,\n",
       "          0.1823, 0.0617, 0.0819, 0.1043, 0.0726, 0.0462]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0392, 0.0357, 0.0997, 0.0479, 0.0494, 0.0463, 0.0403, 0.0284, 0.0643,\n",
       "           0.1823, 0.0617, 0.0819, 0.1043, 0.0726, 0.0462]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0593, 0.0384, 0.1552, 0.0638, 0.0656, 0.0738, 0.0484, 0.0234, 0.0672,\n",
       "          0.0785, 0.0253, 0.0860, 0.0894, 0.0671, 0.0587]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0593, 0.0384, 0.1552, 0.0638, 0.0656, 0.0738, 0.0484, 0.0234, 0.0672,\n",
       "           0.0785, 0.0253, 0.0860, 0.0894, 0.0671, 0.0587]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0555, 0.0553, 0.1110, 0.0688, 0.0806, 0.0743, 0.0396, 0.0371, 0.0792,\n",
       "          0.0762, 0.0374, 0.0585, 0.0808, 0.0753, 0.0705]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0555, 0.0553, 0.1110, 0.0688, 0.0806, 0.0743, 0.0396, 0.0371, 0.0792,\n",
       "           0.0762, 0.0374, 0.0585, 0.0808, 0.0753, 0.0705]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0653, 0.0361, 0.1413, 0.0741, 0.0617, 0.0787, 0.0507, 0.0225, 0.0828,\n",
       "          0.0729, 0.0226, 0.0792, 0.0819, 0.0743, 0.0558]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0653, 0.0361, 0.1413, 0.0741, 0.0617, 0.0787, 0.0507, 0.0225, 0.0828,\n",
       "           0.0729, 0.0226, 0.0792, 0.0819, 0.0743, 0.0558]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0814, 0.0391, 0.1360, 0.0727, 0.0683, 0.0935, 0.0449, 0.0272, 0.0643,\n",
       "          0.0650, 0.0289, 0.0594, 0.0787, 0.0777, 0.0628]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0814, 0.0391, 0.1360, 0.0727, 0.0683, 0.0935, 0.0449, 0.0272, 0.0643,\n",
       "           0.0650, 0.0289, 0.0594, 0.0787, 0.0777, 0.0628]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0351, 0.0399, 0.0755, 0.0436, 0.0437, 0.0449, 0.0402, 0.0281, 0.0903,\n",
       "          0.1688, 0.0534, 0.1116, 0.1431, 0.0427, 0.0391]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0351, 0.0399, 0.0755, 0.0436, 0.0437, 0.0449, 0.0402, 0.0281, 0.0903,\n",
       "           0.1688, 0.0534, 0.1116, 0.1431, 0.0427, 0.0391]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0416, 0.0319, 0.1067, 0.0413, 0.0621, 0.0504, 0.0439, 0.0229, 0.1151,\n",
       "          0.1517, 0.0353, 0.0921, 0.1113, 0.0523, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0416, 0.0319, 0.1067, 0.0413, 0.0621, 0.0504, 0.0439, 0.0229, 0.1151,\n",
       "           0.1517, 0.0353, 0.0921, 0.1113, 0.0523, 0.0414]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0804, 0.0317, 0.1519, 0.0603, 0.0860, 0.0917, 0.0452, 0.0208, 0.0577,\n",
       "          0.0513, 0.0179, 0.0759, 0.0743, 0.0852, 0.0697]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0804, 0.0317, 0.1519, 0.0603, 0.0860, 0.0917, 0.0452, 0.0208, 0.0577,\n",
       "           0.0513, 0.0179, 0.0759, 0.0743, 0.0852, 0.0697]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0722, 0.0396, 0.1125, 0.0698, 0.0718, 0.0603, 0.0599, 0.0278, 0.0814,\n",
       "          0.0646, 0.0293, 0.0955, 0.0898, 0.0746, 0.0509]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0722, 0.0396, 0.1125, 0.0698, 0.0718, 0.0603, 0.0599, 0.0278, 0.0814,\n",
       "           0.0646, 0.0293, 0.0955, 0.0898, 0.0746, 0.0509]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0575, 0.0369, 0.1541, 0.0496, 0.0856, 0.0760, 0.0369, 0.0239, 0.0783,\n",
       "          0.1048, 0.0260, 0.0601, 0.0790, 0.0675, 0.0636]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0575, 0.0369, 0.1541, 0.0496, 0.0856, 0.0760, 0.0369, 0.0239, 0.0783,\n",
       "           0.1048, 0.0260, 0.0601, 0.0790, 0.0675, 0.0636]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0311, 0.0308, 0.0948, 0.0286, 0.0394, 0.0360, 0.0300, 0.0244, 0.0598,\n",
       "          0.0997, 0.0534, 0.1600, 0.2184, 0.0437, 0.0499]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0311, 0.0308, 0.0948, 0.0286, 0.0394, 0.0360, 0.0300, 0.0244, 0.0598,\n",
       "           0.0997, 0.0534, 0.1600, 0.2184, 0.0437, 0.0499]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0524, 0.0386, 0.0927, 0.0731, 0.0664, 0.0545, 0.0558, 0.0280, 0.0859,\n",
       "          0.1248, 0.0340, 0.0692, 0.0822, 0.0847, 0.0577]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0524, 0.0386, 0.0927, 0.0731, 0.0664, 0.0545, 0.0558, 0.0280, 0.0859,\n",
       "           0.1248, 0.0340, 0.0692, 0.0822, 0.0847, 0.0577]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0466, 0.0402, 0.0794, 0.0581, 0.0572, 0.0594, 0.0481, 0.0323, 0.0657,\n",
       "          0.1185, 0.0409, 0.1185, 0.1314, 0.0503, 0.0534]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0466, 0.0402, 0.0794, 0.0581, 0.0572, 0.0594, 0.0481, 0.0323, 0.0657,\n",
       "           0.1185, 0.0409, 0.1185, 0.1314, 0.0503, 0.0534]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0610, 0.0342, 0.1092, 0.0586, 0.0866, 0.0718, 0.0535, 0.0248, 0.0867,\n",
       "          0.1011, 0.0272, 0.0879, 0.0696, 0.0747, 0.0532]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0610, 0.0342, 0.1092, 0.0586, 0.0866, 0.0718, 0.0535, 0.0248, 0.0867,\n",
       "           0.1011, 0.0272, 0.0879, 0.0696, 0.0747, 0.0532]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0702, 0.0470, 0.1230, 0.0585, 0.0735, 0.0707, 0.0509, 0.0277, 0.0692,\n",
       "          0.0871, 0.0308, 0.0852, 0.0881, 0.0605, 0.0575]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0702, 0.0470, 0.1230, 0.0585, 0.0735, 0.0707, 0.0509, 0.0277, 0.0692,\n",
       "           0.0871, 0.0308, 0.0852, 0.0881, 0.0605, 0.0575]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0735, 0.0415, 0.1715, 0.0798, 0.1110, 0.0686, 0.0444, 0.0173, 0.0558,\n",
       "          0.0343, 0.0196, 0.0717, 0.0619, 0.0742, 0.0750]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0735, 0.0415, 0.1715, 0.0798, 0.1110, 0.0686, 0.0444, 0.0173, 0.0558,\n",
       "           0.0343, 0.0196, 0.0717, 0.0619, 0.0742, 0.0750]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0650, 0.0452, 0.1516, 0.0549, 0.0962, 0.0693, 0.0520, 0.0185, 0.0503,\n",
       "          0.0775, 0.0227, 0.0825, 0.0730, 0.0782, 0.0630]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0650, 0.0452, 0.1516, 0.0549, 0.0962, 0.0693, 0.0520, 0.0185, 0.0503,\n",
       "           0.0775, 0.0227, 0.0825, 0.0730, 0.0782, 0.0630]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0947, 0.0363, 0.1374, 0.0865, 0.1122, 0.0518, 0.0442, 0.0160, 0.0652,\n",
       "          0.0432, 0.0174, 0.0765, 0.0623, 0.0883, 0.0680]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0947, 0.0363, 0.1374, 0.0865, 0.1122, 0.0518, 0.0442, 0.0160, 0.0652,\n",
       "           0.0432, 0.0174, 0.0765, 0.0623, 0.0883, 0.0680]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0621, 0.0331, 0.1009, 0.0582, 0.0706, 0.0615, 0.0530, 0.0229, 0.0871,\n",
       "          0.1349, 0.0295, 0.0874, 0.0867, 0.0573, 0.0550]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0621, 0.0331, 0.1009, 0.0582, 0.0706, 0.0615, 0.0530, 0.0229, 0.0871,\n",
       "           0.1349, 0.0295, 0.0874, 0.0867, 0.0573, 0.0550]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0816, 0.0407, 0.1563, 0.0711, 0.1181, 0.0862, 0.0383, 0.0211, 0.0422,\n",
       "          0.0347, 0.0190, 0.0675, 0.0494, 0.1008, 0.0730]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0816, 0.0407, 0.1563, 0.0711, 0.1181, 0.0862, 0.0383, 0.0211, 0.0422,\n",
       "           0.0347, 0.0190, 0.0675, 0.0494, 0.1008, 0.0730]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0790, 0.0449, 0.0911, 0.0574, 0.0908, 0.0613, 0.0500, 0.0222, 0.0862,\n",
       "          0.1185, 0.0280, 0.0824, 0.0660, 0.0610, 0.0614]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0790, 0.0449, 0.0911, 0.0574, 0.0908, 0.0613, 0.0500, 0.0222, 0.0862,\n",
       "           0.1185, 0.0280, 0.0824, 0.0660, 0.0610, 0.0614]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0676, 0.0314, 0.1931, 0.0583, 0.0972, 0.0584, 0.0412, 0.0160, 0.0465,\n",
       "          0.0752, 0.0186, 0.0831, 0.0693, 0.0763, 0.0678]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0676, 0.0314, 0.1931, 0.0583, 0.0972, 0.0584, 0.0412, 0.0160, 0.0465,\n",
       "           0.0752, 0.0186, 0.0831, 0.0693, 0.0763, 0.0678]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0344, 0.0272, 0.0662, 0.0352, 0.0304, 0.0231, 0.0281, 0.0124, 0.1309,\n",
       "          0.3172, 0.0508, 0.1079, 0.0738, 0.0361, 0.0260]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0344, 0.0272, 0.0662, 0.0352, 0.0304, 0.0231, 0.0281, 0.0124, 0.1309,\n",
       "           0.3172, 0.0508, 0.1079, 0.0738, 0.0361, 0.0260]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0578, 0.0322, 0.0838, 0.0494, 0.0571, 0.0516, 0.0469, 0.0199, 0.0967,\n",
       "          0.1447, 0.0468, 0.1111, 0.0952, 0.0570, 0.0498]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0578, 0.0322, 0.0838, 0.0494, 0.0571, 0.0516, 0.0469, 0.0199, 0.0967,\n",
       "           0.1447, 0.0468, 0.1111, 0.0952, 0.0570, 0.0498]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1000, 0.0329, 0.1371, 0.0630, 0.0939, 0.0844, 0.0479, 0.0283, 0.0748,\n",
       "          0.0600, 0.0217, 0.0589, 0.0509, 0.0869, 0.0594]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1000, 0.0329, 0.1371, 0.0630, 0.0939, 0.0844, 0.0479, 0.0283, 0.0748,\n",
       "           0.0600, 0.0217, 0.0589, 0.0509, 0.0869, 0.0594]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0211, 0.0242, 0.0495, 0.0325, 0.0239, 0.0151, 0.0295, 0.0154, 0.1502,\n",
       "          0.4387, 0.0343, 0.0645, 0.0540, 0.0275, 0.0198]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0211, 0.0242, 0.0495, 0.0325, 0.0239, 0.0151, 0.0295, 0.0154, 0.1502,\n",
       "           0.4387, 0.0343, 0.0645, 0.0540, 0.0275, 0.0198]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0896, 0.0355, 0.1520, 0.0681, 0.1382, 0.0724, 0.0457, 0.0153, 0.0491,\n",
       "          0.0357, 0.0199, 0.0651, 0.0504, 0.0922, 0.0709]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0896, 0.0355, 0.1520, 0.0681, 0.1382, 0.0724, 0.0457, 0.0153, 0.0491,\n",
       "           0.0357, 0.0199, 0.0651, 0.0504, 0.0922, 0.0709]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0565, 0.0558, 0.0763, 0.0545, 0.0588, 0.0571, 0.0498, 0.0322, 0.0847,\n",
       "          0.1270, 0.0531, 0.1135, 0.0914, 0.0509, 0.0382]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0565, 0.0558, 0.0763, 0.0545, 0.0588, 0.0571, 0.0498, 0.0322, 0.0847,\n",
       "           0.1270, 0.0531, 0.1135, 0.0914, 0.0509, 0.0382]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0476, 0.0380, 0.0569, 0.0478, 0.0526, 0.0489, 0.0353, 0.0386, 0.1222,\n",
       "          0.1380, 0.0677, 0.1000, 0.1042, 0.0518, 0.0505]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0476, 0.0380, 0.0569, 0.0478, 0.0526, 0.0489, 0.0353, 0.0386, 0.1222,\n",
       "           0.1380, 0.0677, 0.1000, 0.1042, 0.0518, 0.0505]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0837, 0.0343, 0.1323, 0.0879, 0.1247, 0.0701, 0.0336, 0.0157, 0.0672,\n",
       "          0.0353, 0.0196, 0.0594, 0.0607, 0.0864, 0.0890]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0837, 0.0343, 0.1323, 0.0879, 0.1247, 0.0701, 0.0336, 0.0157, 0.0672,\n",
       "           0.0353, 0.0196, 0.0594, 0.0607, 0.0864, 0.0890]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0892, 0.0339, 0.1690, 0.0623, 0.0792, 0.0882, 0.0372, 0.0179, 0.0462,\n",
       "          0.0563, 0.0231, 0.0700, 0.0754, 0.0797, 0.0724]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0892, 0.0339, 0.1690, 0.0623, 0.0792, 0.0882, 0.0372, 0.0179, 0.0462,\n",
       "           0.0563, 0.0231, 0.0700, 0.0754, 0.0797, 0.0724]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0832, 0.0328, 0.0965, 0.0616, 0.1012, 0.0683, 0.0408, 0.0169, 0.1063,\n",
       "          0.1024, 0.0275, 0.0748, 0.0797, 0.0540, 0.0540]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0832, 0.0328, 0.0965, 0.0616, 0.1012, 0.0683, 0.0408, 0.0169, 0.1063,\n",
       "           0.1024, 0.0275, 0.0748, 0.0797, 0.0540, 0.0540]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0402, 0.0226, 0.0578, 0.0249, 0.0460, 0.0339, 0.0275, 0.0169, 0.0710,\n",
       "          0.1550, 0.0467, 0.1733, 0.1917, 0.0438, 0.0486]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0402, 0.0226, 0.0578, 0.0249, 0.0460, 0.0339, 0.0275, 0.0169, 0.0710,\n",
       "           0.1550, 0.0467, 0.1733, 0.1917, 0.0438, 0.0486]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0869, 0.0338, 0.1658, 0.0605, 0.1693, 0.0986, 0.0372, 0.0180, 0.0315,\n",
       "          0.0225, 0.0135, 0.0557, 0.0411, 0.0820, 0.0836]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0869, 0.0338, 0.1658, 0.0605, 0.1693, 0.0986, 0.0372, 0.0180, 0.0315,\n",
       "           0.0225, 0.0135, 0.0557, 0.0411, 0.0820, 0.0836]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0962, 0.0252, 0.1499, 0.0765, 0.1096, 0.0885, 0.0428, 0.0203, 0.0643,\n",
       "          0.0648, 0.0185, 0.0462, 0.0653, 0.0600, 0.0718]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0962, 0.0252, 0.1499, 0.0765, 0.1096, 0.0885, 0.0428, 0.0203, 0.0643,\n",
       "           0.0648, 0.0185, 0.0462, 0.0653, 0.0600, 0.0718]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0241, 0.0202, 0.0435, 0.0268, 0.0503, 0.0303, 0.0273, 0.0184, 0.1825,\n",
       "          0.3241, 0.0346, 0.0655, 0.0829, 0.0281, 0.0413]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0241, 0.0202, 0.0435, 0.0268, 0.0503, 0.0303, 0.0273, 0.0184, 0.1825,\n",
       "           0.3241, 0.0346, 0.0655, 0.0829, 0.0281, 0.0413]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0793, 0.0368, 0.1043, 0.0604, 0.0896, 0.0840, 0.0496, 0.0206, 0.0779,\n",
       "          0.0960, 0.0316, 0.0964, 0.0614, 0.0540, 0.0582]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0793, 0.0368, 0.1043, 0.0604, 0.0896, 0.0840, 0.0496, 0.0206, 0.0779,\n",
       "           0.0960, 0.0316, 0.0964, 0.0614, 0.0540, 0.0582]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0880, 0.0361, 0.1112, 0.0632, 0.0807, 0.0617, 0.0454, 0.0169, 0.0666,\n",
       "          0.0766, 0.0289, 0.1066, 0.0839, 0.0662, 0.0681]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0880, 0.0361, 0.1112, 0.0632, 0.0807, 0.0617, 0.0454, 0.0169, 0.0666,\n",
       "           0.0766, 0.0289, 0.1066, 0.0839, 0.0662, 0.0681]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.1005, 0.0237, 0.1726, 0.0799, 0.1117, 0.1061, 0.0426, 0.0153, 0.0609,\n",
       "          0.0456, 0.0170, 0.0432, 0.0396, 0.0578, 0.0835]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1005, 0.0237, 0.1726, 0.0799, 0.1117, 0.1061, 0.0426, 0.0153, 0.0609,\n",
       "           0.0456, 0.0170, 0.0432, 0.0396, 0.0578, 0.0835]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0339, 0.0252, 0.0416, 0.0312, 0.0417, 0.0421, 0.0313, 0.0168, 0.1456,\n",
       "          0.3155, 0.0582, 0.0857, 0.0693, 0.0361, 0.0259]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0339, 0.0252, 0.0416, 0.0312, 0.0417, 0.0421, 0.0313, 0.0168, 0.1456,\n",
       "           0.3155, 0.0582, 0.0857, 0.0693, 0.0361, 0.0259]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0828, 0.0443, 0.0932, 0.0563, 0.1083, 0.0772, 0.0488, 0.0239, 0.0720,\n",
       "          0.0740, 0.0284, 0.0844, 0.0521, 0.0776, 0.0767]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0828, 0.0443, 0.0932, 0.0563, 0.1083, 0.0772, 0.0488, 0.0239, 0.0720,\n",
       "           0.0740, 0.0284, 0.0844, 0.0521, 0.0776, 0.0767]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0581, 0.0405, 0.0464, 0.0533, 0.0613, 0.0441, 0.0343, 0.0228, 0.0848,\n",
       "          0.1108, 0.0554, 0.1779, 0.1016, 0.0439, 0.0646]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0581, 0.0405, 0.0464, 0.0533, 0.0613, 0.0441, 0.0343, 0.0228, 0.0848,\n",
       "           0.1108, 0.0554, 0.1779, 0.1016, 0.0439, 0.0646]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0098, 0.0067, 0.0168, 0.0184, 0.0215, 0.0116, 0.0074, 0.0032, 0.1061,\n",
       "          0.6317, 0.0224, 0.0719, 0.0415, 0.0135, 0.0176]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0098, 0.0067, 0.0168, 0.0184, 0.0215, 0.0116, 0.0074, 0.0032, 0.1061,\n",
       "           0.6317, 0.0224, 0.0719, 0.0415, 0.0135, 0.0176]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0934, 0.0281, 0.1079, 0.0492, 0.1006, 0.1002, 0.0361, 0.0188, 0.0792,\n",
       "          0.0843, 0.0214, 0.0770, 0.0674, 0.0626, 0.0738]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0934, 0.0281, 0.1079, 0.0492, 0.1006, 0.1002, 0.0361, 0.0188, 0.0792,\n",
       "           0.0843, 0.0214, 0.0770, 0.0674, 0.0626, 0.0738]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0557, 0.0251, 0.0467, 0.0337, 0.0574, 0.0492, 0.0308, 0.0124, 0.0648,\n",
       "          0.1180, 0.0413, 0.2607, 0.1130, 0.0580, 0.0332]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0557, 0.0251, 0.0467, 0.0337, 0.0574, 0.0492, 0.0308, 0.0124, 0.0648,\n",
       "           0.1180, 0.0413, 0.2607, 0.1130, 0.0580, 0.0332]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0801, 0.0261, 0.1051, 0.0592, 0.1071, 0.0845, 0.0442, 0.0144, 0.0779,\n",
       "          0.0965, 0.0222, 0.0871, 0.0396, 0.0783, 0.0775]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0801, 0.0261, 0.1051, 0.0592, 0.1071, 0.0845, 0.0442, 0.0144, 0.0779,\n",
       "           0.0965, 0.0222, 0.0871, 0.0396, 0.0783, 0.0775]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0322, 0.0311, 0.0300, 0.0293, 0.0378, 0.0322, 0.0410, 0.0101, 0.1170,\n",
       "          0.3953, 0.0463, 0.0893, 0.0545, 0.0323, 0.0215]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0322, 0.0311, 0.0300, 0.0293, 0.0378, 0.0322, 0.0410, 0.0101, 0.1170,\n",
       "           0.3953, 0.0463, 0.0893, 0.0545, 0.0323, 0.0215]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1068, 0.0400, 0.0941, 0.0576, 0.1268, 0.1053, 0.0470, 0.0146, 0.0489,\n",
       "          0.0565, 0.0190, 0.0780, 0.0670, 0.0637, 0.0745]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1068, 0.0400, 0.0941, 0.0576, 0.1268, 0.1053, 0.0470, 0.0146, 0.0489,\n",
       "           0.0565, 0.0190, 0.0780, 0.0670, 0.0637, 0.0745]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1141, 0.0504, 0.1006, 0.0691, 0.1138, 0.1164, 0.0596, 0.0200, 0.0526,\n",
       "          0.0326, 0.0185, 0.0565, 0.0382, 0.0742, 0.0833]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1141, 0.0504, 0.1006, 0.0691, 0.1138, 0.1164, 0.0596, 0.0200, 0.0526,\n",
       "           0.0326, 0.0185, 0.0565, 0.0382, 0.0742, 0.0833]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0502, 0.0337, 0.0637, 0.0475, 0.0559, 0.0574, 0.0427, 0.0180, 0.0952,\n",
       "          0.1646, 0.0454, 0.1357, 0.0676, 0.0477, 0.0747]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0502, 0.0337, 0.0637, 0.0475, 0.0559, 0.0574, 0.0427, 0.0180, 0.0952,\n",
       "           0.1646, 0.0454, 0.1357, 0.0676, 0.0477, 0.0747]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0464, 0.0293, 0.0580, 0.0473, 0.0335, 0.0481, 0.0351, 0.0159, 0.0909,\n",
       "          0.1651, 0.0533, 0.1533, 0.1273, 0.0479, 0.0486]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0464, 0.0293, 0.0580, 0.0473, 0.0335, 0.0481, 0.0351, 0.0159, 0.0909,\n",
       "           0.1651, 0.0533, 0.1533, 0.1273, 0.0479, 0.0486]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0221, 0.0204, 0.0282, 0.0198, 0.0406, 0.0297, 0.0329, 0.0166, 0.1357,\n",
       "          0.4255, 0.0475, 0.0835, 0.0529, 0.0239, 0.0208]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0221, 0.0204, 0.0282, 0.0198, 0.0406, 0.0297, 0.0329, 0.0166, 0.1357,\n",
       "           0.4255, 0.0475, 0.0835, 0.0529, 0.0239, 0.0208]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0621, 0.0420, 0.0815, 0.0474, 0.0866, 0.0802, 0.0504, 0.0330, 0.0831,\n",
       "          0.1286, 0.0356, 0.0523, 0.0567, 0.0781, 0.0822]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0621, 0.0420, 0.0815, 0.0474, 0.0866, 0.0802, 0.0504, 0.0330, 0.0831,\n",
       "           0.1286, 0.0356, 0.0523, 0.0567, 0.0781, 0.0822]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1171, 0.0360, 0.1182, 0.0626, 0.0898, 0.1053, 0.0550, 0.0142, 0.0468,\n",
       "          0.0422, 0.0178, 0.0792, 0.0447, 0.0846, 0.0865]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1171, 0.0360, 0.1182, 0.0626, 0.0898, 0.1053, 0.0550, 0.0142, 0.0468,\n",
       "           0.0422, 0.0178, 0.0792, 0.0447, 0.0846, 0.0865]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0101, 0.0071, 0.0165, 0.0135, 0.0156, 0.0096, 0.0104, 0.0025, 0.0912,\n",
       "          0.6714, 0.0140, 0.0728, 0.0335, 0.0130, 0.0190]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0101, 0.0071, 0.0165, 0.0135, 0.0156, 0.0096, 0.0104, 0.0025, 0.0912,\n",
       "           0.6714, 0.0140, 0.0728, 0.0335, 0.0130, 0.0190]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0759, 0.0261, 0.1178, 0.0552, 0.1776, 0.1389, 0.0344, 0.0109, 0.0561,\n",
       "          0.0604, 0.0134, 0.0530, 0.0432, 0.0529, 0.0843]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0759, 0.0261, 0.1178, 0.0552, 0.1776, 0.1389, 0.0344, 0.0109, 0.0561,\n",
       "           0.0604, 0.0134, 0.0530, 0.0432, 0.0529, 0.0843]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0899, 0.0243, 0.1000, 0.0651, 0.1145, 0.1178, 0.0378, 0.0168, 0.0474,\n",
       "          0.0656, 0.0188, 0.0710, 0.0752, 0.0662, 0.0896]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0899, 0.0243, 0.1000, 0.0651, 0.1145, 0.1178, 0.0378, 0.0168, 0.0474,\n",
       "           0.0656, 0.0188, 0.0710, 0.0752, 0.0662, 0.0896]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0287, 0.0227, 0.0301, 0.0287, 0.0457, 0.0336, 0.0243, 0.0122, 0.0751,\n",
       "          0.4067, 0.0336, 0.1184, 0.0839, 0.0249, 0.0314]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0287, 0.0227, 0.0301, 0.0287, 0.0457, 0.0336, 0.0243, 0.0122, 0.0751,\n",
       "           0.4067, 0.0336, 0.1184, 0.0839, 0.0249, 0.0314]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0741, 0.0324, 0.1309, 0.0587, 0.1269, 0.1063, 0.0417, 0.0189, 0.0477,\n",
       "          0.0748, 0.0135, 0.0585, 0.0402, 0.0866, 0.0887]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0741, 0.0324, 0.1309, 0.0587, 0.1269, 0.1063, 0.0417, 0.0189, 0.0477,\n",
       "           0.0748, 0.0135, 0.0585, 0.0402, 0.0866, 0.0887]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1017, 0.0366, 0.0763, 0.0894, 0.1329, 0.0985, 0.0450, 0.0130, 0.0648,\n",
       "          0.0669, 0.0163, 0.0674, 0.0436, 0.0790, 0.0684]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1017, 0.0366, 0.0763, 0.0894, 0.1329, 0.0985, 0.0450, 0.0130, 0.0648,\n",
       "           0.0669, 0.0163, 0.0674, 0.0436, 0.0790, 0.0684]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0576, 0.0334, 0.0686, 0.0531, 0.0757, 0.0723, 0.0464, 0.0289, 0.0850,\n",
       "          0.1028, 0.0332, 0.0937, 0.0903, 0.0682, 0.0908]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0576, 0.0334, 0.0686, 0.0531, 0.0757, 0.0723, 0.0464, 0.0289, 0.0850,\n",
       "           0.1028, 0.0332, 0.0937, 0.0903, 0.0682, 0.0908]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0244, 0.0246, 0.0320, 0.0338, 0.0349, 0.0279, 0.0321, 0.0134, 0.0742,\n",
       "          0.3876, 0.0329, 0.1077, 0.0974, 0.0405, 0.0365]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0244, 0.0246, 0.0320, 0.0338, 0.0349, 0.0279, 0.0321, 0.0134, 0.0742,\n",
       "           0.3876, 0.0329, 0.1077, 0.0974, 0.0405, 0.0365]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0538, 0.0378, 0.0810, 0.0664, 0.1037, 0.0584, 0.0407, 0.0152, 0.1071,\n",
       "          0.1373, 0.0284, 0.0809, 0.0564, 0.0672, 0.0654]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0538, 0.0378, 0.0810, 0.0664, 0.1037, 0.0584, 0.0407, 0.0152, 0.1071,\n",
       "           0.1373, 0.0284, 0.0809, 0.0564, 0.0672, 0.0654]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0168, 0.0135, 0.0277, 0.0192, 0.0244, 0.0190, 0.0159, 0.0094, 0.1494,\n",
       "          0.5304, 0.0357, 0.0516, 0.0514, 0.0169, 0.0187]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0168, 0.0135, 0.0277, 0.0192, 0.0244, 0.0190, 0.0159, 0.0094, 0.1494,\n",
       "           0.5304, 0.0357, 0.0516, 0.0514, 0.0169, 0.0187]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0692, 0.0380, 0.0879, 0.0587, 0.1129, 0.0932, 0.0573, 0.0237, 0.1058,\n",
       "          0.0814, 0.0260, 0.0438, 0.0550, 0.0730, 0.0739]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0692, 0.0380, 0.0879, 0.0587, 0.1129, 0.0932, 0.0573, 0.0237, 0.1058,\n",
       "           0.0814, 0.0260, 0.0438, 0.0550, 0.0730, 0.0739]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0804, 0.0310, 0.0973, 0.0699, 0.1263, 0.1416, 0.0500, 0.0194, 0.0594,\n",
       "          0.0568, 0.0162, 0.0448, 0.0456, 0.0567, 0.1045]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0804, 0.0310, 0.0973, 0.0699, 0.1263, 0.1416, 0.0500, 0.0194, 0.0594,\n",
       "           0.0568, 0.0162, 0.0448, 0.0456, 0.0567, 0.1045]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0789, 0.0399, 0.0748, 0.0740, 0.1087, 0.0928, 0.0476, 0.0209, 0.0622,\n",
       "          0.1025, 0.0211, 0.0631, 0.0528, 0.0653, 0.0952]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0789, 0.0399, 0.0748, 0.0740, 0.1087, 0.0928, 0.0476, 0.0209, 0.0622,\n",
       "           0.1025, 0.0211, 0.0631, 0.0528, 0.0653, 0.0952]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0149, 0.0111, 0.0345, 0.0151, 0.0255, 0.0199, 0.0131, 0.0081, 0.1057,\n",
       "          0.5734, 0.0245, 0.0593, 0.0569, 0.0161, 0.0217]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0149, 0.0111, 0.0345, 0.0151, 0.0255, 0.0199, 0.0131, 0.0081, 0.1057,\n",
       "           0.5734, 0.0245, 0.0593, 0.0569, 0.0161, 0.0217]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0309, 0.0171, 0.0413, 0.0218, 0.0415, 0.0379, 0.0188, 0.0108, 0.0816,\n",
       "          0.1459, 0.0366, 0.2340, 0.2066, 0.0409, 0.0343]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0309, 0.0171, 0.0413, 0.0218, 0.0415, 0.0379, 0.0188, 0.0108, 0.0816,\n",
       "           0.1459, 0.0366, 0.2340, 0.2066, 0.0409, 0.0343]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0804, 0.0348, 0.1384, 0.0694, 0.1516, 0.1275, 0.0436, 0.0204, 0.0317,\n",
       "          0.0228, 0.0156, 0.0468, 0.0403, 0.0904, 0.0863]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0804, 0.0348, 0.1384, 0.0694, 0.1516, 0.1275, 0.0436, 0.0204, 0.0317,\n",
       "           0.0228, 0.0156, 0.0468, 0.0403, 0.0904, 0.0863]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0484, 0.0283, 0.0485, 0.0788, 0.0904, 0.0611, 0.0352, 0.0126, 0.0890,\n",
       "          0.2555, 0.0204, 0.0709, 0.0583, 0.0504, 0.0521]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0484, 0.0283, 0.0485, 0.0788, 0.0904, 0.0611, 0.0352, 0.0126, 0.0890,\n",
       "           0.2555, 0.0204, 0.0709, 0.0583, 0.0504, 0.0521]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1107, 0.0211, 0.1086, 0.0727, 0.0947, 0.1123, 0.0449, 0.0151, 0.0588,\n",
       "          0.0552, 0.0174, 0.0681, 0.0480, 0.0812, 0.0912]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1107, 0.0211, 0.1086, 0.0727, 0.0947, 0.1123, 0.0449, 0.0151, 0.0588,\n",
       "           0.0552, 0.0174, 0.0681, 0.0480, 0.0812, 0.0912]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0359, 0.0267, 0.0584, 0.0388, 0.0424, 0.0346, 0.0504, 0.0101, 0.0573,\n",
       "          0.1102, 0.0372, 0.2855, 0.1361, 0.0455, 0.0309]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0359, 0.0267, 0.0584, 0.0388, 0.0424, 0.0346, 0.0504, 0.0101, 0.0573,\n",
       "           0.1102, 0.0372, 0.2855, 0.1361, 0.0455, 0.0309]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0834, 0.0344, 0.0721, 0.0875, 0.1151, 0.0920, 0.0642, 0.0224, 0.0535,\n",
       "          0.0690, 0.0235, 0.0613, 0.0614, 0.0753, 0.0850]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0834, 0.0344, 0.0721, 0.0875, 0.1151, 0.0920, 0.0642, 0.0224, 0.0535,\n",
       "           0.0690, 0.0235, 0.0613, 0.0614, 0.0753, 0.0850]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0494, 0.0242, 0.0573, 0.0449, 0.0702, 0.0714, 0.0420, 0.0092, 0.1460,\n",
       "          0.1967, 0.0324, 0.0737, 0.0822, 0.0516, 0.0488]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0494, 0.0242, 0.0573, 0.0449, 0.0702, 0.0714, 0.0420, 0.0092, 0.1460,\n",
       "           0.1967, 0.0324, 0.0737, 0.0822, 0.0516, 0.0488]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0716, 0.0359, 0.0910, 0.0713, 0.0938, 0.0935, 0.0513, 0.0163, 0.0753,\n",
       "          0.1012, 0.0253, 0.0721, 0.0571, 0.0733, 0.0711]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0716, 0.0359, 0.0910, 0.0713, 0.0938, 0.0935, 0.0513, 0.0163, 0.0753,\n",
       "           0.1012, 0.0253, 0.0721, 0.0571, 0.0733, 0.0711]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0889, 0.0357, 0.1056, 0.0812, 0.1154, 0.1021, 0.0562, 0.0194, 0.0633,\n",
       "          0.0428, 0.0230, 0.0766, 0.0535, 0.0739, 0.0625]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0889, 0.0357, 0.1056, 0.0812, 0.1154, 0.1021, 0.0562, 0.0194, 0.0633,\n",
       "           0.0428, 0.0230, 0.0766, 0.0535, 0.0739, 0.0625]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0151, 0.0169, 0.0233, 0.0247, 0.0412, 0.0279, 0.0298, 0.0093, 0.2051,\n",
       "          0.4308, 0.0243, 0.0728, 0.0254, 0.0308, 0.0225]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0151, 0.0169, 0.0233, 0.0247, 0.0412, 0.0279, 0.0298, 0.0093, 0.2051,\n",
       "           0.4308, 0.0243, 0.0728, 0.0254, 0.0308, 0.0225]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0697, 0.0420, 0.1145, 0.0755, 0.1577, 0.0911, 0.0442, 0.0169, 0.0625,\n",
       "          0.0637, 0.0168, 0.0457, 0.0424, 0.0765, 0.0805]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0697, 0.0420, 0.1145, 0.0755, 0.1577, 0.0911, 0.0442, 0.0169, 0.0625,\n",
       "           0.0637, 0.0168, 0.0457, 0.0424, 0.0765, 0.0805]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0924, 0.0323, 0.0829, 0.0700, 0.1218, 0.1071, 0.0560, 0.0155, 0.0579,\n",
       "          0.0548, 0.0177, 0.0735, 0.0534, 0.0714, 0.0934]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0924, 0.0323, 0.0829, 0.0700, 0.1218, 0.1071, 0.0560, 0.0155, 0.0579,\n",
       "           0.0548, 0.0177, 0.0735, 0.0534, 0.0714, 0.0934]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0317, 0.0219, 0.0352, 0.0506, 0.0430, 0.0344, 0.0235, 0.0106, 0.0770,\n",
       "          0.1199, 0.0363, 0.2469, 0.1621, 0.0441, 0.0626]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0317, 0.0219, 0.0352, 0.0506, 0.0430, 0.0344, 0.0235, 0.0106, 0.0770,\n",
       "           0.1199, 0.0363, 0.2469, 0.1621, 0.0441, 0.0626]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0663, 0.0303, 0.1289, 0.0802, 0.1172, 0.0829, 0.0431, 0.0179, 0.0686,\n",
       "          0.0883, 0.0197, 0.0617, 0.0426, 0.0699, 0.0825]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0663, 0.0303, 0.1289, 0.0802, 0.1172, 0.0829, 0.0431, 0.0179, 0.0686,\n",
       "           0.0883, 0.0197, 0.0617, 0.0426, 0.0699, 0.0825]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0114, 0.0126, 0.0192, 0.0150, 0.0280, 0.0142, 0.0213, 0.0077, 0.1423,\n",
       "          0.4021, 0.0305, 0.2001, 0.0596, 0.0208, 0.0153]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0114, 0.0126, 0.0192, 0.0150, 0.0280, 0.0142, 0.0213, 0.0077, 0.1423,\n",
       "           0.4021, 0.0305, 0.2001, 0.0596, 0.0208, 0.0153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0728, 0.0325, 0.0806, 0.0766, 0.1161, 0.0896, 0.0564, 0.0225, 0.0674,\n",
       "          0.0725, 0.0231, 0.0705, 0.0469, 0.0727, 0.0997]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0728, 0.0325, 0.0806, 0.0766, 0.1161, 0.0896, 0.0564, 0.0225, 0.0674,\n",
       "           0.0725, 0.0231, 0.0705, 0.0469, 0.0727, 0.0997]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0355, 0.0284, 0.1096, 0.0420, 0.0672, 0.0504, 0.0380, 0.0235, 0.1548,\n",
       "          0.1829, 0.0448, 0.0741, 0.0481, 0.0493, 0.0515]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0355, 0.0284, 0.1096, 0.0420, 0.0672, 0.0504, 0.0380, 0.0235, 0.1548,\n",
       "           0.1829, 0.0448, 0.0741, 0.0481, 0.0493, 0.0515]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0604, 0.0400, 0.0505, 0.0617, 0.0893, 0.0512, 0.0532, 0.0135, 0.1326,\n",
       "          0.1331, 0.0317, 0.0967, 0.0649, 0.0560, 0.0652]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0604, 0.0400, 0.0505, 0.0617, 0.0893, 0.0512, 0.0532, 0.0135, 0.1326,\n",
       "           0.1331, 0.0317, 0.0967, 0.0649, 0.0560, 0.0652]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0482, 0.0266, 0.1051, 0.0605, 0.0830, 0.0496, 0.0419, 0.0121, 0.1080,\n",
       "          0.1388, 0.0205, 0.1064, 0.0570, 0.0692, 0.0732]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0482, 0.0266, 0.1051, 0.0605, 0.0830, 0.0496, 0.0419, 0.0121, 0.1080,\n",
       "           0.1388, 0.0205, 0.1064, 0.0570, 0.0692, 0.0732]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0557, 0.0417, 0.1108, 0.0718, 0.1298, 0.0873, 0.0367, 0.0234, 0.0887,\n",
       "          0.0853, 0.0295, 0.0538, 0.0534, 0.0697, 0.0625]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0557, 0.0417, 0.1108, 0.0718, 0.1298, 0.0873, 0.0367, 0.0234, 0.0887,\n",
       "           0.0853, 0.0295, 0.0538, 0.0534, 0.0697, 0.0625]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0609, 0.0306, 0.1248, 0.0677, 0.1348, 0.0894, 0.0439, 0.0177, 0.0917,\n",
       "          0.0644, 0.0214, 0.0444, 0.0468, 0.0691, 0.0923]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0609, 0.0306, 0.1248, 0.0677, 0.1348, 0.0894, 0.0439, 0.0177, 0.0917,\n",
       "           0.0644, 0.0214, 0.0444, 0.0468, 0.0691, 0.0923]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0650, 0.0286, 0.0938, 0.0522, 0.1843, 0.0846, 0.0443, 0.0184, 0.0947,\n",
       "          0.0552, 0.0194, 0.0648, 0.0431, 0.0560, 0.0955]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0650, 0.0286, 0.0938, 0.0522, 0.1843, 0.0846, 0.0443, 0.0184, 0.0947,\n",
       "           0.0552, 0.0194, 0.0648, 0.0431, 0.0560, 0.0955]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0642, 0.0425, 0.1028, 0.0725, 0.1575, 0.0876, 0.0461, 0.0167, 0.0760,\n",
       "          0.0752, 0.0143, 0.0595, 0.0355, 0.0724, 0.0773]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0642, 0.0425, 0.1028, 0.0725, 0.1575, 0.0876, 0.0461, 0.0167, 0.0760,\n",
       "           0.0752, 0.0143, 0.0595, 0.0355, 0.0724, 0.0773]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0567, 0.0305, 0.1148, 0.0689, 0.1114, 0.0848, 0.0514, 0.0150, 0.0846,\n",
       "          0.0842, 0.0262, 0.0708, 0.0540, 0.0623, 0.0845]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0567, 0.0305, 0.1148, 0.0689, 0.1114, 0.0848, 0.0514, 0.0150, 0.0846,\n",
       "           0.0842, 0.0262, 0.0708, 0.0540, 0.0623, 0.0845]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0080, 0.0099, 0.0094, 0.0145, 0.0200, 0.0101, 0.0076, 0.0030, 0.2552,\n",
       "          0.4480, 0.0285, 0.1136, 0.0462, 0.0119, 0.0141]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0080, 0.0099, 0.0094, 0.0145, 0.0200, 0.0101, 0.0076, 0.0030, 0.2552,\n",
       "           0.4480, 0.0285, 0.1136, 0.0462, 0.0119, 0.0141]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0537, 0.0300, 0.1399, 0.0698, 0.1427, 0.0843, 0.0480, 0.0152, 0.1227,\n",
       "          0.0539, 0.0157, 0.0431, 0.0340, 0.0620, 0.0851]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0537, 0.0300, 0.1399, 0.0698, 0.1427, 0.0843, 0.0480, 0.0152, 0.1227,\n",
       "           0.0539, 0.0157, 0.0431, 0.0340, 0.0620, 0.0851]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0648, 0.0286, 0.1009, 0.0707, 0.1913, 0.1477, 0.0499, 0.0150, 0.0574,\n",
       "          0.0383, 0.0123, 0.0404, 0.0282, 0.0576, 0.0969]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0648, 0.0286, 0.1009, 0.0707, 0.1913, 0.1477, 0.0499, 0.0150, 0.0574,\n",
       "           0.0383, 0.0123, 0.0404, 0.0282, 0.0576, 0.0969]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0452, 0.0342, 0.1012, 0.0790, 0.1049, 0.0627, 0.0466, 0.0174, 0.0763,\n",
       "          0.1069, 0.0242, 0.1033, 0.0619, 0.0527, 0.0836]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0452, 0.0342, 0.1012, 0.0790, 0.1049, 0.0627, 0.0466, 0.0174, 0.0763,\n",
       "           0.1069, 0.0242, 0.1033, 0.0619, 0.0527, 0.0836]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0464, 0.0192, 0.0903, 0.0634, 0.0922, 0.0672, 0.0450, 0.0120, 0.1193,\n",
       "          0.1172, 0.0205, 0.1056, 0.0495, 0.0698, 0.0825]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0464, 0.0192, 0.0903, 0.0634, 0.0922, 0.0672, 0.0450, 0.0120, 0.1193,\n",
       "           0.1172, 0.0205, 0.1056, 0.0495, 0.0698, 0.0825]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[4.3801e-04, 8.6885e-04, 6.7240e-04, 1.1990e-03, 1.6563e-03, 9.1982e-04,\n",
       "          1.0918e-03, 2.3666e-04, 1.0989e-01, 8.4356e-01, 4.6958e-03, 1.7734e-02,\n",
       "          1.3426e-02, 8.4731e-04, 2.7589e-03]], grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[4.3801e-04, 8.6885e-04, 6.7240e-04, 1.1990e-03, 1.6563e-03, 9.1982e-04,\n",
       "           1.0918e-03, 2.3666e-04, 1.0989e-01, 8.4356e-01, 4.6958e-03, 1.7734e-02,\n",
       "           1.3426e-02, 8.4731e-04, 2.7589e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0138, 0.0114, 0.0258, 0.0131, 0.0195, 0.0127, 0.0168, 0.0061, 0.4172,\n",
       "          0.2948, 0.0265, 0.0796, 0.0338, 0.0151, 0.0138]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0138, 0.0114, 0.0258, 0.0131, 0.0195, 0.0127, 0.0168, 0.0061, 0.4172,\n",
       "           0.2948, 0.0265, 0.0796, 0.0338, 0.0151, 0.0138]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0329, 0.0300, 0.1091, 0.0449, 0.0854, 0.0677, 0.0360, 0.0216, 0.1954,\n",
       "          0.1874, 0.0283, 0.0438, 0.0357, 0.0333, 0.0486]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0329, 0.0300, 0.1091, 0.0449, 0.0854, 0.0677, 0.0360, 0.0216, 0.1954,\n",
       "           0.1874, 0.0283, 0.0438, 0.0357, 0.0333, 0.0486]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0681, 0.0287, 0.2071, 0.0688, 0.1418, 0.1070, 0.0457, 0.0153, 0.0456,\n",
       "          0.0339, 0.0098, 0.0260, 0.0210, 0.0585, 0.1227]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0681, 0.0287, 0.2071, 0.0688, 0.1418, 0.1070, 0.0457, 0.0153, 0.0456,\n",
       "           0.0339, 0.0098, 0.0260, 0.0210, 0.0585, 0.1227]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0386, 0.0183, 0.0765, 0.0502, 0.0836, 0.0439, 0.0371, 0.0099, 0.2570,\n",
       "          0.1832, 0.0162, 0.0624, 0.0370, 0.0377, 0.0483]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0386, 0.0183, 0.0765, 0.0502, 0.0836, 0.0439, 0.0371, 0.0099, 0.2570,\n",
       "           0.1832, 0.0162, 0.0624, 0.0370, 0.0377, 0.0483]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0386, 0.0243, 0.0895, 0.0437, 0.0508, 0.0523, 0.0353, 0.0169, 0.1450,\n",
       "          0.2138, 0.0317, 0.0933, 0.0596, 0.0408, 0.0643]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0386, 0.0243, 0.0895, 0.0437, 0.0508, 0.0523, 0.0353, 0.0169, 0.1450,\n",
       "           0.2138, 0.0317, 0.0933, 0.0596, 0.0408, 0.0643]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0695, 0.0240, 0.1561, 0.1000, 0.1037, 0.0609, 0.0449, 0.0163, 0.0725,\n",
       "          0.0853, 0.0156, 0.0518, 0.0312, 0.0720, 0.0962]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0695, 0.0240, 0.1561, 0.1000, 0.1037, 0.0609, 0.0449, 0.0163, 0.0725,\n",
       "           0.0853, 0.0156, 0.0518, 0.0312, 0.0720, 0.0962]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0496, 0.0248, 0.1658, 0.0711, 0.1291, 0.0743, 0.0301, 0.0136, 0.1298,\n",
       "          0.0856, 0.0176, 0.0449, 0.0392, 0.0474, 0.0771]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0496, 0.0248, 0.1658, 0.0711, 0.1291, 0.0743, 0.0301, 0.0136, 0.1298,\n",
       "           0.0856, 0.0176, 0.0449, 0.0392, 0.0474, 0.0771]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0389, 0.0217, 0.0553, 0.0393, 0.0540, 0.0344, 0.0412, 0.0084, 0.1766,\n",
       "          0.2303, 0.0281, 0.1112, 0.0749, 0.0473, 0.0384]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0389, 0.0217, 0.0553, 0.0393, 0.0540, 0.0344, 0.0412, 0.0084, 0.1766,\n",
       "           0.2303, 0.0281, 0.1112, 0.0749, 0.0473, 0.0384]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0938, 0.0342, 0.1842, 0.0868, 0.1424, 0.1111, 0.0464, 0.0125, 0.0733,\n",
       "          0.0248, 0.0064, 0.0252, 0.0181, 0.0432, 0.0975]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0938, 0.0342, 0.1842, 0.0868, 0.1424, 0.1111, 0.0464, 0.0125, 0.0733,\n",
       "           0.0248, 0.0064, 0.0252, 0.0181, 0.0432, 0.0975]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0652, 0.0347, 0.1072, 0.0748, 0.1432, 0.0970, 0.0587, 0.0123, 0.0820,\n",
       "          0.0582, 0.0124, 0.0619, 0.0353, 0.0576, 0.0995]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0652, 0.0347, 0.1072, 0.0748, 0.1432, 0.0970, 0.0587, 0.0123, 0.0820,\n",
       "           0.0582, 0.0124, 0.0619, 0.0353, 0.0576, 0.0995]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0093, 0.0112, 0.0239, 0.0144, 0.0183, 0.0101, 0.0142, 0.0055, 0.3457,\n",
       "          0.4282, 0.0276, 0.0513, 0.0209, 0.0097, 0.0098]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0093, 0.0112, 0.0239, 0.0144, 0.0183, 0.0101, 0.0142, 0.0055, 0.3457,\n",
       "           0.4282, 0.0276, 0.0513, 0.0209, 0.0097, 0.0098]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0364, 0.0184, 0.1052, 0.0526, 0.0433, 0.0447, 0.0239, 0.0141, 0.1404,\n",
       "          0.2278, 0.0159, 0.0942, 0.0627, 0.0350, 0.0853]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0364, 0.0184, 0.1052, 0.0526, 0.0433, 0.0447, 0.0239, 0.0141, 0.1404,\n",
       "           0.2278, 0.0159, 0.0942, 0.0627, 0.0350, 0.0853]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[3.4002e-04, 9.2052e-04, 3.6191e-03, 1.4200e-03, 2.7335e-03, 1.6850e-03,\n",
       "          7.9287e-04, 3.2125e-04, 3.1743e-01, 6.5213e-01, 3.1094e-03, 7.0817e-03,\n",
       "          5.3260e-03, 9.9322e-04, 2.0945e-03]], grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[3.4002e-04, 9.2052e-04, 3.6191e-03, 1.4200e-03, 2.7335e-03, 1.6850e-03,\n",
       "           7.9287e-04, 3.2125e-04, 3.1743e-01, 6.5213e-01, 3.1094e-03, 7.0817e-03,\n",
       "           5.3260e-03, 9.9322e-04, 2.0945e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0435, 0.0338, 0.1172, 0.0576, 0.1253, 0.0765, 0.0390, 0.0146, 0.1313,\n",
       "          0.1155, 0.0197, 0.0558, 0.0335, 0.0620, 0.0748]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0435, 0.0338, 0.1172, 0.0576, 0.1253, 0.0765, 0.0390, 0.0146, 0.1313,\n",
       "           0.1155, 0.0197, 0.0558, 0.0335, 0.0620, 0.0748]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0200, 0.0153, 0.0595, 0.0141, 0.0279, 0.0349, 0.0237, 0.0146, 0.1061,\n",
       "          0.1120, 0.0277, 0.3710, 0.1142, 0.0190, 0.0400]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0200, 0.0153, 0.0595, 0.0141, 0.0279, 0.0349, 0.0237, 0.0146, 0.1061,\n",
       "           0.1120, 0.0277, 0.3710, 0.1142, 0.0190, 0.0400]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0659, 0.0275, 0.1153, 0.0471, 0.0883, 0.0785, 0.0427, 0.0136, 0.1543,\n",
       "          0.1220, 0.0150, 0.0461, 0.0472, 0.0596, 0.0768]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0659, 0.0275, 0.1153, 0.0471, 0.0883, 0.0785, 0.0427, 0.0136, 0.1543,\n",
       "           0.1220, 0.0150, 0.0461, 0.0472, 0.0596, 0.0768]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0597, 0.0385, 0.1834, 0.0635, 0.1725, 0.1004, 0.0395, 0.0125, 0.0509,\n",
       "          0.0431, 0.0094, 0.0239, 0.0181, 0.0463, 0.1383]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0597, 0.0385, 0.1834, 0.0635, 0.1725, 0.1004, 0.0395, 0.0125, 0.0509,\n",
       "           0.0431, 0.0094, 0.0239, 0.0181, 0.0463, 0.1383]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[7.0668e-04, 2.0295e-03, 3.7941e-03, 1.3662e-03, 2.6728e-03, 1.8296e-03,\n",
       "          1.6987e-03, 1.1626e-03, 1.1356e-01, 8.0983e-01, 4.9854e-03, 3.7992e-02,\n",
       "          1.3439e-02, 1.5441e-03, 3.3860e-03]], grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[7.0668e-04, 2.0295e-03, 3.7941e-03, 1.3662e-03, 2.6728e-03, 1.8296e-03,\n",
       "           1.6987e-03, 1.1626e-03, 1.1356e-01, 8.0983e-01, 4.9854e-03, 3.7992e-02,\n",
       "           1.3439e-02, 1.5441e-03, 3.3860e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0155, 0.0088, 0.0426, 0.0190, 0.0166, 0.0091, 0.0088, 0.0042, 0.1012,\n",
       "          0.2392, 0.0163, 0.3347, 0.1342, 0.0219, 0.0278]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0155, 0.0088, 0.0426, 0.0190, 0.0166, 0.0091, 0.0088, 0.0042, 0.1012,\n",
       "           0.2392, 0.0163, 0.3347, 0.1342, 0.0219, 0.0278]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0388, 0.0210, 0.1619, 0.0379, 0.1092, 0.0960, 0.0297, 0.0093, 0.1070,\n",
       "          0.1043, 0.0102, 0.0454, 0.0347, 0.0659, 0.1285]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0388, 0.0210, 0.1619, 0.0379, 0.1092, 0.0960, 0.0297, 0.0093, 0.1070,\n",
       "           0.1043, 0.0102, 0.0454, 0.0347, 0.0659, 0.1285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0725, 0.0252, 0.2405, 0.0547, 0.1093, 0.1020, 0.0378, 0.0102, 0.0453,\n",
       "          0.0210, 0.0082, 0.0455, 0.0260, 0.0736, 0.1283]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0725, 0.0252, 0.2405, 0.0547, 0.1093, 0.1020, 0.0378, 0.0102, 0.0453,\n",
       "           0.0210, 0.0082, 0.0455, 0.0260, 0.0736, 0.1283]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0607, 0.0345, 0.1378, 0.1018, 0.0785, 0.0690, 0.0431, 0.0121, 0.0749,\n",
       "          0.1232, 0.0133, 0.0555, 0.0444, 0.0679, 0.0833]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0607, 0.0345, 0.1378, 0.1018, 0.0785, 0.0690, 0.0431, 0.0121, 0.0749,\n",
       "           0.1232, 0.0133, 0.0555, 0.0444, 0.0679, 0.0833]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0724, 0.0274, 0.2110, 0.0764, 0.1207, 0.0761, 0.0423, 0.0140, 0.0553,\n",
       "          0.0663, 0.0108, 0.0339, 0.0296, 0.0642, 0.0997]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0724, 0.0274, 0.2110, 0.0764, 0.1207, 0.0761, 0.0423, 0.0140, 0.0553,\n",
       "           0.0663, 0.0108, 0.0339, 0.0296, 0.0642, 0.0997]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0384, 0.0109, 0.1454, 0.0445, 0.0980, 0.0523, 0.0192, 0.0066, 0.1670,\n",
       "          0.1962, 0.0095, 0.0311, 0.0435, 0.0346, 0.1027]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0384, 0.0109, 0.1454, 0.0445, 0.0980, 0.0523, 0.0192, 0.0066, 0.1670,\n",
       "           0.1962, 0.0095, 0.0311, 0.0435, 0.0346, 0.1027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0898, 0.0202, 0.1491, 0.0689, 0.1421, 0.1031, 0.0308, 0.0130, 0.0572,\n",
       "          0.0385, 0.0080, 0.0389, 0.0422, 0.0568, 0.1415]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0898, 0.0202, 0.1491, 0.0689, 0.1421, 0.1031, 0.0308, 0.0130, 0.0572,\n",
       "           0.0385, 0.0080, 0.0389, 0.0422, 0.0568, 0.1415]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0104, 0.0099, 0.0254, 0.0183, 0.0232, 0.0089, 0.0122, 0.0068, 0.1531,\n",
       "          0.4327, 0.0346, 0.1466, 0.0923, 0.0127, 0.0129]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0104, 0.0099, 0.0254, 0.0183, 0.0232, 0.0089, 0.0122, 0.0068, 0.1531,\n",
       "           0.4327, 0.0346, 0.1466, 0.0923, 0.0127, 0.0129]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0554, 0.0229, 0.1412, 0.0604, 0.0791, 0.0590, 0.0381, 0.0152, 0.1057,\n",
       "          0.1008, 0.0175, 0.0838, 0.0650, 0.0645, 0.0913]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0554, 0.0229, 0.1412, 0.0604, 0.0791, 0.0590, 0.0381, 0.0152, 0.1057,\n",
       "           0.1008, 0.0175, 0.0838, 0.0650, 0.0645, 0.0913]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0578, 0.0223, 0.1778, 0.0670, 0.1546, 0.0940, 0.0366, 0.0148, 0.0471,\n",
       "          0.0310, 0.0087, 0.0531, 0.0391, 0.0665, 0.1295]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0578, 0.0223, 0.1778, 0.0670, 0.1546, 0.0940, 0.0366, 0.0148, 0.0471,\n",
       "           0.0310, 0.0087, 0.0531, 0.0391, 0.0665, 0.1295]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0593, 0.0260, 0.1094, 0.0692, 0.0913, 0.0804, 0.0363, 0.0127, 0.0928,\n",
       "          0.1262, 0.0160, 0.0879, 0.0471, 0.0613, 0.0842]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0593, 0.0260, 0.1094, 0.0692, 0.0913, 0.0804, 0.0363, 0.0127, 0.0928,\n",
       "           0.1262, 0.0160, 0.0879, 0.0471, 0.0613, 0.0842]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0298, 0.0187, 0.0534, 0.0273, 0.0406, 0.0484, 0.0223, 0.0104, 0.1741,\n",
       "          0.2514, 0.0270, 0.1520, 0.0804, 0.0277, 0.0365]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0298, 0.0187, 0.0534, 0.0273, 0.0406, 0.0484, 0.0223, 0.0104, 0.1741,\n",
       "           0.2514, 0.0270, 0.1520, 0.0804, 0.0277, 0.0365]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0147, 0.0046, 0.0304, 0.0086, 0.0140, 0.0097, 0.0049, 0.0031, 0.0747,\n",
       "          0.1807, 0.0133, 0.4544, 0.1470, 0.0140, 0.0260]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0147, 0.0046, 0.0304, 0.0086, 0.0140, 0.0097, 0.0049, 0.0031, 0.0747,\n",
       "           0.1807, 0.0133, 0.4544, 0.1470, 0.0140, 0.0260]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0642, 0.0309, 0.1468, 0.0729, 0.0976, 0.0973, 0.0423, 0.0124, 0.0642,\n",
       "          0.0887, 0.0127, 0.0609, 0.0375, 0.0691, 0.1025]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0642, 0.0309, 0.1468, 0.0729, 0.0976, 0.0973, 0.0423, 0.0124, 0.0642,\n",
       "           0.0887, 0.0127, 0.0609, 0.0375, 0.0691, 0.1025]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0560, 0.0220, 0.0953, 0.0607, 0.0718, 0.0513, 0.0340, 0.0089, 0.1394,\n",
       "          0.1727, 0.0141, 0.0471, 0.0464, 0.0557, 0.1246]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0560, 0.0220, 0.0953, 0.0607, 0.0718, 0.0513, 0.0340, 0.0089, 0.1394,\n",
       "           0.1727, 0.0141, 0.0471, 0.0464, 0.0557, 0.1246]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1085, 0.0194, 0.1960, 0.0790, 0.1023, 0.0892, 0.0438, 0.0106, 0.0466,\n",
       "          0.0402, 0.0069, 0.0482, 0.0356, 0.0758, 0.0980]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1085, 0.0194, 0.1960, 0.0790, 0.1023, 0.0892, 0.0438, 0.0106, 0.0466,\n",
       "           0.0402, 0.0069, 0.0482, 0.0356, 0.0758, 0.0980]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0754, 0.0149, 0.1883, 0.0902, 0.1243, 0.0780, 0.0318, 0.0073, 0.0589,\n",
       "          0.0546, 0.0062, 0.0311, 0.0293, 0.0575, 0.1521]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0754, 0.0149, 0.1883, 0.0902, 0.1243, 0.0780, 0.0318, 0.0073, 0.0589,\n",
       "           0.0546, 0.0062, 0.0311, 0.0293, 0.0575, 0.1521]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0656, 0.0212, 0.0756, 0.0589, 0.0784, 0.0537, 0.0347, 0.0073, 0.0829,\n",
       "          0.1911, 0.0175, 0.1099, 0.0730, 0.0547, 0.0755]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0656, 0.0212, 0.0756, 0.0589, 0.0784, 0.0537, 0.0347, 0.0073, 0.0829,\n",
       "           0.1911, 0.0175, 0.1099, 0.0730, 0.0547, 0.0755]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0379, 0.0208, 0.0903, 0.0448, 0.0370, 0.0381, 0.0257, 0.0088, 0.1493,\n",
       "          0.2523, 0.0241, 0.0874, 0.0895, 0.0322, 0.0619]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0379, 0.0208, 0.0903, 0.0448, 0.0370, 0.0381, 0.0257, 0.0088, 0.1493,\n",
       "           0.2523, 0.0241, 0.0874, 0.0895, 0.0322, 0.0619]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0804, 0.0243, 0.1392, 0.0592, 0.0824, 0.0977, 0.0423, 0.0117, 0.0440,\n",
       "          0.0468, 0.0169, 0.0992, 0.0947, 0.0649, 0.0964]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0804, 0.0243, 0.1392, 0.0592, 0.0824, 0.0977, 0.0423, 0.0117, 0.0440,\n",
       "           0.0468, 0.0169, 0.0992, 0.0947, 0.0649, 0.0964]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0273, 0.0055, 0.0170, 0.0188, 0.0121, 0.0122, 0.0067, 0.0026, 0.0257,\n",
       "          0.1669, 0.0146, 0.3177, 0.3219, 0.0188, 0.0322]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0273, 0.0055, 0.0170, 0.0188, 0.0121, 0.0122, 0.0067, 0.0026, 0.0257,\n",
       "           0.1669, 0.0146, 0.3177, 0.3219, 0.0188, 0.0322]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0586, 0.0154, 0.1619, 0.0475, 0.0707, 0.0812, 0.0276, 0.0072, 0.0503,\n",
       "          0.0764, 0.0107, 0.0889, 0.0807, 0.0592, 0.1636]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0586, 0.0154, 0.1619, 0.0475, 0.0707, 0.0812, 0.0276, 0.0072, 0.0503,\n",
       "           0.0764, 0.0107, 0.0889, 0.0807, 0.0592, 0.1636]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[1.4964e-03, 6.0997e-04, 1.8232e-03, 1.2980e-03, 1.0117e-03, 1.0666e-03,\n",
       "          1.6595e-03, 2.7562e-04, 1.1658e-01, 8.1562e-01, 1.5845e-02, 1.9421e-02,\n",
       "          1.7596e-02, 3.5874e-03, 2.1082e-03]], grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[1.4964e-03, 6.0997e-04, 1.8232e-03, 1.2980e-03, 1.0117e-03, 1.0666e-03,\n",
       "           1.6595e-03, 2.7562e-04, 1.1658e-01, 8.1562e-01, 1.5845e-02, 1.9421e-02,\n",
       "           1.7596e-02, 3.5874e-03, 2.1082e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1276, 0.0242, 0.2111, 0.1025, 0.1044, 0.0706, 0.0250, 0.0151, 0.0161,\n",
       "          0.0155, 0.0096, 0.0352, 0.0364, 0.0783, 0.1285]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1276, 0.0242, 0.2111, 0.1025, 0.1044, 0.0706, 0.0250, 0.0151, 0.0161,\n",
       "           0.0155, 0.0096, 0.0352, 0.0364, 0.0783, 0.1285]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[6.6274e-04, 3.9182e-04, 1.4836e-03, 1.4737e-03, 1.4463e-03, 9.8423e-04,\n",
       "          7.8889e-04, 2.6280e-04, 3.4498e-02, 9.2371e-01, 2.5044e-03, 1.4247e-02,\n",
       "          1.3385e-02, 2.1331e-03, 2.0290e-03]], grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[6.6274e-04, 3.9182e-04, 1.4836e-03, 1.4737e-03, 1.4463e-03, 9.8423e-04,\n",
       "           7.8889e-04, 2.6280e-04, 3.4498e-02, 9.2371e-01, 2.5044e-03, 1.4247e-02,\n",
       "           1.3385e-02, 2.1331e-03, 2.0290e-03]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1210, 0.0221, 0.1437, 0.0696, 0.1246, 0.1007, 0.0418, 0.0111, 0.0748,\n",
       "          0.0279, 0.0103, 0.0518, 0.0347, 0.0484, 0.1175]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1210, 0.0221, 0.1437, 0.0696, 0.1246, 0.1007, 0.0418, 0.0111, 0.0748,\n",
       "           0.0279, 0.0103, 0.0518, 0.0347, 0.0484, 0.1175]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0146, 0.0071, 0.0330, 0.0155, 0.0109, 0.0129, 0.0116, 0.0051, 0.1174,\n",
       "          0.4478, 0.0237, 0.1649, 0.1016, 0.0167, 0.0174]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0146, 0.0071, 0.0330, 0.0155, 0.0109, 0.0129, 0.0116, 0.0051, 0.1174,\n",
       "           0.4478, 0.0237, 0.1649, 0.1016, 0.0167, 0.0174]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1575, 0.0214, 0.1853, 0.0706, 0.0849, 0.0910, 0.0343, 0.0130, 0.0398,\n",
       "          0.0234, 0.0064, 0.0332, 0.0585, 0.0653, 0.1153]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1575, 0.0214, 0.1853, 0.0706, 0.0849, 0.0910, 0.0343, 0.0130, 0.0398,\n",
       "           0.0234, 0.0064, 0.0332, 0.0585, 0.0653, 0.1153]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1115, 0.0232, 0.1650, 0.0637, 0.0999, 0.0720, 0.0404, 0.0218, 0.0416,\n",
       "          0.0354, 0.0176, 0.0387, 0.0537, 0.0842, 0.1315]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1115, 0.0232, 0.1650, 0.0637, 0.0999, 0.0720, 0.0404, 0.0218, 0.0416,\n",
       "           0.0354, 0.0176, 0.0387, 0.0537, 0.0842, 0.1315]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.1201, 0.0204, 0.3380, 0.0495, 0.0779, 0.1004, 0.0387, 0.0059, 0.0277,\n",
       "          0.0146, 0.0047, 0.0302, 0.0266, 0.0449, 0.1003]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1201, 0.0204, 0.3380, 0.0495, 0.0779, 0.1004, 0.0387, 0.0059, 0.0277,\n",
       "           0.0146, 0.0047, 0.0302, 0.0266, 0.0449, 0.1003]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0907, 0.0218, 0.1525, 0.0848, 0.1068, 0.1022, 0.0420, 0.0163, 0.0564,\n",
       "          0.0449, 0.0177, 0.0408, 0.0409, 0.0643, 0.1179]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0907, 0.0218, 0.1525, 0.0848, 0.1068, 0.1022, 0.0420, 0.0163, 0.0564,\n",
       "           0.0449, 0.0177, 0.0408, 0.0409, 0.0643, 0.1179]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0328, 0.0234, 0.0843, 0.0422, 0.0373, 0.0409, 0.0243, 0.0125, 0.0876,\n",
       "          0.1181, 0.0367, 0.1770, 0.1762, 0.0460, 0.0609]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0328, 0.0234, 0.0843, 0.0422, 0.0373, 0.0409, 0.0243, 0.0125, 0.0876,\n",
       "           0.1181, 0.0367, 0.1770, 0.1762, 0.0460, 0.0609]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0447, 0.0205, 0.0676, 0.0345, 0.0472, 0.0454, 0.0308, 0.0103, 0.0851,\n",
       "          0.0954, 0.0208, 0.2345, 0.1525, 0.0560, 0.0547]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0447, 0.0205, 0.0676, 0.0345, 0.0472, 0.0454, 0.0308, 0.0103, 0.0851,\n",
       "           0.0954, 0.0208, 0.2345, 0.1525, 0.0560, 0.0547]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0745, 0.0124, 0.1500, 0.0600, 0.0632, 0.0614, 0.0227, 0.0066, 0.1439,\n",
       "          0.1554, 0.0096, 0.0360, 0.0627, 0.0536, 0.0878]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0745, 0.0124, 0.1500, 0.0600, 0.0632, 0.0614, 0.0227, 0.0066, 0.1439,\n",
       "           0.1554, 0.0096, 0.0360, 0.0627, 0.0536, 0.0878]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1322, 0.0139, 0.2322, 0.0601, 0.1136, 0.1041, 0.0369, 0.0060, 0.0320,\n",
       "          0.0136, 0.0042, 0.0524, 0.0413, 0.0625, 0.0952]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1322, 0.0139, 0.2322, 0.0601, 0.1136, 0.1041, 0.0369, 0.0060, 0.0320,\n",
       "           0.0136, 0.0042, 0.0524, 0.0413, 0.0625, 0.0952]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0917, 0.0244, 0.1449, 0.0637, 0.0783, 0.0836, 0.0405, 0.0204, 0.0920,\n",
       "          0.0656, 0.0201, 0.0418, 0.0733, 0.0684, 0.0914]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0917, 0.0244, 0.1449, 0.0637, 0.0783, 0.0836, 0.0405, 0.0204, 0.0920,\n",
       "           0.0656, 0.0201, 0.0418, 0.0733, 0.0684, 0.0914]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0029, 0.0020, 0.0052, 0.0061, 0.0027, 0.0056, 0.0029, 0.0011, 0.0794,\n",
       "          0.8210, 0.0093, 0.0115, 0.0430, 0.0047, 0.0027]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0029, 0.0020, 0.0052, 0.0061, 0.0027, 0.0056, 0.0029, 0.0011, 0.0794,\n",
       "           0.8210, 0.0093, 0.0115, 0.0430, 0.0047, 0.0027]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1033, 0.0172, 0.2590, 0.0622, 0.0833, 0.0834, 0.0317, 0.0074, 0.0444,\n",
       "          0.0308, 0.0085, 0.0341, 0.0321, 0.0787, 0.1237]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1033, 0.0172, 0.2590, 0.0622, 0.0833, 0.0834, 0.0317, 0.0074, 0.0444,\n",
       "           0.0308, 0.0085, 0.0341, 0.0321, 0.0787, 0.1237]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0273, 0.0195, 0.0307, 0.0308, 0.0174, 0.0302, 0.0152, 0.0108, 0.0511,\n",
       "          0.1924, 0.0531, 0.1972, 0.2609, 0.0334, 0.0301]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0273, 0.0195, 0.0307, 0.0308, 0.0174, 0.0302, 0.0152, 0.0108, 0.0511,\n",
       "           0.1924, 0.0531, 0.1972, 0.2609, 0.0334, 0.0301]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0116, 0.0058, 0.0096, 0.0091, 0.0094, 0.0077, 0.0055, 0.0022, 0.1227,\n",
       "          0.6684, 0.0144, 0.0395, 0.0718, 0.0099, 0.0125]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0116, 0.0058, 0.0096, 0.0091, 0.0094, 0.0077, 0.0055, 0.0022, 0.1227,\n",
       "           0.6684, 0.0144, 0.0395, 0.0718, 0.0099, 0.0125]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1004, 0.0275, 0.1183, 0.0576, 0.0833, 0.0929, 0.0287, 0.0185, 0.0792,\n",
       "          0.0592, 0.0170, 0.0577, 0.0746, 0.0830, 0.1020]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1004, 0.0275, 0.1183, 0.0576, 0.0833, 0.0929, 0.0287, 0.0185, 0.0792,\n",
       "           0.0592, 0.0170, 0.0577, 0.0746, 0.0830, 0.1020]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0042, 0.0031, 0.0046, 0.0060, 0.0050, 0.0040, 0.0035, 0.0016, 0.0849,\n",
       "          0.8165, 0.0117, 0.0136, 0.0294, 0.0067, 0.0051]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0042, 0.0031, 0.0046, 0.0060, 0.0050, 0.0040, 0.0035, 0.0016, 0.0849,\n",
       "           0.8165, 0.0117, 0.0136, 0.0294, 0.0067, 0.0051]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0477, 0.0140, 0.0634, 0.0216, 0.0277, 0.0420, 0.0184, 0.0065, 0.1164,\n",
       "          0.1859, 0.0159, 0.1283, 0.2273, 0.0466, 0.0383]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0477, 0.0140, 0.0634, 0.0216, 0.0277, 0.0420, 0.0184, 0.0065, 0.1164,\n",
       "           0.1859, 0.0159, 0.1283, 0.2273, 0.0466, 0.0383]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[0.0176, 0.0103, 0.0197, 0.0179, 0.0188, 0.0312, 0.0139, 0.0060, 0.0649,\n",
       "          0.1990, 0.0303, 0.1821, 0.3310, 0.0304, 0.0270]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0176, 0.0103, 0.0197, 0.0179, 0.0188, 0.0312, 0.0139, 0.0060, 0.0649,\n",
       "           0.1990, 0.0303, 0.1821, 0.3310, 0.0304, 0.0270]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])),\n",
       " tensor([[0.0598, 0.0146, 0.0570, 0.0296, 0.0374, 0.0485, 0.0337, 0.0078, 0.0400,\n",
       "          0.0612, 0.0171, 0.2491, 0.2284, 0.0632, 0.0527]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0598, 0.0146, 0.0570, 0.0296, 0.0374, 0.0485, 0.0337, 0.0078, 0.0400,\n",
       "           0.0612, 0.0171, 0.2491, 0.2284, 0.0632, 0.0527]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0951, 0.0246, 0.1479, 0.0610, 0.0965, 0.1279, 0.0267, 0.0144, 0.0566,\n",
       "          0.0404, 0.0098, 0.0529, 0.0717, 0.0789, 0.0955]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0951, 0.0246, 0.1479, 0.0610, 0.0965, 0.1279, 0.0267, 0.0144, 0.0566,\n",
       "           0.0404, 0.0098, 0.0529, 0.0717, 0.0789, 0.0955]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1309, 0.0313, 0.1444, 0.0671, 0.1159, 0.2003, 0.0348, 0.0137, 0.0216,\n",
       "          0.0074, 0.0049, 0.0268, 0.0402, 0.0791, 0.0817]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1309, 0.0313, 0.1444, 0.0671, 0.1159, 0.2003, 0.0348, 0.0137, 0.0216,\n",
       "           0.0074, 0.0049, 0.0268, 0.0402, 0.0791, 0.0817]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0862, 0.0246, 0.1321, 0.0632, 0.0668, 0.1151, 0.0330, 0.0149, 0.0639,\n",
       "          0.0807, 0.0177, 0.0519, 0.1221, 0.0540, 0.0738]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0862, 0.0246, 0.1321, 0.0632, 0.0668, 0.1151, 0.0330, 0.0149, 0.0639,\n",
       "           0.0807, 0.0177, 0.0519, 0.1221, 0.0540, 0.0738]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0216, 0.0130, 0.0196, 0.0265, 0.0195, 0.0206, 0.0158, 0.0049, 0.0593,\n",
       "          0.2538, 0.0221, 0.2599, 0.2100, 0.0329, 0.0205]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0216, 0.0130, 0.0196, 0.0265, 0.0195, 0.0206, 0.0158, 0.0049, 0.0593,\n",
       "           0.2538, 0.0221, 0.2599, 0.2100, 0.0329, 0.0205]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " tensor([[1.4097e-04, 1.0874e-04, 2.6458e-04, 1.2276e-04, 2.6713e-04, 3.1994e-04,\n",
       "          1.4712e-04, 4.0043e-05, 3.3490e-02, 9.2222e-01, 6.4807e-04, 6.0008e-03,\n",
       "          3.5339e-02, 2.4723e-04, 6.4262e-04]], grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[1.4097e-04, 1.0874e-04, 2.6458e-04, 1.2276e-04, 2.6713e-04, 3.1994e-04,\n",
       "           1.4712e-04, 4.0043e-05, 3.3490e-02, 9.2222e-01, 6.4807e-04, 6.0008e-03,\n",
       "           3.5339e-02, 2.4723e-04, 6.4262e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1055, 0.0188, 0.1203, 0.0698, 0.0881, 0.1006, 0.0318, 0.0149, 0.0362,\n",
       "          0.0334, 0.0139, 0.0543, 0.0768, 0.0704, 0.1650]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1055, 0.0188, 0.1203, 0.0698, 0.0881, 0.1006, 0.0318, 0.0149, 0.0362,\n",
       "           0.0334, 0.0139, 0.0543, 0.0768, 0.0704, 0.1650]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       " tensor([[0.0701, 0.0382, 0.2012, 0.0613, 0.1008, 0.1167, 0.0276, 0.0153, 0.0346,\n",
       "          0.0213, 0.0123, 0.0374, 0.0413, 0.0891, 0.1328]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0701, 0.0382, 0.2012, 0.0613, 0.1008, 0.1167, 0.0276, 0.0153, 0.0346,\n",
       "           0.0213, 0.0123, 0.0374, 0.0413, 0.0891, 0.1328]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.0857, 0.0374, 0.1417, 0.0608, 0.0895, 0.1326, 0.0430, 0.0258, 0.0345,\n",
       "          0.0179, 0.0147, 0.0547, 0.0544, 0.0644, 0.1428]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0857, 0.0374, 0.1417, 0.0608, 0.0895, 0.1326, 0.0430, 0.0258, 0.0345,\n",
       "           0.0179, 0.0147, 0.0547, 0.0544, 0.0644, 0.1428]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])),\n",
       " tensor([[0.1061, 0.0285, 0.1661, 0.0709, 0.1112, 0.1416, 0.0366, 0.0138, 0.0398,\n",
       "          0.0214, 0.0103, 0.0277, 0.0545, 0.0740, 0.0974]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1061, 0.0285, 0.1661, 0.0709, 0.1112, 0.1416, 0.0366, 0.0138, 0.0398,\n",
       "           0.0214, 0.0103, 0.0277, 0.0545, 0.0740, 0.0974]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1162, 0.0386, 0.1338, 0.0636, 0.1130, 0.2056, 0.0475, 0.0136, 0.0174,\n",
       "          0.0058, 0.0054, 0.0246, 0.0302, 0.0600, 0.1247]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1162, 0.0386, 0.1338, 0.0636, 0.1130, 0.2056, 0.0475, 0.0136, 0.0174,\n",
       "           0.0058, 0.0054, 0.0246, 0.0302, 0.0600, 0.1247]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0315, 0.0304, 0.0500, 0.0250, 0.0435, 0.0423, 0.0461, 0.0152, 0.0872,\n",
       "          0.2716, 0.0466, 0.1130, 0.1097, 0.0404, 0.0475]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0315, 0.0304, 0.0500, 0.0250, 0.0435, 0.0423, 0.0461, 0.0152, 0.0872,\n",
       "           0.2716, 0.0466, 0.1130, 0.1097, 0.0404, 0.0475]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0617, 0.0153, 0.0591, 0.0539, 0.0760, 0.0513, 0.0313, 0.0101, 0.1296,\n",
       "          0.1919, 0.0132, 0.0704, 0.0826, 0.0667, 0.0869]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0617, 0.0153, 0.0591, 0.0539, 0.0760, 0.0513, 0.0313, 0.0101, 0.1296,\n",
       "           0.1919, 0.0132, 0.0704, 0.0826, 0.0667, 0.0869]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[8.1074e-05, 6.6966e-05, 1.1729e-04, 2.4396e-04, 1.4658e-04, 2.1238e-04,\n",
       "          1.0392e-04, 3.8792e-05, 1.3043e-02, 9.7933e-01, 3.4430e-04, 2.0356e-03,\n",
       "          3.5275e-03, 1.4418e-04, 5.6239e-04]], grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[8.1074e-05, 6.6966e-05, 1.1729e-04, 2.4396e-04, 1.4658e-04, 2.1238e-04,\n",
       "           1.0392e-04, 3.8792e-05, 1.3043e-02, 9.7933e-01, 3.4430e-04, 2.0356e-03,\n",
       "           3.5275e-03, 1.4418e-04, 5.6239e-04]], grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0990, 0.0323, 0.1802, 0.0534, 0.1085, 0.1515, 0.0211, 0.0143, 0.0211,\n",
       "          0.0073, 0.0074, 0.0286, 0.0540, 0.0841, 0.1372]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0990, 0.0323, 0.1802, 0.0534, 0.1085, 0.1515, 0.0211, 0.0143, 0.0211,\n",
       "           0.0073, 0.0074, 0.0286, 0.0540, 0.0841, 0.1372]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.1061, 0.0195, 0.1403, 0.1105, 0.1408, 0.1057, 0.0284, 0.0119, 0.0310,\n",
       "          0.0081, 0.0037, 0.0213, 0.0292, 0.0617, 0.1818]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.1061, 0.0195, 0.1403, 0.1105, 0.1408, 0.1057, 0.0284, 0.0119, 0.0310,\n",
       "           0.0081, 0.0037, 0.0213, 0.0292, 0.0617, 0.1818]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       " tensor([[0.0155, 0.0160, 0.0233, 0.0145, 0.0128, 0.0153, 0.0124, 0.0066, 0.0208,\n",
       "          0.0809, 0.0170, 0.2751, 0.4524, 0.0187, 0.0187]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " (tensor([[0.0155, 0.0160, 0.0233, 0.0145, 0.0128, 0.0153, 0.0124, 0.0066, 0.0208,\n",
       "           0.0809, 0.0170, 0.2751, 0.4524, 0.0187, 0.0187]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])),\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn.state_dict(), 'gcn_model55-52-0001-100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gcn1): GCNConv(7, 64)\n",
       "  (r1): ReLU()\n",
       "  (gcn2): GCNConv(64, 128)\n",
       "  (linear): Linear(in_features=128, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool, global_sort_pool\n",
    "from torch_geometric.nn import GCNConv, JumpingKnowledge, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GCN, self).__init__()\n",
    "        \n",
    "#         num_node_features = 7\n",
    "#         num_output_classes = 15\n",
    "        \n",
    "#         self.conv1 = GCNConv(num_node_features, 64)\n",
    "#         self.conv2 = GCNConv(64, 64)\n",
    "        \n",
    "#         self.jk = JumpingKnowledge(mode='cat')\n",
    "        \n",
    "#         self.conv3 = GCNConv(128, 128)\n",
    "#         self.linear = nn.Linear(128, num_output_classes)  # Adjust the input size based on JK mode\n",
    "        \n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x1 = F.relu(self.conv1(x, edge_index))\n",
    "#         x2 = F.relu(self.conv2(x1, edge_index))\n",
    "#         # x3 = F.relu(self.conv3(x2, edge_index))\n",
    "        \n",
    "#         # x = self.jk([x1, x2])\n",
    "#         x = F.dropout(x, p=0.2, training=self.training)\n",
    "#         x = self.conv3(x, edge_index)\n",
    "        \n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         x = global_mean_pool(x, batch)\n",
    "        \n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.linear(x)\n",
    "        \n",
    "#         probs = F.softmax(x, dim=-1)\n",
    "        \n",
    "#         return probs\n",
    "        \n",
    "        \n",
    "        \n",
    "        # KNN\n",
    "        # embeddings\n",
    "        # PCA\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        num_node_features = 7\n",
    "        num_output_classes = 15\n",
    "        \n",
    "        # num_channels = 32\n",
    "        \n",
    "        self.gcn1 = GCNConv(num_node_features, 64)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.gcn2 = GCNConv(64, 128)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=128, out_features=num_output_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "    \n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.r1(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        \n",
    "        x_temp = global_mean_pool(x, batch)\n",
    "        \n",
    "        \n",
    "        x = F.dropout(x_temp, p = 0.7, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        probs = F.softmax(x, dim=-1)\n",
    "        \n",
    "        return probs, x_temp\n",
    "    \n",
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.0809, Test Acc: 0.0769\n",
      "Epoch: 002, Train Acc: 0.0809, Test Acc: 0.0769\n",
      "Epoch: 003, Train Acc: 0.0809, Test Acc: 0.0769\n",
      "Epoch: 004, Train Acc: 0.0809, Test Acc: 0.0769\n",
      "Epoch: 005, Train Acc: 0.0809, Test Acc: 0.0769\n",
      "Epoch: 006, Train Acc: 0.0809, Test Acc: 0.0769\n",
      "Epoch: 007, Train Acc: 0.1100, Test Acc: 0.1410\n",
      "Epoch: 008, Train Acc: 0.1327, Test Acc: 0.1410\n",
      "Epoch: 009, Train Acc: 0.1456, Test Acc: 0.1282\n",
      "Epoch: 010, Train Acc: 0.1036, Test Acc: 0.1026\n",
      "Epoch: 011, Train Acc: 0.0906, Test Acc: 0.0897\n",
      "Epoch: 012, Train Acc: 0.0777, Test Acc: 0.0769\n",
      "Epoch: 013, Train Acc: 0.1392, Test Acc: 0.1154\n",
      "Epoch: 014, Train Acc: 0.1553, Test Acc: 0.1538\n",
      "Epoch: 015, Train Acc: 0.1489, Test Acc: 0.1410\n",
      "Epoch: 016, Train Acc: 0.1392, Test Acc: 0.1154\n",
      "Epoch: 017, Train Acc: 0.2006, Test Acc: 0.1667\n",
      "Epoch: 018, Train Acc: 0.2039, Test Acc: 0.1667\n",
      "Epoch: 019, Train Acc: 0.2168, Test Acc: 0.1795\n",
      "Epoch: 020, Train Acc: 0.2071, Test Acc: 0.1795\n",
      "Epoch: 021, Train Acc: 0.2071, Test Acc: 0.1795\n",
      "Epoch: 022, Train Acc: 0.2104, Test Acc: 0.1795\n",
      "Epoch: 023, Train Acc: 0.2168, Test Acc: 0.2308\n",
      "Epoch: 024, Train Acc: 0.2201, Test Acc: 0.2308\n",
      "Epoch: 025, Train Acc: 0.1974, Test Acc: 0.2051\n",
      "Epoch: 026, Train Acc: 0.2136, Test Acc: 0.1923\n",
      "Epoch: 027, Train Acc: 0.2104, Test Acc: 0.1923\n",
      "Epoch: 028, Train Acc: 0.2071, Test Acc: 0.1923\n",
      "Epoch: 029, Train Acc: 0.1909, Test Acc: 0.1923\n",
      "Epoch: 030, Train Acc: 0.2265, Test Acc: 0.2179\n",
      "Epoch: 031, Train Acc: 0.2265, Test Acc: 0.2051\n",
      "Epoch: 032, Train Acc: 0.2136, Test Acc: 0.2051\n",
      "Epoch: 033, Train Acc: 0.2136, Test Acc: 0.1923\n",
      "Epoch: 034, Train Acc: 0.2104, Test Acc: 0.2051\n",
      "Epoch: 035, Train Acc: 0.2168, Test Acc: 0.2051\n",
      "Epoch: 036, Train Acc: 0.2201, Test Acc: 0.2051\n",
      "Epoch: 037, Train Acc: 0.2589, Test Acc: 0.2436\n",
      "Epoch: 038, Train Acc: 0.2265, Test Acc: 0.2051\n",
      "Epoch: 039, Train Acc: 0.2330, Test Acc: 0.2051\n",
      "Epoch: 040, Train Acc: 0.2460, Test Acc: 0.2308\n",
      "Epoch: 041, Train Acc: 0.2265, Test Acc: 0.1795\n",
      "Epoch: 042, Train Acc: 0.2265, Test Acc: 0.1795\n",
      "Epoch: 043, Train Acc: 0.2524, Test Acc: 0.2179\n",
      "Epoch: 044, Train Acc: 0.2233, Test Acc: 0.2308\n",
      "Epoch: 045, Train Acc: 0.2492, Test Acc: 0.2692\n",
      "Epoch: 046, Train Acc: 0.2039, Test Acc: 0.2051\n",
      "Epoch: 047, Train Acc: 0.2006, Test Acc: 0.2308\n",
      "Epoch: 048, Train Acc: 0.2621, Test Acc: 0.2949\n",
      "Epoch: 049, Train Acc: 0.2654, Test Acc: 0.2436\n",
      "Epoch: 050, Train Acc: 0.2751, Test Acc: 0.2308\n",
      "Epoch: 051, Train Acc: 0.2589, Test Acc: 0.2692\n",
      "Epoch: 052, Train Acc: 0.2654, Test Acc: 0.2821\n",
      "Epoch: 053, Train Acc: 0.2039, Test Acc: 0.1923\n",
      "Epoch: 054, Train Acc: 0.2104, Test Acc: 0.1923\n",
      "Epoch: 055, Train Acc: 0.2039, Test Acc: 0.1795\n",
      "Epoch: 056, Train Acc: 0.1909, Test Acc: 0.1795\n",
      "Epoch: 057, Train Acc: 0.2233, Test Acc: 0.2179\n",
      "Epoch: 058, Train Acc: 0.2104, Test Acc: 0.2051\n",
      "Epoch: 059, Train Acc: 0.1942, Test Acc: 0.2051\n",
      "Epoch: 060, Train Acc: 0.1877, Test Acc: 0.1923\n",
      "Epoch: 061, Train Acc: 0.2104, Test Acc: 0.1923\n",
      "Epoch: 062, Train Acc: 0.2136, Test Acc: 0.1923\n",
      "Epoch: 063, Train Acc: 0.2168, Test Acc: 0.2051\n",
      "Epoch: 064, Train Acc: 0.2298, Test Acc: 0.2179\n",
      "Epoch: 065, Train Acc: 0.2330, Test Acc: 0.2179\n",
      "Epoch: 066, Train Acc: 0.2816, Test Acc: 0.2564\n",
      "Epoch: 067, Train Acc: 0.2168, Test Acc: 0.2051\n",
      "Epoch: 068, Train Acc: 0.2136, Test Acc: 0.1923\n",
      "Epoch: 069, Train Acc: 0.2654, Test Acc: 0.2436\n",
      "Epoch: 070, Train Acc: 0.2233, Test Acc: 0.2308\n",
      "Epoch: 071, Train Acc: 0.2362, Test Acc: 0.2308\n",
      "Epoch: 072, Train Acc: 0.2492, Test Acc: 0.2564\n",
      "Epoch: 073, Train Acc: 0.2330, Test Acc: 0.2564\n",
      "Epoch: 074, Train Acc: 0.2427, Test Acc: 0.2436\n",
      "Epoch: 075, Train Acc: 0.2395, Test Acc: 0.2564\n",
      "Epoch: 076, Train Acc: 0.2330, Test Acc: 0.2564\n",
      "Epoch: 077, Train Acc: 0.2330, Test Acc: 0.2308\n",
      "Epoch: 078, Train Acc: 0.2233, Test Acc: 0.2308\n",
      "Epoch: 079, Train Acc: 0.2233, Test Acc: 0.2308\n",
      "Epoch: 080, Train Acc: 0.2395, Test Acc: 0.2308\n",
      "Epoch: 081, Train Acc: 0.2265, Test Acc: 0.2308\n",
      "Epoch: 082, Train Acc: 0.2233, Test Acc: 0.2308\n",
      "Epoch: 083, Train Acc: 0.2233, Test Acc: 0.2308\n",
      "Epoch: 084, Train Acc: 0.2395, Test Acc: 0.2308\n",
      "Epoch: 085, Train Acc: 0.2039, Test Acc: 0.2051\n",
      "Epoch: 086, Train Acc: 0.2039, Test Acc: 0.2051\n",
      "Epoch: 087, Train Acc: 0.2071, Test Acc: 0.2179\n",
      "Epoch: 088, Train Acc: 0.2071, Test Acc: 0.2179\n",
      "Epoch: 089, Train Acc: 0.2039, Test Acc: 0.2051\n",
      "Epoch: 090, Train Acc: 0.2136, Test Acc: 0.2308\n",
      "Epoch: 091, Train Acc: 0.2298, Test Acc: 0.2308\n",
      "Epoch: 092, Train Acc: 0.2298, Test Acc: 0.2308\n",
      "Epoch: 093, Train Acc: 0.2233, Test Acc: 0.2308\n",
      "Epoch: 094, Train Acc: 0.2136, Test Acc: 0.2308\n",
      "Epoch: 095, Train Acc: 0.2265, Test Acc: 0.2308\n",
      "Epoch: 096, Train Acc: 0.2395, Test Acc: 0.2308\n",
      "Epoch: 097, Train Acc: 0.2330, Test Acc: 0.2308\n",
      "Epoch: 098, Train Acc: 0.2330, Test Acc: 0.2308\n",
      "Epoch: 099, Train Acc: 0.2395, Test Acc: 0.2308\n",
      "Epoch: 100, Train Acc: 0.2330, Test Acc: 0.2436\n",
      "Epoch: 101, Train Acc: 0.2265, Test Acc: 0.2179\n",
      "Epoch: 102, Train Acc: 0.2071, Test Acc: 0.2179\n",
      "Epoch: 103, Train Acc: 0.2427, Test Acc: 0.2308\n",
      "Epoch: 104, Train Acc: 0.2427, Test Acc: 0.2308\n",
      "Epoch: 105, Train Acc: 0.2427, Test Acc: 0.2308\n",
      "Epoch: 106, Train Acc: 0.2362, Test Acc: 0.2179\n",
      "Epoch: 107, Train Acc: 0.2589, Test Acc: 0.2308\n",
      "Epoch: 108, Train Acc: 0.2362, Test Acc: 0.2436\n",
      "Epoch: 109, Train Acc: 0.2362, Test Acc: 0.2308\n",
      "Epoch: 110, Train Acc: 0.2330, Test Acc: 0.2436\n",
      "Epoch: 111, Train Acc: 0.2460, Test Acc: 0.2436\n",
      "Epoch: 112, Train Acc: 0.2460, Test Acc: 0.2436\n",
      "Epoch: 113, Train Acc: 0.2913, Test Acc: 0.2564\n",
      "Epoch: 114, Train Acc: 0.2880, Test Acc: 0.2308\n",
      "Epoch: 115, Train Acc: 0.2621, Test Acc: 0.2308\n",
      "Epoch: 116, Train Acc: 0.2298, Test Acc: 0.2564\n",
      "Epoch: 117, Train Acc: 0.2362, Test Acc: 0.2564\n",
      "Epoch: 118, Train Acc: 0.2265, Test Acc: 0.2308\n",
      "Epoch: 119, Train Acc: 0.2427, Test Acc: 0.2564\n",
      "Epoch: 120, Train Acc: 0.2686, Test Acc: 0.2436\n",
      "Epoch: 121, Train Acc: 0.2848, Test Acc: 0.2051\n",
      "Epoch: 122, Train Acc: 0.2783, Test Acc: 0.2051\n",
      "Epoch: 123, Train Acc: 0.2880, Test Acc: 0.2308\n",
      "Epoch: 124, Train Acc: 0.2848, Test Acc: 0.2308\n",
      "Epoch: 125, Train Acc: 0.2524, Test Acc: 0.2308\n",
      "Epoch: 126, Train Acc: 0.2362, Test Acc: 0.2179\n",
      "Epoch: 127, Train Acc: 0.2557, Test Acc: 0.2179\n",
      "Epoch: 128, Train Acc: 0.2362, Test Acc: 0.2179\n",
      "Epoch: 129, Train Acc: 0.2621, Test Acc: 0.2692\n",
      "Epoch: 130, Train Acc: 0.2880, Test Acc: 0.2692\n",
      "Epoch: 131, Train Acc: 0.2945, Test Acc: 0.2692\n",
      "Epoch: 132, Train Acc: 0.2977, Test Acc: 0.2692\n",
      "Epoch: 133, Train Acc: 0.2848, Test Acc: 0.2564\n",
      "Epoch: 134, Train Acc: 0.2848, Test Acc: 0.2564\n",
      "Epoch: 135, Train Acc: 0.2718, Test Acc: 0.2308\n",
      "Epoch: 136, Train Acc: 0.3204, Test Acc: 0.2949\n",
      "Epoch: 137, Train Acc: 0.3074, Test Acc: 0.2692\n",
      "Epoch: 138, Train Acc: 0.3042, Test Acc: 0.2179\n",
      "Epoch: 139, Train Acc: 0.2751, Test Acc: 0.2564\n",
      "Epoch: 140, Train Acc: 0.2816, Test Acc: 0.2436\n",
      "Epoch: 141, Train Acc: 0.3366, Test Acc: 0.3077\n",
      "Epoch: 142, Train Acc: 0.2945, Test Acc: 0.2821\n",
      "Epoch: 143, Train Acc: 0.3754, Test Acc: 0.3333\n",
      "Epoch: 144, Train Acc: 0.3657, Test Acc: 0.2949\n",
      "Epoch: 145, Train Acc: 0.3172, Test Acc: 0.2692\n",
      "Epoch: 146, Train Acc: 0.2783, Test Acc: 0.2308\n",
      "Epoch: 147, Train Acc: 0.3107, Test Acc: 0.2308\n",
      "Epoch: 148, Train Acc: 0.3172, Test Acc: 0.2436\n",
      "Epoch: 149, Train Acc: 0.3463, Test Acc: 0.2692\n",
      "Epoch: 150, Train Acc: 0.3689, Test Acc: 0.2949\n",
      "Training duration: 283.9847345352173 seconds\n"
     ]
    }
   ],
   "source": [
    "out_prob = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gcn = GCN()\n",
    "gcn = gcn.to(device)\n",
    "# print(gcn.parameters())\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "# loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "out_labels = []\n",
    "\n",
    "# training_running_loss = 0.0\n",
    "def train(train_loader, embeddings):\n",
    "    \n",
    "    gcn.train()\n",
    "    # print(gcn.parameters())\n",
    "    for batch_data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        for data in batch_data:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            #forward pass\n",
    "            out, e = gcn(data.x, data.edge_index, data.batch)\n",
    "            embeddings.append(e)\n",
    "            out_labels.append(out)\n",
    "            # print(out)\n",
    "            # calculate the loss\n",
    "            loss = criterion(out, data.y)\n",
    "            # zero the gradients of the weights so that the gradients are not accumulated\n",
    "            # calculate the gradients using backpropagation\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss\n",
    "            # training_running_loss += loss.detach().item()\n",
    "            \n",
    "            out_labels.append((out, data.y))\n",
    "        out_prob.append(out)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "testing_labels = []\n",
    "def test(loader):\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    for batch_data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        for data in batch_data:\n",
    "            out, _ = gcn(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            testing_labels.append(pred)\n",
    "            y_label = (data.y.tolist())\n",
    "            y_label = y_label[0].index(1.0)\n",
    "            pred_label = (pred.tolist())[0]\n",
    "            # print(pred_label)\n",
    "            # print(y_label)\n",
    "            if y_label == pred_label:\n",
    "                correct += 1            \n",
    "            # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 150\n",
    "# Your training code here\n",
    "for epoch in range(num_epochs):\n",
    "    embeddings = []\n",
    "    embeddings = train(train_loader, embeddings)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch + 1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(\"Training duration:\", duration, \"seconds\")\n",
    "# with open(\"out_labels.txt\", \"w\") as output:\n",
    "#         output.write(str(out_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn.state_dict(), 'gcn_model34-33-0001-150.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gcn.eval()\n",
    "    train_embeddings = []\n",
    "    test_embeddings = []\n",
    "    \n",
    "    for data in X_train:\n",
    "        res, res2 = gcn(data.x, data.edge_index, data.batch)\n",
    "        res2 = res2.detach().numpy()\n",
    "        # res2_flat = [item for sublist in res2 for item in sublist]\n",
    "        # print(len(data.x))\n",
    "        # print(len(res2[0]))\n",
    "        # print(len(res2_flat))\n",
    "        train_embeddings.append(res2[0])\n",
    "    \n",
    "    for data in X_test:\n",
    "        res, res2 = gcn(data.x, data.edge_index, data.batch)\n",
    "        res2 = res2.detach().numpy()\n",
    "        test_embeddings.append(res2[0])\n",
    "\n",
    "# Use node embeddings as features for classical ML model (e.g., logistic regression)\n",
    "X_train_em, y_train = train_embeddings, y_train\n",
    "X_test_em, y_test = test_embeddings, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train_em:\n",
    "    if len(x) != 128:\n",
    "        print(len(x))\n",
    "        \n",
    "for x in X_test_em:\n",
    "    if len(x) != 128:\n",
    "        print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=7)\n",
    "X_train_pca = pca.fit_transform(X_train_em)\n",
    "X_test_pca = pca.transform(X_test_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train_pca:\n",
    "    if len(x) != 7:\n",
    "        print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6538461538461539\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train KNN classifier on PCA-transformed data\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluate KNN classifier on test set\n",
    "y_pred = knn_classifier.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(knn_classifier, 'knn_model_embeddings_pca_65.pkl')\n",
    "\n",
    "KNN_loaded_model = joblib.load('knn_model_embeddings_pca_65.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(11, 12): 6,\n",
       " (13, 11): 2,\n",
       " (0, 5): 2,\n",
       " (8, 9): 1,\n",
       " (0, 3): 1,\n",
       " (0, 7): 2,\n",
       " (0, 2): 6,\n",
       " (11, 1): 3,\n",
       " (8, 3): 1,\n",
       " (0, 14): 6,\n",
       " (13, 5): 2,\n",
       " (0, 13): 5,\n",
       " (8, 6): 4,\n",
       " (0, 4): 6,\n",
       " (8, 0): 1,\n",
       " (13, 3): 1,\n",
       " (9, 3): 2,\n",
       " (8, 5): 2,\n",
       " (13, 2): 1,\n",
       " (0, 1): 1}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num = random.randint(0, len(verilog_files))\n",
    "# print(verilog_files[num])\n",
    "output_dict = dict()\n",
    "for data_trial in test_loader.dataset:\n",
    "    \n",
    "    out,_ = gcn(data_trial.x, data_trial.edge_index, data_trial.batch)\n",
    "    pred = out.argmax(dim=1)\n",
    "    \n",
    "    if pred.tolist()[0] != (data_trial.y.tolist())[0].index(1.0):\n",
    "        output_dict[(pred.tolist()[0], (data_trial.y.tolist())[0].index(1.0))] = output_dict.get((pred.tolist()[0], (data_trial.y.tolist())[0].index(1.0)), 0) +1\n",
    "        # print(pred.tolist())\n",
    "        # print((data_trial.y.tolist())[0].index(1.0))\n",
    "\n",
    "\n",
    "output_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gcn = GCN()\n",
    "# gcn.load_state_dict(torch.load('gcn_model85-59-0001-200.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 321 verilog files \n",
    "* only 3 features             [type, operation_type, num_of_connections]\n",
    "* no edge attribute\n",
    "* 18 classes \n",
    "* 200 epochs \n",
    "* learning rate = 0.01\n",
    "* Dropoout = 0.4\n",
    "* Adam Optimizer\n",
    "* train 70, test 30 (on whole dataset, not each class)\n",
    "* time of training = seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train acc:  0.2902\n",
    "* Test Acc: 0.1959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Clean dataset (by removing unnecessay, uninformative or wrong code files)\n",
    "2) remove reduntant parsing (different files but same parsing)\n",
    "3) include more informative features\n",
    "4) improve encoding format\n",
    "5) try using less classes (most important ones, so that less classes but more balanced dataset)\n",
    "6) adding more files\n",
    "7) adjusting hyperparameters such as learning rate, dropout, ...etc\n",
    "8) splitting train, val, test\n",
    "9) using equal percentages of each class (adjusting splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "droput 0.4\n",
    "\n",
    "314 files\n",
    "\n",
    "17 features (node_type)\n",
    "\n",
    "16 classes\n",
    "\n",
    "conv relu conv relu conv relu conv linear\n",
    "\n",
    "train = 40, test = 27\n",
    "\n",
    "200 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as 5 but 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = 43, test = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "conv relu conv relu conv dropout linear\n",
    "\n",
    "9 classes\n",
    "\n",
    "164 file\n",
    "\n",
    "train = 34, test = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 0.001\n",
    "\n",
    "9 classes\n",
    "\n",
    "conv relu conv relu conv dropout linear \n",
    "\n",
    "train = 64, test = 52\n",
    "\n",
    "164 \n",
    "\n",
    "17 features (node type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2022.5.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
