{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "!pip install torch_geometric -q\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['done\\\\adder11.txt', 'done\\\\adder12.txt', 'done\\\\adder13.txt', 'done\\\\adder14.txt', 'done\\\\adder15.txt', 'done\\\\adder16.txt', 'done\\\\adder17.txt', 'done\\\\adder18.txt', 'done\\\\adder19.txt', 'done\\\\adder2.txt', 'done\\\\adder20.txt', 'done\\\\adder5.txt', 'done\\\\adder6.txt', 'done\\\\adder8.txt', 'done\\\\ALU10.txt', 'done\\\\ALU13.txt', 'done\\\\ALU14.txt', 'done\\\\ALU15.txt', 'done\\\\ALU2.txt', 'done\\\\ALU6.txt', 'done\\\\ALU7.txt', 'done\\\\ALU8.txt', 'done\\\\ALU9.txt', 'done\\\\and1.txt', 'done\\\\and10.txt', 'done\\\\and12.txt', 'done\\\\and13.txt', 'done\\\\and14.txt', 'done\\\\and15.txt', 'done\\\\and16.txt', 'done\\\\and17.txt', 'done\\\\and18.txt', 'done\\\\and19.txt', 'done\\\\and2.txt', 'done\\\\and20.txt', 'done\\\\and21.txt', 'done\\\\and23.txt', 'done\\\\and25.txt', 'done\\\\and26.txt', 'done\\\\and27.txt', 'done\\\\and28.txt', 'done\\\\and29.txt', 'done\\\\and3.txt', 'done\\\\and30.txt', 'done\\\\and6.txt', 'done\\\\and7.txt', 'done\\\\and8.txt', 'done\\\\and9.txt', 'done\\\\comparator1.txt', 'done\\\\comparator13.txt', 'done\\\\comparator14.txt', 'done\\\\comparator15.txt', 'done\\\\comparator16.txt', 'done\\\\comparator17.txt', 'done\\\\comparator18.txt', 'done\\\\comparator19.txt', 'done\\\\comparator2.txt', 'done\\\\comparator20.txt', 'done\\\\comparator21.txt', 'done\\\\comparator22.txt', 'done\\\\comparator23.txt', 'done\\\\comparator6.txt', 'done\\\\comparator7.txt', 'done\\\\comparator8.txt', 'done\\\\comparator9.txt', 'done\\\\decoder1.txt', 'done\\\\decoder11.txt', 'done\\\\decoder12.txt', 'done\\\\decoder13.txt', 'done\\\\decoder14.txt', 'done\\\\decoder15.txt', 'done\\\\decoder16.txt', 'done\\\\decoder17.txt', 'done\\\\decoder18.txt', 'done\\\\decoder19.txt', 'done\\\\decoder2.txt', 'done\\\\decoder20.txt', 'done\\\\decoder21.txt', 'done\\\\decoder22.txt', 'done\\\\decoder23.txt', 'done\\\\decoder24.txt', 'done\\\\decoder25.txt', 'done\\\\decoder26.txt', 'done\\\\decoder27.txt', 'done\\\\decoder28.txt', 'done\\\\decoder29.txt', 'done\\\\decoder3.txt', 'done\\\\decoder30.txt', 'done\\\\decoder31.txt', 'done\\\\decoder32.txt', 'done\\\\decoder33.txt', 'done\\\\decoder34.txt', 'done\\\\decoder35.txt', 'done\\\\decoder36.txt', 'done\\\\decoder37.txt', 'done\\\\decoder38.txt', 'done\\\\decoder39.txt', 'done\\\\decoder4.txt', 'done\\\\decoder40.txt', 'done\\\\decoder41.txt', 'done\\\\decoder43.txt', 'done\\\\decoder44.txt', 'done\\\\decoder45.txt', 'done\\\\decoder46.txt', 'done\\\\decoder47.txt', 'done\\\\decoder48.txt', 'done\\\\decoder49.txt', 'done\\\\decoder50.txt', 'done\\\\encoder1.txt', 'done\\\\encoder10.txt', 'done\\\\encoder11.txt', 'done\\\\encoder12.txt', 'done\\\\encoder13.txt', 'done\\\\encoder14.txt', 'done\\\\encoder15.txt', 'done\\\\encoder16.txt', 'done\\\\encoder17.txt', 'done\\\\encoder18.txt', 'done\\\\encoder19.txt', 'done\\\\encoder2.txt', 'done\\\\encoder20.txt', 'done\\\\encoder21.txt', 'done\\\\encoder24.txt', 'done\\\\encoder25.txt', 'done\\\\encoder26.txt', 'done\\\\encoder27.txt', 'done\\\\encoder28.txt', 'done\\\\encoder29.txt', 'done\\\\encoder3.txt', 'done\\\\encoder30.txt', 'done\\\\encoder31.txt', 'done\\\\encoder32.txt', 'done\\\\encoder33.txt', 'done\\\\encoder35.txt', 'done\\\\encoder36.txt', 'done\\\\encoder37.txt', 'done\\\\encoder38.txt', 'done\\\\encoder4.txt', 'done\\\\encoder40.txt', 'done\\\\encoder41.txt', 'done\\\\encoder42.txt', 'done\\\\encoder43.txt', 'done\\\\encoder44.txt', 'done\\\\encoder45.txt', 'done\\\\encoder46.txt', 'done\\\\encoder47.txt', 'done\\\\encoder48.txt', 'done\\\\encoder5.txt', 'done\\\\encoder50.txt', 'done\\\\encoder51.txt', 'done\\\\encoder52.txt', 'done\\\\encoder6.txt', 'done\\\\encoder7.txt', 'done\\\\encoder8.txt', 'done\\\\encoder9.txt', 'done\\\\mult1.txt', 'done\\\\mult10.txt', 'done\\\\mult11.txt', 'done\\\\mult12.txt', 'done\\\\mult13.txt', 'done\\\\mult14.txt', 'done\\\\mult15.txt', 'done\\\\mult16.txt', 'done\\\\mult17.txt', 'done\\\\mult18.txt', 'done\\\\mult19.txt', 'done\\\\mult2.txt', 'done\\\\mult3.txt', 'done\\\\mult30.txt', 'done\\\\mult31.txt', 'done\\\\mult32.txt', 'done\\\\mult33.txt', 'done\\\\mult34.txt', 'done\\\\mult35.txt', 'done\\\\mult4.txt', 'done\\\\mult5.txt', 'done\\\\mult6.txt', 'done\\\\mult8.txt', 'done\\\\mult9.txt', 'done\\\\mux1.txt', 'done\\\\mux10.txt', 'done\\\\mux11.txt', 'done\\\\mux12.txt', 'done\\\\mux13.txt', 'done\\\\mux14.txt', 'done\\\\mux15.txt', 'done\\\\mux16.txt', 'done\\\\mux17.txt', 'done\\\\mux18.txt', 'done\\\\mux19.txt', 'done\\\\mux2.txt', 'done\\\\mux20.txt', 'done\\\\mux21.txt', 'done\\\\mux22.txt', 'done\\\\mux23.txt', 'done\\\\mux24.txt', 'done\\\\mux25.txt', 'done\\\\mux26.txt', 'done\\\\mux27.txt', 'done\\\\mux28.txt', 'done\\\\mux29.txt', 'done\\\\mux3.txt', 'done\\\\mux30.txt', 'done\\\\mux31.txt', 'done\\\\mux33.txt', 'done\\\\mux34.txt', 'done\\\\mux35.txt', 'done\\\\mux36.txt', 'done\\\\mux37.txt', 'done\\\\mux38.txt', 'done\\\\mux39.txt', 'done\\\\mux4.txt', 'done\\\\mux40.txt', 'done\\\\mux41.txt', 'done\\\\mux42.txt', 'done\\\\mux43.txt', 'done\\\\mux44.txt', 'done\\\\mux45.txt', 'done\\\\mux47.txt', 'done\\\\mux48.txt', 'done\\\\mux49.txt', 'done\\\\mux5.txt', 'done\\\\mux50.txt', 'done\\\\mux51.txt', 'done\\\\mux52.txt', 'done\\\\mux53.txt', 'done\\\\mux55.txt', 'done\\\\mux56.txt', 'done\\\\mux57.txt', 'done\\\\mux58.txt', 'done\\\\mux59.txt', 'done\\\\mux6.txt', 'done\\\\mux60.txt', 'done\\\\mux61.txt', 'done\\\\mux62.txt', 'done\\\\mux63.txt', 'done\\\\mux64.txt', 'done\\\\mux66.txt', 'done\\\\mux67.txt', 'done\\\\mux68.txt', 'done\\\\mux69.txt', 'done\\\\mux7.txt', 'done\\\\mux70.txt', 'done\\\\mux71.txt', 'done\\\\mux72.txt', 'done\\\\mux73.txt', 'done\\\\mux74.txt', 'done\\\\mux75.txt', 'done\\\\mux8.txt', 'done\\\\mux9.txt', 'done\\\\nand1.txt', 'done\\\\nand10.txt', 'done\\\\nand11.txt', 'done\\\\nand12.txt', 'done\\\\nand14.txt', 'done\\\\nand15.txt', 'done\\\\nand16.txt', 'done\\\\nand17.txt', 'done\\\\nand18.txt', 'done\\\\nand19.txt', 'done\\\\nand2.txt', 'done\\\\nand20.txt', 'done\\\\nand21.txt', 'done\\\\nand22.txt', 'done\\\\nand3.txt', 'done\\\\nand4.txt', 'done\\\\nand6.txt', 'done\\\\nand7.txt', 'done\\\\nand8.txt', 'done\\\\nand9.txt', 'done\\\\nor1.txt', 'done\\\\nor10.txt', 'done\\\\nor11.txt', 'done\\\\nor12.txt', 'done\\\\nor13.txt', 'done\\\\nor14.txt', 'done\\\\nor15.txt', 'done\\\\nor16.txt', 'done\\\\nor17.txt', 'done\\\\nor2.txt', 'done\\\\nor3.txt', 'done\\\\nor4.txt', 'done\\\\nor6.txt', 'done\\\\nor7.txt', 'done\\\\nor8.txt', 'done\\\\nor9.txt', 'done\\\\not1.txt', 'done\\\\not10.txt', 'done\\\\not11.txt', 'done\\\\not12.txt', 'done\\\\not13.txt', 'done\\\\not14.txt', 'done\\\\not2.txt', 'done\\\\not3.txt', 'done\\\\not6.txt', 'done\\\\not7.txt', 'done\\\\not9.txt', 'done\\\\or1.txt', 'done\\\\or10.txt', 'done\\\\or11.txt', 'done\\\\or12.txt', 'done\\\\or13.txt', 'done\\\\or14.txt', 'done\\\\or15.txt', 'done\\\\or16.txt', 'done\\\\or17.txt', 'done\\\\or18.txt', 'done\\\\or19.txt', 'done\\\\or2.txt', 'done\\\\or20.txt', 'done\\\\or21.txt', 'done\\\\or22.txt', 'done\\\\or23.txt', 'done\\\\or24.txt', 'done\\\\or25.txt', 'done\\\\or26.txt', 'done\\\\or28.txt', 'done\\\\or4.txt', 'done\\\\or7.txt', 'done\\\\or8.txt', 'done\\\\or9.txt', 'done\\\\pe1.txt', 'done\\\\pe14.txt', 'done\\\\pe15.txt', 'done\\\\pe16.txt', 'done\\\\pe17.txt', 'done\\\\pe18.txt', 'done\\\\pe19.txt', 'done\\\\pe2.txt', 'done\\\\pe20.txt', 'done\\\\pe21.txt', 'done\\\\pe22.txt', 'done\\\\pe23.txt', 'done\\\\pe24.txt', 'done\\\\pe25.txt', 'done\\\\pe26.txt', 'done\\\\pe27.txt', 'done\\\\pe28.txt', 'done\\\\pe29.txt', 'done\\\\pe3.txt', 'done\\\\pe30.txt', 'done\\\\pe4.txt', 'done\\\\pe5.txt', 'done\\\\pe6.txt', 'done\\\\pe7.txt', 'done\\\\seg1.txt', 'done\\\\seg2.txt', 'done\\\\seg3.txt', 'done\\\\seg5.txt', 'done\\\\seg9.txt', 'done\\\\sub1.txt', 'done\\\\sub10.txt', 'done\\\\sub11.txt', 'done\\\\sub12.txt', 'done\\\\sub2.txt', 'done\\\\sub5.txt', 'done\\\\sub7.txt', 'done\\\\sub8.txt', 'done\\\\sub9.txt', 'done\\\\xnor1.txt', 'done\\\\xnor10.txt', 'done\\\\xnor11.txt', 'done\\\\xnor12.txt', 'done\\\\xnor13.txt', 'done\\\\xnor14.txt', 'done\\\\xnor15.txt', 'done\\\\xnor16.txt', 'done\\\\xnor17.txt', 'done\\\\xnor18.txt', 'done\\\\xnor19.txt', 'done\\\\xnor2.txt', 'done\\\\xnor20.txt', 'done\\\\xnor21.txt', 'done\\\\xnor22.txt', 'done\\\\xnor23.txt', 'done\\\\xnor24.txt', 'done\\\\xnor25.txt', 'done\\\\xnor4.txt', 'done\\\\xnor6.txt', 'done\\\\xnor7.txt', 'done\\\\xnor8.txt', 'done\\\\xnor9.txt', 'done\\\\xor1.txt', 'done\\\\xor10.txt', 'done\\\\xor12.txt', 'done\\\\xor13.txt', 'done\\\\xor14.txt', 'done\\\\xor15.txt', 'done\\\\xor16.txt', 'done\\\\xor17.txt', 'done\\\\xor18.txt', 'done\\\\xor19.txt', 'done\\\\xor2.txt', 'done\\\\xor20.txt', 'done\\\\xor21.txt', 'done\\\\xor22.txt', 'done\\\\xor23.txt', 'done\\\\xor24.txt', 'done\\\\xor25.txt', 'done\\\\xor26.txt', 'done\\\\xor3.txt', 'done\\\\xor6.txt', 'done\\\\xor7.txt', 'done\\\\xor8.txt', 'done\\\\xor9.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_files_in_folder(folder_path):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "# Replace 'folder_path' with the path to the folder you want to read files from\n",
    "folder_path = \"done\"\n",
    "verilog_files = get_files_in_folder(folder_path)\n",
    "print(verilog_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_files = []\n",
    "for v in verilog_files:\n",
    "    with open(v, \"r\") as file:\n",
    "        loaded_data = json.load(file)\n",
    "        # print(loaded_data[0])\n",
    "        \n",
    "        for node in loaded_data[0]:\n",
    "            if len(node) != 4:\n",
    "                failed_files.append(v)\n",
    "                break\n",
    "            \n",
    "#how to save list in a txt file\n",
    "with open(\"failed_files.txt\", \"w\") as output:\n",
    "    output.write(str(failed_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                edge_atr = loaded_data[2]\n",
    "                label = loaded_data[3]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.long)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                # edge_atr = torch.tensor(edge_atr, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.long)\n",
    "                num_nodes = x.size(0)\n",
    "                # print(num_nodes)\n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "# temp=extracting_attributes(\"./done/adder6.txt\")\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 405 Verilog files.\n",
      "405\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[165, 4], edge_index=[2, 208], y=[1], batch=[165])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dataset[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_num_nodes: 3\n",
      "max_num_nodes: 165\n",
      "mean_num_nodes: 15.59753086419753\n",
      "min_num_edges: 2\n",
      "max_num_edges: 208\n",
      "mean_num_edges: 19.409876543209876\n"
     ]
    }
   ],
   "source": [
    "def graph_stat(dataset, verilog_files):\n",
    "    \"\"\"\n",
    "    TODO: calculate the statistics of the ENZYMES dataset.\n",
    "    \n",
    "    Outputs:\n",
    "        min_num_nodes: min number of nodes\n",
    "        max_num_nodes: max number of nodes\n",
    "        mean_num_nodes: average number of nodes\n",
    "        min_num_edges: min number of edges\n",
    "        max_num_edges: max number of edges\n",
    "        mean_num_edges: average number of edges\n",
    "    \"\"\"\n",
    "    # for ind,data in enumerate(dataset):\n",
    "        # print(verilog_files[ind])\n",
    "        # print(data)\n",
    "        # print(len(data.x[1]))\n",
    "        \n",
    "    nodes_edges = [(data.num_nodes, data.num_edges) for data in dataset]\n",
    "    num_nodes, num_edges = list(list(zip(*nodes_edges))[0]), list(list(zip(*nodes_edges))[1])\n",
    "    min_num_nodes = min(num_nodes)\n",
    "    max_num_nodes = max(num_nodes)\n",
    "    mean_num_nodes = np.mean(num_nodes)\n",
    "    min_num_edges = min(num_edges)\n",
    "    max_num_edges = max(num_edges)\n",
    "    mean_num_edges = np.mean(num_edges)\n",
    "    \n",
    "    print(f\"min_num_nodes: {min_num_nodes}\")\n",
    "    print(f\"max_num_nodes: {max_num_nodes}\")\n",
    "    print(f\"mean_num_nodes: {mean_num_nodes}\")\n",
    "    print(f\"min_num_edges: {min_num_edges}\")\n",
    "    print(f\"max_num_edges: {max_num_edges}\")\n",
    "    print(f\"mean_num_edges: {mean_num_edges}\")\n",
    "\n",
    "graph_stat(dataset, verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "# # Define the sizes of training, validation, and test sets\n",
    "# train_size = int(0.7 * len(dataset))  # 70% of the data for training\n",
    "# val_size = int(0.15 * len(dataset))   # 15% of the data for validation\n",
    "# test_size = len(dataset) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create DataLoader for each set\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define the size of the training set (e.g., 70% of the data)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "\n",
    "# Calculate the size of the testing set\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "5\n",
      "10\n",
      "24\n",
      "3\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "43\n",
      "11\n",
      "5\n",
      "38\n",
      "4\n",
      "16\n",
      "5\n",
      "4\n",
      "9\n",
      "7\n",
      "18\n",
      "31\n",
      "7\n",
      "19\n",
      "15\n",
      "8\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "35\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "10\n",
      "4\n",
      "8\n",
      "12\n",
      "[Data(x=[14, 4], edge_index=[2, 13], edge_attr=[13], y=[1], batch=[14]), Data(x=[5, 4], edge_index=[2, 4], edge_attr=[4], y=[1], batch=[5]), Data(x=[10, 4], edge_index=[2, 10], edge_attr=[10], y=[1], batch=[10]), Data(x=[24, 4], edge_index=[2, 27], edge_attr=[27], y=[1], batch=[24]), Data(x=[3, 4], edge_index=[2, 2], edge_attr=[2], y=[1], batch=[3]), ValueError('expected sequence of length 4 at dim 1 (got 3)'), Data(x=[43, 4], edge_index=[2, 51], edge_attr=[51], y=[1], batch=[43]), Data(x=[11, 4], edge_index=[2, 18], edge_attr=[18], y=[1], batch=[11]), Data(x=[5, 4], edge_index=[2, 4], edge_attr=[4], y=[1], batch=[5]), Data(x=[38, 4], edge_index=[2, 44], edge_attr=[44], y=[1], batch=[38]), Data(x=[4, 4], edge_index=[2, 4], edge_attr=[4], y=[1], batch=[4]), Data(x=[16, 4], edge_index=[2, 19], edge_attr=[19], y=[1], batch=[16]), Data(x=[5, 4], edge_index=[2, 4], edge_attr=[4], y=[1], batch=[5]), Data(x=[4, 4], edge_index=[2, 3], edge_attr=[3], y=[1], batch=[4]), Data(x=[9, 4], edge_index=[2, 8], edge_attr=[8], y=[1], batch=[9]), Data(x=[7, 4], edge_index=[2, 6], edge_attr=[6], y=[1], batch=[7]), Data(x=[18, 4], edge_index=[2, 21], edge_attr=[21], y=[1], batch=[18]), Data(x=[31, 4], edge_index=[2, 36], edge_attr=[36], y=[1], batch=[31]), Data(x=[7, 4], edge_index=[2, 6], edge_attr=[6], y=[1], batch=[7]), Data(x=[19, 4], edge_index=[2, 24], edge_attr=[24], y=[1], batch=[19]), Data(x=[15, 4], edge_index=[2, 24], edge_attr=[24], y=[1], batch=[15]), Data(x=[8, 4], edge_index=[2, 13], edge_attr=[13], y=[1], batch=[8]), ValueError('expected sequence of length 4 at dim 1 (got 3)'), Data(x=[35, 4], edge_index=[2, 41], edge_attr=[41], y=[1], batch=[35]), Data(x=[4, 4], edge_index=[2, 3], edge_attr=[3], y=[1], batch=[4]), Data(x=[4, 4], edge_index=[2, 3], edge_attr=[3], y=[1], batch=[4]), Data(x=[3, 4], edge_index=[2, 2], edge_attr=[2], y=[1], batch=[3]), Data(x=[7, 4], edge_index=[2, 6], edge_attr=[6], y=[1], batch=[7]), Data(x=[10, 4], edge_index=[2, 9], edge_attr=[9], y=[1], batch=[10]), Data(x=[4, 4], edge_index=[2, 3], edge_attr=[3], y=[1], batch=[4]), Data(x=[8, 4], edge_index=[2, 7], edge_attr=[7], y=[1], batch=[8]), Data(x=[12, 4], edge_index=[2, 11], edge_attr=[11], y=[1], batch=[12])]\n"
     ]
    }
   ],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "batch = next(loader_iter)\n",
    "print(batch)\n",
    "# print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "48\n",
      "4\n",
      "52\n",
      "3\n",
      "16\n",
      "7\n",
      "17\n",
      "15\n",
      "6\n",
      "27\n",
      "4\n",
      "38\n",
      "48\n",
      "16\n",
      "4\n",
      "8\n",
      "21\n",
      "12\n",
      "11\n",
      "31\n",
      "15\n",
      "18\n",
      "3\n",
      "7\n",
      "5\n",
      "4\n",
      "9\n",
      "11\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "4\n",
      "51\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(len(data))\n",
    "    # print(data.num_graphs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch_geometric.utils import to_dense_adj, add_self_loops\n",
    "\n",
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(in_channels, out_channels))\n",
    "        # Initialize the parameters.\n",
    "        stdv = 1. / math.sqrt(out_channels)\n",
    "        self.theta.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Generate the adjacency matrix with self-loop \\hat{A} using edge_index.\n",
    "            2. Calculate the diagonal degree matrix \\hat{D}.\n",
    "            3. Calculate the output X' with torch.mm using the equation above.\n",
    "        \"\"\"\n",
    "\n",
    "        num_nodes = x.shape[0]\n",
    "        A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (num_nodes, num_nodes))\n",
    "        A = A.to_dense()\n",
    "        A_hat = A + torch.eye(num_nodes)\n",
    "        \n",
    "        A_sum = torch.sum(A_hat, dim=1)\n",
    "        D = torch.pow(A_sum, -0.5)\n",
    "        D[D == float('inf')] = 0.0\n",
    "        D_hat_sqrt = torch.diag(D)\n",
    "        \n",
    "        first = torch.mm(torch.mm(D_hat_sqrt, A_hat), D_hat_sqrt)\n",
    "        second = torch.mm(x, self.theta)\n",
    "        \n",
    "        ret = torch.mm(first, second)\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gcn1): GCNConv()\n",
       "  (a1): ReLU()\n",
       "  (gcn2): GCNConv()\n",
       "  (a2): ReLU()\n",
       "  (gcn3): GCNConv()\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (linear): Linear(in_features=64, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Define the first convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            2. Define the first activation layer using `nn.ReLU()`;\n",
    "            3. Define the second convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            4. Define the second activation layer using `nn.ReLU()`;\n",
    "            5. Define the third convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            6. Define the dropout layer using `nn.Dropout()`;\n",
    "            7. Define the linear layer using `nn.Linear()`. Set `output_size` to 2.\n",
    "\n",
    "        Note that for MUTAG dataset, the number of node features is 7, and the number of classes is 2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gcn1 = GCNConv(in_channels=3, out_channels=64)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.gcn2 = GCNConv(in_channels=64, out_channels=64)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.gcn3 = GCNConv(in_channels=64, out_channels=64)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.linear = nn.Linear(in_features=64, out_features=18)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Pass the data through the frst convolution layer;\n",
    "            2. Pass the data through the activation layer;\n",
    "            3. Pass the data through the second convolution layer;\n",
    "            4. Obtain the graph embeddings using the readout layer with `global_mean_pool()`;\n",
    "            5. Pass the graph embeddgins through the dropout layer;\n",
    "            6. Pass the graph embeddings through the linear layer.\n",
    "            \n",
    "        Arguments:\n",
    "            x: [num_nodes, 3], node features\n",
    "            edge_index: [2, num_edges], edges\n",
    "            batch: [num_nodes], batch assignment vector which maps each node to its \n",
    "                   respective graph in the batch\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size, 18)\n",
    "        \"\"\"\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.a1(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = self.a2(x)\n",
    "        x = self.gcn3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(x, dim=-1)\n",
    "        \n",
    "        return probs\n",
    "        \n",
    "        \n",
    "        \n",
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "16\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "48\n",
      "9\n",
      "5\n",
      "15\n",
      "4\n",
      "5\n",
      "7\n",
      "7\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "5\n",
      "11\n",
      "4\n",
      "7\n",
      "16\n",
      "8\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "7\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "19\n",
      "8\n",
      "43\n",
      "52\n",
      "5\n",
      "6\n",
      "expected sequence of length 4 at dim 1 (got 3)\n",
      "7\n",
      "22\n",
      "17\n",
      "21\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x4 and 3x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Your training code here\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[0;32m     43\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)\n",
      "Cell \u001b[1;32mIn [35], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# Iterate in batches over the training dataset.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m batch_data:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# print(data.x, data.edge_index, data.batch, data.y)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     15\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [34], line 48\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    TODO:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m        1. Pass the data through the frst convolution layer;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m        probs: probabilities of shape (batch_size, 18)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1(x)\n\u001b[0;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn2(x, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [33], line 31\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     28\u001b[0m D_hat_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(D)\n\u001b[0;32m     30\u001b[0m first \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(torch\u001b[38;5;241m.\u001b[39mmm(D_hat_sqrt, A_hat), D_hat_sqrt)\n\u001b[1;32m---> 31\u001b[0m second \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(first, second)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20x4 and 3x64)"
     ]
    }
   ],
   "source": [
    "gcn = GCN()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)\n",
    "# loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader):\n",
    "    gcn.train()\n",
    "    for batch_data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        for data in batch_data:\n",
    "            # print(data.x, data.edge_index, data.batch, data.y)\n",
    "            out = gcn(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "def test(loader):\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    for batch_data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        for data in batch_data:\n",
    "            # print(data.x, data.edge_index, data.batch, data.y)\n",
    "            out = gcn(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            print(pred)\n",
    "            print(data.y)\n",
    "            print((pred == data.y))\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Your training code here\n",
    "for epoch in range(200):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch + 1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(\"Training duration:\", duration, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor(2, dtype=torch.long)\n",
    "e = torch.tensor(2, dtype=torch.long)\n",
    "\n",
    "correct= int((z == e).sum())\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([17])\n",
      "tensor([10])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([5])\n",
      "tensor([13])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([4])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([2])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([12])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([2])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([12])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([1])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([13])\n",
      "tensor([8])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([17])\n",
      "tensor([12])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([10])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([4])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([4])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([5])\n",
      "tensor([8])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([9])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([True])\n",
      "tensor([8])\n",
      "tensor([0])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([2])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([13])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([True])\n",
      "tensor([13])\n",
      "tensor([9])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([3])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([4])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([8])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([2])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([12])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([9])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([11])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([13])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([5])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([0])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([5])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([0])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([2])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([9])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([9])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([12])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([17])\n",
      "tensor([12])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([17])\n",
      "tensor([10])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([0])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([12])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([8])\n",
      "tensor([1])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([9])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([5])\n",
      "tensor([0])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([1])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([16])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([2])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([4])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([9])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([11])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([15])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([11])\n",
      "tensor([False])\n",
      "tensor([13])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([15])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([0])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([4])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([7])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([14])\n",
      "tensor([False])\n",
      "tensor([17])\n",
      "tensor([17])\n",
      "tensor([True])\n",
      "tensor([8])\n",
      "tensor([4])\n",
      "tensor([False])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([True])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([5])\n",
      "tensor([6])\n",
      "tensor([False])\n",
      "tensor([5])\n",
      "tensor([5])\n",
      "tensor([True])\n",
      "tensor([17])\n",
      "tensor([2])\n",
      "tensor([False])\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1958762886597938"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcn.state_dict(), 'gcn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0,0,1], [0,0,1], [1,0,1], [3,3,1]], dtype=torch.float)\n",
    "y = torch.tensor([2], dtype=torch.long)\n",
    "edge_index = torch.tensor([[0, 1, 2],\n",
    "                           [3, 3, 3]], dtype=torch.long)\n",
    "gcn.eval()\n",
    "batch = torch.tensor([0, 0, 0, 0], dtype=torch.long)\n",
    "data_trial = Data(x=x, edge_index=edge_index, y=y, batch = batch)\n",
    "\n",
    "out = gcn(data_trial.x, data_trial.edge_index, data_trial.batch)\n",
    "pred = out.argmax(dim=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn = GCN()\n",
    "# gcn.load_state_dict(torch.load('gcn_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode of Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 321 verilog files \n",
    "* only 3 features             [type, operation_type, num_of_connections]\n",
    "* no edge attribute\n",
    "* 18 classes \n",
    "* 200 epochs \n",
    "* learning rate = 0.01\n",
    "* Dropoout = 0.4\n",
    "* Adam Optimizer\n",
    "* train 70, test 30 (on whole dataset, not each class)\n",
    "* time of training = seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train acc:  0.2902\n",
    "* Test Acc: 0.1959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Modifications for upcoming experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Clean dataset (by removing unnecessay, uninformative or wrong code files)\n",
    "2) remove reduntant parsing (different files but same parsing)\n",
    "3) include more informative features\n",
    "4) improve encoding format\n",
    "5) try using less classes (most important ones, so that less classes but more balanced dataset)\n",
    "6) adding more files\n",
    "7) adjusting hyperparameters such as learning rate, dropout, ...etc\n",
    "8) splitting train, val, test\n",
    "9) using equal percentages of each class (adjusting splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
