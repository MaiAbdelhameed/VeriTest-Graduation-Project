{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mai\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from read_data import read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = \"encoded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (2.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.11.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (4.64.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.7.3)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (5.9.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.22.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torch_geometric) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "<class 'str'>\n",
      "encoded\\adder11.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the folder containing your Verilog files\n",
    "folder_path = 'encoded'\n",
    "\n",
    "# Use list comprehension to create a list of file paths\n",
    "verilog_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.txt')]\n",
    "\n",
    "# Print the list of file paths\n",
    "print(len(verilog_files))\n",
    "print(type(verilog_files[0]))\n",
    "print(verilog_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to label: encoded\\adder11.txt\n",
      "Failed to label: encoded\\adder12.txt\n",
      "Failed to label: encoded\\adder13.txt\n",
      "Failed to label: encoded\\adder14.txt\n",
      "Failed to label: encoded\\adder15.txt\n",
      "Failed to label: encoded\\adder16.txt\n",
      "Failed to label: encoded\\adder17.txt\n",
      "Failed to label: encoded\\adder18.txt\n",
      "Failed to label: encoded\\adder19.txt\n",
      "Failed to label: encoded\\adder2.txt\n",
      "Failed to label: encoded\\adder20.txt\n",
      "Failed to label: encoded\\adder5.txt\n",
      "Failed to label: encoded\\adder8.txt\n",
      "Failed to label: encoded\\ALU10.txt\n",
      "Failed to label: encoded\\ALU13.txt\n",
      "Failed to label: encoded\\ALU14.txt\n",
      "Failed to label: encoded\\ALU15.txt\n",
      "Failed to label: encoded\\ALU2.txt\n",
      "Failed to label: encoded\\ALU6.txt\n",
      "Failed to label: encoded\\ALU7.txt\n",
      "Failed to label: encoded\\ALU9.txt\n",
      "Failed to label: encoded\\and1.txt\n",
      "Failed to label: encoded\\and10.txt\n",
      "Failed to label: encoded\\and12.txt\n",
      "Failed to label: encoded\\and13.txt\n",
      "Failed to label: encoded\\and14.txt\n",
      "Failed to label: encoded\\and15.txt\n",
      "Failed to label: encoded\\and16.txt\n",
      "Failed to label: encoded\\and17.txt\n",
      "Failed to label: encoded\\and18.txt\n",
      "Failed to label: encoded\\and19.txt\n",
      "Failed to label: encoded\\and2.txt\n",
      "Failed to label: encoded\\and20.txt\n",
      "Failed to label: encoded\\and21.txt\n",
      "Failed to label: encoded\\and23.txt\n",
      "Failed to label: encoded\\and25.txt\n",
      "Failed to label: encoded\\and26.txt\n",
      "Failed to label: encoded\\and27.txt\n",
      "Failed to label: encoded\\and28.txt\n",
      "Failed to label: encoded\\and29.txt\n",
      "Failed to label: encoded\\and3.txt\n",
      "Failed to label: encoded\\and30.txt\n",
      "Failed to label: encoded\\and6.txt\n",
      "Failed to label: encoded\\and7.txt\n",
      "Failed to label: encoded\\and8.txt\n",
      "Failed to label: encoded\\and9.txt\n",
      "Failed to label: encoded\\bcd3.txt\n",
      "Failed to label: encoded\\comparator14.txt\n",
      "Failed to label: encoded\\comparator15.txt\n",
      "Failed to label: encoded\\comparator16.txt\n",
      "Failed to label: encoded\\comparator18.txt\n",
      "Failed to label: encoded\\comparator2.txt\n",
      "Failed to label: encoded\\comparator21.txt\n",
      "Failed to label: encoded\\comparator23.txt\n",
      "Failed to label: encoded\\comparator3.txt\n",
      "Failed to label: encoded\\comparator5.txt\n",
      "Failed to label: encoded\\comparator6.txt\n",
      "Failed to label: encoded\\comparator7.txt\n",
      "Failed to label: encoded\\comparator8.txt\n",
      "Failed to label: encoded\\decoder1.txt\n",
      "Failed to label: encoded\\decoder11.txt\n",
      "Failed to label: encoded\\decoder12.txt\n",
      "Failed to label: encoded\\decoder13.txt\n",
      "Failed to label: encoded\\decoder14.txt\n",
      "Failed to label: encoded\\decoder15.txt\n",
      "Failed to label: encoded\\decoder16.txt\n",
      "Failed to label: encoded\\decoder17.txt\n",
      "Failed to label: encoded\\decoder18.txt\n",
      "Failed to label: encoded\\decoder19.txt\n",
      "Failed to label: encoded\\decoder2.txt\n",
      "Failed to label: encoded\\decoder20.txt\n",
      "Failed to label: encoded\\decoder21.txt\n",
      "Failed to label: encoded\\decoder22.txt\n",
      "Failed to label: encoded\\decoder23.txt\n",
      "Failed to label: encoded\\decoder24.txt\n",
      "Failed to label: encoded\\decoder25.txt\n",
      "Failed to label: encoded\\decoder26.txt\n",
      "Failed to label: encoded\\decoder27.txt\n",
      "Failed to label: encoded\\decoder28.txt\n",
      "Failed to label: encoded\\decoder29.txt\n",
      "Failed to label: encoded\\decoder30.txt\n",
      "Failed to label: encoded\\decoder31.txt\n",
      "Failed to label: encoded\\decoder32.txt\n",
      "Failed to label: encoded\\decoder33.txt\n",
      "Failed to label: encoded\\decoder34.txt\n",
      "Failed to label: encoded\\decoder35.txt\n",
      "Failed to label: encoded\\decoder36.txt\n",
      "Failed to label: encoded\\decoder37.txt\n",
      "Failed to label: encoded\\decoder38.txt\n",
      "Failed to label: encoded\\decoder39.txt\n",
      "Failed to label: encoded\\decoder40.txt\n",
      "Failed to label: encoded\\decoder41.txt\n",
      "Failed to label: encoded\\decoder43.txt\n",
      "Failed to label: encoded\\decoder44.txt\n",
      "Failed to label: encoded\\decoder45.txt\n",
      "Failed to label: encoded\\decoder46.txt\n",
      "Failed to label: encoded\\decoder47.txt\n",
      "Failed to label: encoded\\decoder48.txt\n",
      "Failed to label: encoded\\decoder49.txt\n",
      "Failed to label: encoded\\decoder5.txt\n",
      "Failed to label: encoded\\decoder50.txt\n",
      "Failed to label: encoded\\decoder6.txt\n",
      "Failed to label: encoded\\encoder1.txt\n",
      "Failed to label: encoded\\encoder10.txt\n",
      "Failed to label: encoded\\encoder11.txt\n",
      "Failed to label: encoded\\encoder12.txt\n",
      "Failed to label: encoded\\encoder13.txt\n",
      "Failed to label: encoded\\encoder14.txt\n",
      "Failed to label: encoded\\encoder15.txt\n",
      "Failed to label: encoded\\encoder16.txt\n",
      "Failed to label: encoded\\encoder17.txt\n",
      "Failed to label: encoded\\encoder18.txt\n",
      "Failed to label: encoded\\encoder19.txt\n",
      "Failed to label: encoded\\encoder2.txt\n",
      "Failed to label: encoded\\encoder20.txt\n",
      "Failed to label: encoded\\encoder21.txt\n",
      "Failed to label: encoded\\encoder24.txt\n",
      "Failed to label: encoded\\encoder25.txt\n",
      "Failed to label: encoded\\encoder26.txt\n",
      "Failed to label: encoded\\encoder27.txt\n",
      "Failed to label: encoded\\encoder28.txt\n",
      "Failed to label: encoded\\encoder29.txt\n",
      "Failed to label: encoded\\encoder3.txt\n",
      "Failed to label: encoded\\encoder30.txt\n",
      "Failed to label: encoded\\encoder31.txt\n",
      "Failed to label: encoded\\encoder32.txt\n",
      "Failed to label: encoded\\encoder33.txt\n",
      "Failed to label: encoded\\encoder35.txt\n",
      "Failed to label: encoded\\encoder36.txt\n",
      "Failed to label: encoded\\encoder37.txt\n",
      "Failed to label: encoded\\encoder38.txt\n",
      "Failed to label: encoded\\encoder39.txt\n",
      "Failed to label: encoded\\encoder4.txt\n",
      "Failed to label: encoded\\encoder40.txt\n",
      "Failed to label: encoded\\encoder41.txt\n",
      "Failed to label: encoded\\encoder42.txt\n",
      "Failed to label: encoded\\encoder43.txt\n",
      "Failed to label: encoded\\encoder44.txt\n",
      "Failed to label: encoded\\encoder45.txt\n",
      "Failed to label: encoded\\encoder46.txt\n",
      "Failed to label: encoded\\encoder47.txt\n",
      "Failed to label: encoded\\encoder48.txt\n",
      "Failed to label: encoded\\encoder5.txt\n",
      "Failed to label: encoded\\encoder50.txt\n",
      "Failed to label: encoded\\encoder51.txt\n",
      "Failed to label: encoded\\encoder6.txt\n",
      "Failed to label: encoded\\encoder7.txt\n",
      "Failed to label: encoded\\encoder8.txt\n",
      "Failed to label: encoded\\encoder9.txt\n",
      "Failed to label: encoded\\mult1.txt\n",
      "Failed to label: encoded\\mult10.txt\n",
      "Failed to label: encoded\\mult11.txt\n",
      "Failed to label: encoded\\mult12.txt\n",
      "Failed to label: encoded\\mult13.txt\n",
      "Failed to label: encoded\\mult14.txt\n",
      "Failed to label: encoded\\mult15.txt\n",
      "Failed to label: encoded\\mult16.txt\n",
      "Failed to label: encoded\\mult17.txt\n",
      "Failed to label: encoded\\mult18.txt\n",
      "Failed to label: encoded\\mult19.txt\n",
      "Failed to label: encoded\\mult2.txt\n",
      "Failed to label: encoded\\mult20.txt\n",
      "Failed to label: encoded\\mult3.txt\n",
      "Failed to label: encoded\\mult30.txt\n",
      "Failed to label: encoded\\mult31.txt\n",
      "Failed to label: encoded\\mult32.txt\n",
      "Failed to label: encoded\\mult33.txt\n",
      "Failed to label: encoded\\mult35.txt\n",
      "Failed to label: encoded\\mult6.txt\n",
      "Failed to label: encoded\\mult8.txt\n",
      "Failed to label: encoded\\mult9.txt\n",
      "Failed to label: encoded\\mux1.txt\n",
      "Failed to label: encoded\\mux10.txt\n",
      "Failed to label: encoded\\mux11.txt\n",
      "Failed to label: encoded\\mux12.txt\n",
      "Failed to label: encoded\\mux13.txt\n",
      "Failed to label: encoded\\mux14.txt\n",
      "Failed to label: encoded\\mux15.txt\n",
      "Failed to label: encoded\\mux16.txt\n",
      "Failed to label: encoded\\mux17.txt\n",
      "Failed to label: encoded\\mux18.txt\n",
      "Failed to label: encoded\\mux19.txt\n",
      "Failed to label: encoded\\mux2.txt\n",
      "Failed to label: encoded\\mux20.txt\n",
      "Failed to label: encoded\\mux21.txt\n",
      "Failed to label: encoded\\mux22.txt\n",
      "Failed to label: encoded\\mux23.txt\n",
      "Failed to label: encoded\\mux24.txt\n",
      "Failed to label: encoded\\mux25.txt\n",
      "Failed to label: encoded\\mux26.txt\n",
      "Failed to label: encoded\\mux28.txt\n",
      "Failed to label: encoded\\mux29.txt\n",
      "Failed to label: encoded\\mux30.txt\n",
      "Failed to label: encoded\\mux31.txt\n",
      "Failed to label: encoded\\mux33.txt\n",
      "Failed to label: encoded\\mux34.txt\n",
      "Failed to label: encoded\\mux35.txt\n",
      "Failed to label: encoded\\mux36.txt\n",
      "Failed to label: encoded\\mux38.txt\n",
      "Failed to label: encoded\\mux39.txt\n",
      "Failed to label: encoded\\mux4.txt\n",
      "Failed to label: encoded\\mux40.txt\n",
      "Failed to label: encoded\\mux41.txt\n",
      "Failed to label: encoded\\mux42.txt\n",
      "Failed to label: encoded\\mux43.txt\n",
      "Failed to label: encoded\\mux44.txt\n",
      "Failed to label: encoded\\mux45.txt\n",
      "Failed to label: encoded\\mux47.txt\n",
      "Failed to label: encoded\\mux48.txt\n",
      "Failed to label: encoded\\mux49.txt\n",
      "Failed to label: encoded\\mux5.txt\n",
      "Failed to label: encoded\\mux50.txt\n",
      "Failed to label: encoded\\mux51.txt\n",
      "Failed to label: encoded\\mux52.txt\n",
      "Failed to label: encoded\\mux53.txt\n",
      "Failed to label: encoded\\mux55.txt\n",
      "Failed to label: encoded\\mux56.txt\n",
      "Failed to label: encoded\\mux57.txt\n",
      "Failed to label: encoded\\mux58.txt\n",
      "Failed to label: encoded\\mux59.txt\n",
      "Failed to label: encoded\\mux6.txt\n",
      "Failed to label: encoded\\mux60.txt\n",
      "Failed to label: encoded\\mux61.txt\n",
      "Failed to label: encoded\\mux62.txt\n",
      "Failed to label: encoded\\mux63.txt\n",
      "Failed to label: encoded\\mux64.txt\n",
      "Failed to label: encoded\\mux66.txt\n",
      "Failed to label: encoded\\mux67.txt\n",
      "Failed to label: encoded\\mux68.txt\n",
      "Failed to label: encoded\\mux69.txt\n",
      "Failed to label: encoded\\mux7.txt\n",
      "Failed to label: encoded\\mux70.txt\n",
      "Failed to label: encoded\\mux71.txt\n",
      "Failed to label: encoded\\mux72.txt\n",
      "Failed to label: encoded\\mux73.txt\n",
      "Failed to label: encoded\\mux74.txt\n",
      "Failed to label: encoded\\mux75.txt\n",
      "Failed to label: encoded\\mux8.txt\n",
      "Failed to label: encoded\\mux9.txt\n",
      "Failed to label: encoded\\nand1.txt\n",
      "Failed to label: encoded\\nand10.txt\n",
      "Failed to label: encoded\\nand11.txt\n",
      "Failed to label: encoded\\nand12.txt\n",
      "Failed to label: encoded\\nand14.txt\n",
      "Failed to label: encoded\\nand15.txt\n",
      "Failed to label: encoded\\nand16.txt\n",
      "Failed to label: encoded\\nand18.txt\n",
      "Failed to label: encoded\\nand19.txt\n",
      "Failed to label: encoded\\nand2.txt\n",
      "Failed to label: encoded\\nand20.txt\n",
      "Failed to label: encoded\\nand22.txt\n",
      "Failed to label: encoded\\nand4.txt\n",
      "Failed to label: encoded\\nand6.txt\n",
      "Failed to label: encoded\\nand7.txt\n",
      "Failed to label: encoded\\nand8.txt\n",
      "Failed to label: encoded\\nand9.txt\n",
      "Failed to label: encoded\\nor1.txt\n",
      "Failed to label: encoded\\nor10.txt\n",
      "Failed to label: encoded\\nor11.txt\n",
      "Failed to label: encoded\\nor12.txt\n",
      "Failed to label: encoded\\nor13.txt\n",
      "Failed to label: encoded\\nor14.txt\n",
      "Failed to label: encoded\\nor15.txt\n",
      "Failed to label: encoded\\nor16.txt\n",
      "Failed to label: encoded\\nor17.txt\n",
      "Failed to label: encoded\\nor2.txt\n",
      "Failed to label: encoded\\nor4.txt\n",
      "Failed to label: encoded\\nor6.txt\n",
      "Failed to label: encoded\\nor7.txt\n",
      "Failed to label: encoded\\nor8.txt\n",
      "Failed to label: encoded\\nor9.txt\n",
      "Failed to label: encoded\\not1.txt\n",
      "Failed to label: encoded\\not10.txt\n",
      "Failed to label: encoded\\not11.txt\n",
      "Failed to label: encoded\\not12.txt\n",
      "Failed to label: encoded\\not13.txt\n",
      "Failed to label: encoded\\not14.txt\n",
      "Failed to label: encoded\\not2.txt\n",
      "Failed to label: encoded\\not3.txt\n",
      "Failed to label: encoded\\not6.txt\n",
      "Failed to label: encoded\\not7.txt\n",
      "Failed to label: encoded\\not9.txt\n",
      "Failed to label: encoded\\or1.txt\n",
      "Failed to label: encoded\\or10.txt\n",
      "Failed to label: encoded\\or11.txt\n",
      "Failed to label: encoded\\or12.txt\n",
      "Failed to label: encoded\\or13.txt\n",
      "Failed to label: encoded\\or14.txt\n",
      "Failed to label: encoded\\or15.txt\n",
      "Failed to label: encoded\\or16.txt\n",
      "Failed to label: encoded\\or17.txt\n",
      "Failed to label: encoded\\or18.txt\n",
      "Failed to label: encoded\\or19.txt\n",
      "Failed to label: encoded\\or2.txt\n",
      "Failed to label: encoded\\or20.txt\n",
      "Failed to label: encoded\\or21.txt\n",
      "Failed to label: encoded\\or22.txt\n",
      "Failed to label: encoded\\or23.txt\n",
      "Failed to label: encoded\\or24.txt\n",
      "Failed to label: encoded\\or25.txt\n",
      "Failed to label: encoded\\or26.txt\n",
      "Failed to label: encoded\\or28.txt\n",
      "Failed to label: encoded\\or4.txt\n",
      "Failed to label: encoded\\or7.txt\n",
      "Failed to label: encoded\\or8.txt\n",
      "Failed to label: encoded\\or9.txt\n",
      "Failed to label: encoded\\pe1.txt\n",
      "Failed to label: encoded\\pe15.txt\n",
      "Failed to label: encoded\\pe16.txt\n",
      "Failed to label: encoded\\pe17.txt\n",
      "Failed to label: encoded\\pe18.txt\n",
      "Failed to label: encoded\\pe2.txt\n",
      "Failed to label: encoded\\pe20.txt\n",
      "Failed to label: encoded\\pe21.txt\n",
      "Failed to label: encoded\\pe22.txt\n",
      "Failed to label: encoded\\pe24.txt\n",
      "Failed to label: encoded\\pe25.txt\n",
      "Failed to label: encoded\\pe26.txt\n",
      "Failed to label: encoded\\pe27.txt\n",
      "Failed to label: encoded\\pe28.txt\n",
      "Failed to label: encoded\\pe29.txt\n",
      "Failed to label: encoded\\pe30.txt\n",
      "Failed to label: encoded\\pe4.txt\n",
      "Failed to label: encoded\\pe5.txt\n",
      "Failed to label: encoded\\pe6.txt\n",
      "Failed to label: encoded\\seg1.txt\n",
      "Failed to label: encoded\\seg2.txt\n",
      "Failed to label: encoded\\seg3.txt\n",
      "Failed to label: encoded\\seg5.txt\n",
      "Failed to label: encoded\\seg9.txt\n",
      "Failed to label: encoded\\sub1.txt\n",
      "Failed to label: encoded\\sub10.txt\n",
      "Failed to label: encoded\\sub11.txt\n",
      "Failed to label: encoded\\sub12.txt\n",
      "Failed to label: encoded\\sub5.txt\n",
      "Failed to label: encoded\\sub7.txt\n",
      "Failed to label: encoded\\sub8.txt\n",
      "Failed to label: encoded\\sub9.txt\n",
      "Failed to label: encoded\\xnor1.txt\n",
      "Failed to label: encoded\\xnor10.txt\n",
      "Failed to label: encoded\\xnor11.txt\n",
      "Failed to label: encoded\\xnor12.txt\n",
      "Failed to label: encoded\\xnor13.txt\n",
      "Failed to label: encoded\\xnor14.txt\n",
      "Failed to label: encoded\\xnor15.txt\n",
      "Failed to label: encoded\\xnor16.txt\n",
      "Failed to label: encoded\\xnor17.txt\n",
      "Failed to label: encoded\\xnor18.txt\n",
      "Failed to label: encoded\\xnor19.txt\n",
      "Failed to label: encoded\\xnor2.txt\n",
      "Failed to label: encoded\\xnor20.txt\n",
      "Failed to label: encoded\\xnor21.txt\n",
      "Failed to label: encoded\\xnor22.txt\n",
      "Failed to label: encoded\\xnor23.txt\n",
      "Failed to label: encoded\\xnor24.txt\n",
      "Failed to label: encoded\\xnor25.txt\n",
      "Failed to label: encoded\\xnor4.txt\n",
      "Failed to label: encoded\\xnor6.txt\n",
      "Failed to label: encoded\\xnor7.txt\n",
      "Failed to label: encoded\\xnor8.txt\n",
      "Failed to label: encoded\\xnor9.txt\n",
      "Failed to label: encoded\\xor1.txt\n",
      "Failed to label: encoded\\xor10.txt\n",
      "Failed to label: encoded\\xor12.txt\n",
      "Failed to label: encoded\\xor13.txt\n",
      "Failed to label: encoded\\xor14.txt\n",
      "Failed to label: encoded\\xor15.txt\n",
      "Failed to label: encoded\\xor16.txt\n",
      "Failed to label: encoded\\xor17.txt\n",
      "Failed to label: encoded\\xor18.txt\n",
      "Failed to label: encoded\\xor19.txt\n",
      "Failed to label: encoded\\xor2.txt\n",
      "Failed to label: encoded\\xor20.txt\n",
      "Failed to label: encoded\\xor21.txt\n",
      "Failed to label: encoded\\xor22.txt\n",
      "Failed to label: encoded\\xor23.txt\n",
      "Failed to label: encoded\\xor24.txt\n",
      "Failed to label: encoded\\xor25.txt\n",
      "Failed to label: encoded\\xor26.txt\n",
      "Failed to label: encoded\\xor27.txt\n",
      "Failed to label: encoded\\xor3.txt\n",
      "Failed to label: encoded\\xor6.txt\n",
      "Failed to label: encoded\\xor7.txt\n",
      "Failed to label: encoded\\xor8.txt\n",
      "Failed to label: encoded\\xor9.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "def label_verilog_file(file_name):\n",
    "    label_mapping = {\n",
    "        'adder': 0, 'ALU': 1, 'and': 2, 'bcd': 3, 'comparator': 4, 'decoder': 5,\n",
    "        'encoder': 6, 'mult': 7, 'mux': 8, 'nand': 9, 'nor': 10, 'not': 11, 'or': 12,\n",
    "        'pe': 13, 'seg': 14, 'sub': 15, 'xnor': 16, 'xor': 17\n",
    "    }\n",
    "    pattern = r\"([a-zA-Z]+)(\\d+)?\"\n",
    "    match = re.match(pattern, file_name)\n",
    "    if match:\n",
    "        base_name = match.group(1)\n",
    "        if base_name in label_mapping:\n",
    "            return label_mapping[base_name]\n",
    "    return None\n",
    "\n",
    "def add_label_to_verilog_file(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            loaded_data = json.load(file)\n",
    "            label = label_verilog_file(os.path.basename(file_path))\n",
    "            if label is not None and [label] not in loaded_data:  # Check if label already exists\n",
    "                loaded_data.append([label])  # Add label as the third list\n",
    "                with open(file_path, \"w\") as file:\n",
    "                    json.dump(loaded_data, file)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "for file_path in verilog_files:\n",
    "    if not add_label_to_verilog_file(file_path):\n",
    "        print(f\"Failed to label: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                label = loaded_data[2]\n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y)\n",
    "                return data\n",
    "    except Exception as e:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 385 Verilog files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_num_nodes: 3\n",
      "max_num_nodes: 165\n",
      "mean_num_nodes: 15.501298701298701\n",
      "min_num_edges: 2\n",
      "max_num_edges: 163\n",
      "mean_num_edges: 14.275324675324676\n"
     ]
    }
   ],
   "source": [
    "def graph_stat(dataset):\n",
    "    \"\"\"\n",
    "    TODO: calculate the statistics of the ENZYMES dataset.\n",
    "    \n",
    "    Outputs:\n",
    "        min_num_nodes: min number of nodes\n",
    "        max_num_nodes: max number of nodes\n",
    "        mean_num_nodes: average number of nodes\n",
    "        min_num_edges: min number of edges\n",
    "        max_num_edges: max number of edges\n",
    "        mean_num_edges: average number of edges\n",
    "    \"\"\"\n",
    "    nodes_edges = [(data.num_nodes, data.num_edges) for data in dataset]\n",
    "    num_nodes, num_edges = list(list(zip(*nodes_edges))[0]), list(list(zip(*nodes_edges))[1])\n",
    "    min_num_nodes = min(num_nodes)\n",
    "    max_num_nodes = max(num_nodes)\n",
    "    mean_num_nodes = np.mean(num_nodes)\n",
    "    min_num_edges = min(num_edges)\n",
    "    max_num_edges = max(num_edges)\n",
    "    mean_num_edges = np.mean(num_edges)\n",
    "    \n",
    "    print(f\"min_num_nodes: {min_num_nodes}\")\n",
    "    print(f\"max_num_nodes: {max_num_nodes}\")\n",
    "    print(f\"mean_num_nodes: {mean_num_nodes}\")\n",
    "    print(f\"min_num_edges: {min_num_edges}\")\n",
    "    print(f\"max_num_edges: {max_num_edges}\")\n",
    "    print(f\"mean_num_edges: {mean_num_edges}\")\n",
    "\n",
    "graph_stat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[637, 3], edge_index=[2, 593], y=[32], batch=[637], ptr=[33])\n",
      "32\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [205], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset[:split_idx]\n\u001b[0;32m     15\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m dataset[split_idx:]\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen train:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen test:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_dataset))\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# from torch_geometric.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# loader_iter = iter(loader)\n",
    "# batch = next(loader_iter)\n",
    "# print(batch)\n",
    "# print(batch.num_graphs)\n",
    "\n",
    "\n",
    "\n",
    "# # split\n",
    "# split_idx = int(len(dataset) * 0.8)\n",
    "# train_dataset = dataset[:split_idx]\n",
    "# test_dataset = dataset[split_idx:]\n",
    "\n",
    "# print(\"len train:\", len(train_dataset))\n",
    "# print(\"len test:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "# # Define the sizes of training, validation, and test sets\n",
    "# train_size = int(0.7 * len(dataset))  # 70% of the data for training\n",
    "# val_size = int(0.15 * len(dataset))   # 15% of the data for validation\n",
    "# test_size = len(dataset) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create DataLoader for each set\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define the size of the training set (e.g., 70% of the data)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "\n",
    "# Calculate the size of the testing set\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for each set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch_geometric.utils import to_dense_adj, add_self_loops\n",
    "\n",
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(in_channels, out_channels))\n",
    "        # Initialize the parameters.\n",
    "        stdv = 1. / math.sqrt(out_channels)\n",
    "        self.theta.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Generate the adjacency matrix with self-loop \\hat{A} using edge_index.\n",
    "            2. Calculate the diagonal degree matrix \\hat{D}.\n",
    "            3. Calculate the output X' with torch.mm using the equation above.\n",
    "        \"\"\"\n",
    "\n",
    "        num_nodes = x.shape[0]\n",
    "        A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (num_nodes, num_nodes))\n",
    "        A = A.to_dense()\n",
    "        A_hat = A + torch.eye(num_nodes)\n",
    "        \n",
    "        A_sum = torch.sum(A_hat, dim=1)\n",
    "        D = torch.pow(A_sum, -0.5)\n",
    "        D[D == float('inf')] = 0.0\n",
    "        D_hat_sqrt = torch.diag(D)\n",
    "        \n",
    "        first = torch.mm(torch.mm(D_hat_sqrt, A_hat), D_hat_sqrt)\n",
    "        second = torch.mm(x, self.theta)\n",
    "        \n",
    "        ret = torch.mm(first, second)\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gcn1): GCNConv()\n",
       "  (a1): ReLU()\n",
       "  (gcn2): GCNConv()\n",
       "  (a2): ReLU()\n",
       "  (gcn3): GCNConv()\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Define the first convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            2. Define the first activation layer using `nn.ReLU()`;\n",
    "            3. Define the second convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            4. Define the second activation layer using `nn.ReLU()`;\n",
    "            5. Define the third convolution layer using `GCNConv()`. Set `out_channels` to 64;\n",
    "            6. Define the dropout layer using `nn.Dropout()`;\n",
    "            7. Define the linear layer using `nn.Linear()`. Set `output_size` to 2.\n",
    "\n",
    "        Note that for MUTAG dataset, the number of node features is 7, and the number of classes is 2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gcn1 = GCNConv(in_channels=7, out_channels=64)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.gcn2 = GCNConv(in_channels=64, out_channels=64)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.gcn3 = GCNConv(in_channels=64, out_channels=64)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.linear = nn.Linear(in_features=64, out_features=2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Pass the data through the frst convolution layer;\n",
    "            2. Pass the data through the activation layer;\n",
    "            3. Pass the data through the second convolution layer;\n",
    "            4. Obtain the graph embeddings using the readout layer with `global_mean_pool()`;\n",
    "            5. Pass the graph embeddgins through the dropout layer;\n",
    "            6. Pass the graph embeddings through the linear layer.\n",
    "            \n",
    "        Arguments:\n",
    "            x: [num_nodes, 7], node features\n",
    "            edge_index: [2, num_edges], edges\n",
    "            batch: [num_nodes], batch assignment vector which maps each node to its \n",
    "                   respective graph in the batch\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size, 2)\n",
    "        \"\"\"\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.a1(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = self.a2(x)\n",
    "        x = self.gcn3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(x, dim=-1)\n",
    "        \n",
    "        return probs\n",
    "        \n",
    "        \n",
    "        \n",
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (369x3 and 7x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [214], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)  \u001b[38;5;66;03m# Derive ratio of correct predictions.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(train_dataloader)\n\u001b[0;32m     37\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_dataloader)\n",
      "Cell \u001b[1;32mIn [214], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# Iterate in batches over the training dataset.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    TODO: train the model for one epoch.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    Note that you can acess the batch data using `data.x`, `data.edge_index`, `data.batch`, `data,y`.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     19\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [213], line 48\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    TODO:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m        1. Pass the data through the frst convolution layer;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m        probs: probabilities of shape (batch_size, 2)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1(x)\n\u001b[0;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn2(x, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mai\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [212], line 31\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     28\u001b[0m D_hat_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(D)\n\u001b[0;32m     30\u001b[0m first \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(torch\u001b[38;5;241m.\u001b[39mmm(D_hat_sqrt, A_hat), D_hat_sqrt)\n\u001b[1;32m---> 31\u001b[0m second \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(first, second)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (369x3 and 7x64)"
     ]
    }
   ],
   "source": [
    "gcn = GCN()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)\n",
    "# loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader):\n",
    "    gcn.train()\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        \"\"\"\n",
    "        TODO: train the model for one epoch.\n",
    "        \n",
    "        Note that you can acess the batch data using `data.x`, `data.edge_index`, `data.batch`, `data,y`.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = gcn(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "def test(loader):\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = gcn(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    train(train_dataloader)\n",
    "    train_acc = test(train_dataloader)\n",
    "    test_acc = test(test_dataloader)\n",
    "    print(f'Epoch: {epoch + 1:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
