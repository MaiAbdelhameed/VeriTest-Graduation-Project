{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (2.5.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (3.8.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (5.9.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.11.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.22.4)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: scipy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.7.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2022.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.27.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (5.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (4.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->torch_geometric) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->torch_geometric) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (1.26.9)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torch_geometric) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%pip install torch_geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n",
      "['done\\\\adder10_synth.txt', 'done\\\\adder11_synth.txt', 'done\\\\adder12_synth.txt', 'done\\\\adder13_synth.txt', 'done\\\\adder14_synth.txt', 'done\\\\adder15_synth.txt', 'done\\\\adder16_synth.txt', 'done\\\\adder17_synth.txt', 'done\\\\adder18_synth.txt', 'done\\\\adder19_synth.txt', 'done\\\\adder1_synth.txt', 'done\\\\adder20_synth.txt', 'done\\\\adder21_synth.txt', 'done\\\\adder22_synth.txt', 'done\\\\adder23_synth.txt', 'done\\\\adder24_synth.txt', 'done\\\\adder25_synth.txt', 'done\\\\adder26_synth.txt', 'done\\\\adder27_synth.txt', 'done\\\\adder28_synth.txt', 'done\\\\adder2_synth.txt', 'done\\\\adder3_synth.txt', 'done\\\\adder4_synth.txt', 'done\\\\adder5_synth.txt', 'done\\\\adder6_synth.txt', 'done\\\\adder7_synth.txt', 'done\\\\adder8_synth.txt', 'done\\\\adder9_synth.txt', 'done\\\\and10_gate_synth.txt', 'done\\\\and11_gate_synth.txt', 'done\\\\and12_gate_synth.txt', 'done\\\\and13_synth.txt', 'done\\\\and14_synth.txt', 'done\\\\and15_synth.txt', 'done\\\\and16_synth.txt', 'done\\\\and17_synth.txt', 'done\\\\and18_gate_synth.txt', 'done\\\\and19_synth.txt', 'done\\\\and1_synth.txt', 'done\\\\and20_synth.txt', 'done\\\\and21_synth.txt', 'done\\\\and22_synth.txt', 'done\\\\and23_synth.txt', 'done\\\\and24_synth.txt', 'done\\\\and25_synth.txt', 'done\\\\and26_synth.txt', 'done\\\\and27_synth.txt', 'done\\\\and28_synth.txt', 'done\\\\and29_synth.txt', 'done\\\\and2_gate_synth.txt', 'done\\\\and30_synth.txt', 'done\\\\and3_gate_synth.txt', 'done\\\\and4_gate_synth.txt', 'done\\\\and5_gate_synth.txt', 'done\\\\and6_gate_synth.txt', 'done\\\\and7_gate_synth.txt', 'done\\\\and8_gate_synth.txt', 'done\\\\and9_gate_synth.txt', 'done\\\\comparator10_synth.txt', 'done\\\\comparator11_synth.txt', 'done\\\\comparator12_synth.txt', 'done\\\\comparator13_synth.txt', 'done\\\\comparator14_synth.txt', 'done\\\\comparator15_synth.txt', 'done\\\\comparator16_synth.txt', 'done\\\\comparator17_synth.txt', 'done\\\\comparator18_synth.txt', 'done\\\\comparator19_synth.txt', 'done\\\\comparator1_synth.txt', 'done\\\\comparator20_synth.txt', 'done\\\\comparator21_synth.txt', 'done\\\\comparator22_synth.txt', 'done\\\\comparator2_synth.txt', 'done\\\\comparator3_synth.txt', 'done\\\\comparator4_synth.txt', 'done\\\\comparator5_synth.txt', 'done\\\\comparator6_synth.txt', 'done\\\\comparator7_synth.txt', 'done\\\\comparator8_synth.txt', 'done\\\\comparator9_synth.txt', 'done\\\\decoder10_synth.txt', 'done\\\\decoder11_synth.txt', 'done\\\\decoder12_synth.txt', 'done\\\\decoder13_synth.txt', 'done\\\\decoder14_synth.txt', 'done\\\\decoder15_synth.txt', 'done\\\\decoder16_synth.txt', 'done\\\\decoder17_synth.txt', 'done\\\\decoder18_synth.txt', 'done\\\\decoder19_synth.txt', 'done\\\\decoder1_synth.txt', 'done\\\\decoder20_synth.txt', 'done\\\\decoder21_synth.txt', 'done\\\\decoder22_synth.txt', 'done\\\\decoder23_synth.txt', 'done\\\\decoder24_synth.txt', 'done\\\\decoder25_synth.txt', 'done\\\\decoder26_synth.txt', 'done\\\\decoder27_synth.txt', 'done\\\\decoder28_synth.txt', 'done\\\\decoder29_synth.txt', 'done\\\\decoder2_synth.txt', 'done\\\\decoder30_synth.txt', 'done\\\\decoder31_synth.txt', 'done\\\\decoder32_synth.txt', 'done\\\\decoder3_synth.txt', 'done\\\\decoder4_synth.txt', 'done\\\\decoder5_synth.txt', 'done\\\\decoder6_synth.txt', 'done\\\\decoder7_synth.txt', 'done\\\\decoder8_synth.txt', 'done\\\\decoder9_synth.txt', 'done\\\\encoder10_synth.txt', 'done\\\\encoder11_synth.txt', 'done\\\\encoder12_synth.txt', 'done\\\\encoder13_synth.txt', 'done\\\\encoder14_synth.txt', 'done\\\\encoder15_synth.txt', 'done\\\\encoder16_synth.txt', 'done\\\\encoder17_synth.txt', 'done\\\\encoder18_synth.txt', 'done\\\\encoder19_synth.txt', 'done\\\\encoder1_synth.txt', 'done\\\\encoder20_synth.txt', 'done\\\\encoder21_synth.txt', 'done\\\\encoder22_synth.txt', 'done\\\\encoder23_synth.txt', 'done\\\\encoder24_synth.txt', 'done\\\\encoder25_synth.txt', 'done\\\\encoder2_synth.txt', 'done\\\\encoder3_synth.txt', 'done\\\\encoder4_synth.txt', 'done\\\\encoder5_synth.txt', 'done\\\\encoder6_synth.txt', 'done\\\\encoder7_synth.txt', 'done\\\\encoder8_synth.txt', 'done\\\\encoder9_synth.txt', 'done\\\\mult10_synth.txt', 'done\\\\mult11_synth.txt', 'done\\\\mult12_synth.txt', 'done\\\\mult13_synth.txt', 'done\\\\mult14_synth.txt', 'done\\\\mult15_synth.txt', 'done\\\\mult16_synth.txt', 'done\\\\mult17_synth.txt', 'done\\\\mult18_synth.txt', 'done\\\\mult19_synth.txt', 'done\\\\mult1_synth.txt', 'done\\\\mult20_synth.txt', 'done\\\\mult21_synth.txt', 'done\\\\mult22_synth.txt', 'done\\\\mult23_synth.txt', 'done\\\\mult25_synth.txt', 'done\\\\mult26_synth.txt', 'done\\\\mult27_synth.txt', 'done\\\\mult28_synth.txt', 'done\\\\mult29_synth.txt', 'done\\\\mult2_synth.txt', 'done\\\\mult30_synth.txt', 'done\\\\mult3_synth.txt', 'done\\\\mult4_synth.txt', 'done\\\\mult5_synth.txt', 'done\\\\mult6_synth.txt', 'done\\\\mult7_synth.txt', 'done\\\\mult8_synth.txt', 'done\\\\mult9_synth.txt', 'done\\\\mux10_synth.txt', 'done\\\\mux11_synth.txt', 'done\\\\mux12_synth.txt', 'done\\\\mux13_synth.txt', 'done\\\\mux15_synth.txt', 'done\\\\mux16_synth.txt', 'done\\\\mux17_synth.txt', 'done\\\\mux18_synth.txt', 'done\\\\mux19_synth.txt', 'done\\\\mux1_synth.txt', 'done\\\\mux20_synth.txt', 'done\\\\mux21_synth.txt', 'done\\\\mux22_synth.txt', 'done\\\\mux23_synth.txt', 'done\\\\mux24_synth.txt', 'done\\\\mux25_synth.txt', 'done\\\\mux26_synth.txt', 'done\\\\mux27_synth.txt', 'done\\\\mux28_synth.txt', 'done\\\\mux2_synth.txt', 'done\\\\mux3_synth.txt', 'done\\\\mux4_synth.txt', 'done\\\\mux5_synth.txt', 'done\\\\mux6_synth.txt', 'done\\\\mux7_synth.txt', 'done\\\\mux8_synth.txt', 'done\\\\mux9_synth.txt', 'done\\\\nand10_synth.txt', 'done\\\\nand11_synth.txt', 'done\\\\nand12_gate_synth.txt', 'done\\\\nand13_synth.txt', 'done\\\\nand14_synth.txt', 'done\\\\nand15_synth.txt', 'done\\\\nand16_synth.txt', 'done\\\\nand17_synth.txt', 'done\\\\nand18_synth.txt', 'done\\\\nand19_synth.txt', 'done\\\\nand1_synth.txt', 'done\\\\nand20_gate_synth.txt', 'done\\\\nand21_synth.txt', 'done\\\\nand22_synth.txt', 'done\\\\nand23_synth.txt', 'done\\\\nand24_synth.txt', 'done\\\\nand25_synth.txt', 'done\\\\nand26_synth.txt', 'done\\\\nand27_synth.txt', 'done\\\\nand28_synth.txt', 'done\\\\nand29_synth.txt', 'done\\\\nand2_gate_synth.txt', 'done\\\\nand30_synth.txt', 'done\\\\nand31_synth.txt', 'done\\\\nand3_gate_synth.txt', 'done\\\\nand4_gate_synth.txt', 'done\\\\nand5_gate_synth.txt', 'done\\\\nand6_gate_synth.txt', 'done\\\\nand7_gate_synth.txt', 'done\\\\nand8_gate_synth.txt', 'done\\\\nand9_gate_synth.txt', 'done\\\\nor10_synth.txt', 'done\\\\nor11_synth.txt', 'done\\\\nor12_gate_synth.txt', 'done\\\\nor13_synth.txt', 'done\\\\nor14_synth.txt', 'done\\\\nor15_synth.txt', 'done\\\\nor16_synth.txt', 'done\\\\nor17_synth.txt', 'done\\\\nor18_synth.txt', 'done\\\\nor19_synth.txt', 'done\\\\nor1_synth.txt', 'done\\\\nor20_synth.txt', 'done\\\\nor21_synth.txt', 'done\\\\nor22_synth.txt', 'done\\\\nor23_synth.txt', 'done\\\\nor24_synth.txt', 'done\\\\nor25_synth.txt', 'done\\\\nor26_synth.txt', 'done\\\\nor27_gate_synth.txt', 'done\\\\nor28_synth.txt', 'done\\\\nor29_synth.txt', 'done\\\\nor2_gate_synth.txt', 'done\\\\nor30_synth.txt', 'done\\\\nor3_gate_synth.txt', 'done\\\\nor4_gate_synth.txt', 'done\\\\nor5_gate_synth.txt', 'done\\\\nor6_gate_synth.txt', 'done\\\\nor7_synth.txt', 'done\\\\nor8_gate_synth.txt', 'done\\\\nor9_synth.txt', 'done\\\\not10_synth.txt', 'done\\\\not11_synth.txt', 'done\\\\not12_synth.txt', 'done\\\\not13_synth.txt', 'done\\\\not14_synth.txt', 'done\\\\not15_synth.txt', 'done\\\\not16_synth.txt', 'done\\\\not1_synth.txt', 'done\\\\not2_synth.txt', 'done\\\\not3_synth.txt', 'done\\\\not4_synth.txt', 'done\\\\not5_synth.txt', 'done\\\\not6_synth.txt', 'done\\\\not7_synth.txt', 'done\\\\not8_synth.txt', 'done\\\\not9_synth.txt', 'done\\\\or11_synth.txt', 'done\\\\or13_synth.txt', 'done\\\\or14_synth.txt', 'done\\\\or15_synth.txt', 'done\\\\or16_synth.txt', 'done\\\\or17_gate_synth.txt', 'done\\\\or18_synth.txt', 'done\\\\or19_synth.txt', 'done\\\\or1_synth.txt', 'done\\\\or20_synth.txt', 'done\\\\or21_synth.txt', 'done\\\\or22_synth.txt', 'done\\\\or23_synth.txt', 'done\\\\or24_synth.txt', 'done\\\\or25_synth.txt', 'done\\\\or26_synth.txt', 'done\\\\or27_synth.txt', 'done\\\\or28_synth.txt', 'done\\\\or29_synth.txt', 'done\\\\or2_gate_synth.txt', 'done\\\\or3_gate_synth.txt', 'done\\\\or4_gate_synth.txt', 'done\\\\or5_gate_synth.txt', 'done\\\\or6_gate_synth.txt', 'done\\\\or7_synth.txt', 'done\\\\or8_gate_synth.txt', 'done\\\\or9_synth.txt', 'done\\\\pe10_synth.txt', 'done\\\\pe11_synth.txt', 'done\\\\pe12_synth.txt', 'done\\\\pe13_synth.txt', 'done\\\\pe14_synth.txt', 'done\\\\pe15_synth.txt', 'done\\\\pe16_synth.txt', 'done\\\\pe17_synth.txt', 'done\\\\pe18_synth.txt', 'done\\\\pe19_synth.txt', 'done\\\\pe1_synth.txt', 'done\\\\pe20_synth.txt', 'done\\\\pe21_synth.txt', 'done\\\\pe22_synth.txt', 'done\\\\pe2_synth.txt', 'done\\\\pe3_synth.txt', 'done\\\\pe4_synth.txt', 'done\\\\pe5_synth.txt', 'done\\\\pe6_synth.txt', 'done\\\\pe8_synth.txt', 'done\\\\pe9_synth.txt', 'done\\\\sub10_synth.txt', 'done\\\\sub11_synth.txt', 'done\\\\sub12_synth.txt', 'done\\\\sub1_synth.txt', 'done\\\\sub2_synth.txt', 'done\\\\sub4_synth.txt', 'done\\\\sub5_synth.txt', 'done\\\\sub6_synth.txt', 'done\\\\sub7_synth.txt', 'done\\\\sub8_synth.txt', 'done\\\\sub9_synth.txt', 'done\\\\xnor10_synth.txt', 'done\\\\xnor11_synth.txt', 'done\\\\xnor12_synth.txt', 'done\\\\xnor13_synth.txt', 'done\\\\xnor14_synth.txt', 'done\\\\xnor15_synth.txt', 'done\\\\xnor16_synth.txt', 'done\\\\xnor17_synth.txt', 'done\\\\xnor18_synth.txt', 'done\\\\xnor19_synth.txt', 'done\\\\xnor1_synth.txt', 'done\\\\xnor20_synth.txt', 'done\\\\xnor21_synth.txt', 'done\\\\xnor22_synth.txt', 'done\\\\xnor23_synth.txt', 'done\\\\xnor24_synth.txt', 'done\\\\xnor25_synth.txt', 'done\\\\xnor26_synth.txt', 'done\\\\xnor27_synth.txt', 'done\\\\xnor28_synth.txt', 'done\\\\xnor29_synth.txt', 'done\\\\xnor2_synth.txt', 'done\\\\xnor30_synth.txt', 'done\\\\xnor3_synth.txt', 'done\\\\xnor4_synth.txt', 'done\\\\xnor5_synth.txt', 'done\\\\xnor6_synth.txt', 'done\\\\xnor7_synth.txt', 'done\\\\xnor8_synth.txt', 'done\\\\xnor9_synth.txt', 'done\\\\xor10_synth.txt', 'done\\\\xor11_synth.txt', 'done\\\\xor12_synth.txt', 'done\\\\xor13_synth.txt', 'done\\\\xor14_synth.txt', 'done\\\\xor15_synth.txt', 'done\\\\xor16_synth.txt', 'done\\\\xor17_synth.txt', 'done\\\\xor18_synth.txt', 'done\\\\xor19_synth.txt', 'done\\\\xor1_synth.txt', 'done\\\\xor20_synth.txt', 'done\\\\xor21_synth.txt', 'done\\\\xor22_synth.txt', 'done\\\\xor23_synth.txt', 'done\\\\xor24_synth.txt', 'done\\\\xor25_synth.txt', 'done\\\\xor26_synth.txt', 'done\\\\xor27_synth.txt', 'done\\\\xor28_synth.txt', 'done\\\\xor29_synth.txt', 'done\\\\xor2_synth.txt', 'done\\\\xor3_synth.txt', 'done\\\\xor4_synth.txt', 'done\\\\xor5_synth.txt', 'done\\\\xor6_synth.txt', 'done\\\\xor7_synth.txt', 'done\\\\xor8_synth.txt']\n"
     ]
    }
   ],
   "source": [
    "def get_files_in_folder(input_folder):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'done'\n",
    "verilog_files = get_files_in_folder(folder_path)\n",
    "print(len(verilog_files))\n",
    "print(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                label = loaded_data[2]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                num_nodes = x.size(0)\n",
    "                \n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 387 Verilog files.\n",
      "387\n"
     ]
    }
   ],
   "source": [
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[54, 7], edge_index=[2, 72], y=[1, 15], batch=[54])\n",
      "done\\adder10_synth.txt\n",
      "done\\adder10_synth.txt\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(verilog_files[0])\n",
    "print(dataset.verilog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data objects are unique.\n"
     ]
    }
   ],
   "source": [
    "def are_all_data_objects_unique(dataset):\n",
    "    data_objects = []\n",
    "    for data in dataset:\n",
    "        if data in data_objects:\n",
    "            return False\n",
    "        data_objects.append(data)\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "is_unique = are_all_data_objects_unique(dataset)\n",
    "if is_unique:\n",
    "    print(\"All data objects are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate data objects found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = []\n",
    "for data in dataset:\n",
    "    # print(data)\n",
    "    # print(data.y.tolist())\n",
    "    y_labels.append(np.argmax(data.y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y_labels, test_size=0.2, stratify = y_labels, random_state=41)\n",
    "train_loader = DataLoader(X_train, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(X_test, batch_size=16, shuffle = False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[57, 7], edge_index=[2, 69], y=[1, 15], batch=[57])\n"
     ]
    }
   ],
   "source": [
    "# len(train_loader.dataset)\n",
    "print(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "batch = next(loader_iter)\n",
    "# print(batch)\n",
    "# print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "\n",
    "class FunctionalGNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FunctionalGNN, self).__init__(aggr='mean')  # or other aggregations like 'max' or 'sum'\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_j):\n",
    "        # x_j contains the node features of neighbors\n",
    "        return x_j\n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        # Apply a transformation to the aggregated messages\n",
    "        return self.lin(aggr_out)\n",
    "\n",
    "class NetlistModel(nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(NetlistModel, self).__init__()\n",
    "        self.conv1 = FunctionalGNN(num_node_features, 64)\n",
    "        self.conv2 = FunctionalGNN(64, num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch = None)\n",
    "        return nn.LogSoftmax(dim=1)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6034250259399414\n",
      "Epoch 1, Loss: 0.6033401489257812\n",
      "Epoch 2, Loss: 5.123282432556152\n",
      "Epoch 3, Loss: 1.8114774227142334\n",
      "Epoch 4, Loss: 1.9421035051345825\n",
      "Epoch 5, Loss: 1.8711310625076294\n",
      "Epoch 6, Loss: 0.6114148497581482\n",
      "Epoch 7, Loss: 1.3488481044769287\n",
      "Epoch 8, Loss: 0.784159243106842\n",
      "Epoch 9, Loss: 1.988486409187317\n",
      "Epoch 10, Loss: 0.8302208185195923\n",
      "Epoch 11, Loss: 2.229836940765381\n",
      "Epoch 12, Loss: 0.4226386845111847\n",
      "Epoch 13, Loss: 0.7152817249298096\n",
      "Epoch 14, Loss: 0.4947541058063507\n",
      "Epoch 15, Loss: 1.3369243144989014\n",
      "Epoch 16, Loss: 0.6345152258872986\n",
      "Epoch 17, Loss: 1.3860591650009155\n",
      "Epoch 18, Loss: 0.03320732340216637\n",
      "Epoch 19, Loss: 1.4694687128067017\n",
      "Epoch 20, Loss: 2.4377429485321045\n",
      "Epoch 21, Loss: 0.43617990612983704\n",
      "Epoch 22, Loss: 1.7916841506958008\n",
      "Epoch 23, Loss: 0.7586542963981628\n",
      "Epoch 24, Loss: 0.3030214309692383\n",
      "Epoch 25, Loss: 1.1925098896026611\n",
      "Epoch 26, Loss: 1.203300952911377\n",
      "Epoch 27, Loss: 3.6402111053466797\n",
      "Epoch 28, Loss: 0.7376676797866821\n",
      "Epoch 29, Loss: 0.5415331721305847\n",
      "Epoch 30, Loss: 0.26498934626579285\n",
      "Epoch 31, Loss: 0.22355487942695618\n",
      "Epoch 32, Loss: 1.9382946491241455\n",
      "Epoch 33, Loss: 0.17876273393630981\n",
      "Epoch 34, Loss: 0.1620393693447113\n",
      "Epoch 35, Loss: 0.5592374205589294\n",
      "Epoch 36, Loss: 1.2916228771209717\n",
      "Epoch 37, Loss: 1.9567742347717285\n",
      "Epoch 38, Loss: 0.45164260268211365\n",
      "Epoch 39, Loss: 0.40688571333885193\n",
      "Epoch 40, Loss: 0.06884118169546127\n",
      "Epoch 41, Loss: 0.7368384003639221\n",
      "Epoch 42, Loss: 1.5707517862319946\n",
      "Epoch 43, Loss: 1.7171348333358765\n",
      "Epoch 44, Loss: 0.7253339290618896\n",
      "Epoch 45, Loss: 0.5242853164672852\n",
      "Epoch 46, Loss: 0.670117199420929\n",
      "Epoch 47, Loss: 1.8704712390899658\n",
      "Epoch 48, Loss: 0.9546530842781067\n",
      "Epoch 49, Loss: 3.047328233718872\n",
      "Epoch 50, Loss: 0.13349764049053192\n",
      "Epoch 51, Loss: 0.3686752915382385\n",
      "Epoch 52, Loss: 0.03981677442789078\n",
      "Epoch 53, Loss: 0.06830967962741852\n",
      "Epoch 54, Loss: 3.259333848953247\n",
      "Epoch 55, Loss: 1.7450419664382935\n",
      "Epoch 56, Loss: 0.5676823854446411\n",
      "Epoch 57, Loss: 0.4554147720336914\n",
      "Epoch 58, Loss: 1.3192288875579834\n",
      "Epoch 59, Loss: 1.5583857297897339\n",
      "Epoch 60, Loss: 0.19915638864040375\n",
      "Epoch 61, Loss: 0.864953875541687\n",
      "Epoch 62, Loss: 0.032867658883333206\n",
      "Epoch 63, Loss: 1.3033764362335205\n",
      "Epoch 64, Loss: 0.15640513598918915\n",
      "Epoch 65, Loss: 1.726264238357544\n",
      "Epoch 66, Loss: 0.3296498954296112\n",
      "Epoch 67, Loss: 0.4942163825035095\n",
      "Epoch 68, Loss: 1.3454352617263794\n",
      "Epoch 69, Loss: 0.45107102394104004\n",
      "Epoch 70, Loss: 0.9365060925483704\n",
      "Epoch 71, Loss: 1.3152625560760498\n",
      "Epoch 72, Loss: 1.7128829956054688\n",
      "Epoch 73, Loss: 1.452739953994751\n",
      "Epoch 74, Loss: 0.13099636137485504\n",
      "Epoch 75, Loss: 2.853986978530884\n",
      "Epoch 76, Loss: 1.4210667610168457\n",
      "Epoch 77, Loss: 0.5221356749534607\n",
      "Epoch 78, Loss: 1.4329196214675903\n",
      "Epoch 79, Loss: 0.0226905457675457\n",
      "Epoch 80, Loss: 0.835673451423645\n",
      "Epoch 81, Loss: 0.050319597125053406\n",
      "Epoch 82, Loss: 1.3470771312713623\n",
      "Epoch 83, Loss: 0.8220400214195251\n",
      "Epoch 84, Loss: 0.14937709271907806\n",
      "Epoch 85, Loss: 2.8172614574432373\n",
      "Epoch 86, Loss: 0.5567682385444641\n",
      "Epoch 87, Loss: 0.8079812526702881\n",
      "Epoch 88, Loss: 0.9688730835914612\n",
      "Epoch 89, Loss: 0.5505821108818054\n",
      "Epoch 90, Loss: 1.0615148544311523\n",
      "Epoch 91, Loss: 0.09574879705905914\n",
      "Epoch 92, Loss: 1.647505760192871\n",
      "Epoch 93, Loss: 0.7085108757019043\n",
      "Epoch 94, Loss: 0.5632047057151794\n",
      "Epoch 95, Loss: 0.0950876921415329\n",
      "Epoch 96, Loss: 0.6546429395675659\n",
      "Epoch 97, Loss: 0.32597431540489197\n",
      "Epoch 98, Loss: 1.1049792766571045\n",
      "Epoch 99, Loss: 1.6295647621154785\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "num_node_features = 7\n",
    "num_classes = 15\n",
    "# Assuming you have a dataset of netlists\n",
    "train_loader = DataLoader(X_train, batch_size=1, shuffle=True)\n",
    "model = NetlistModel(num_node_features, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        # print(data)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # print(out)\n",
    "        # print(data.y)\n",
    "        loss = loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6245954692556634"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "for data in X_train:\n",
    "    out = model(data)  \n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    y_label = (data.y.tolist())\n",
    "    y_label = y_label[0].index(1.0)\n",
    "    pred_label = (pred.tolist())[0]\n",
    "    # print(pred_label)\n",
    "    # print(y_label)\n",
    "    if y_label == pred_label:\n",
    "        correct += 1            \n",
    "    # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "train_acc = correct / len(X_train)  # Derive ratio of correct predictions.\n",
    "\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5769230769230769"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "for data in X_test:\n",
    "    out = model(data)  \n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    y_label = (data.y.tolist())\n",
    "    y_label = y_label[0].index(1.0)\n",
    "    pred_label = (pred.tolist())[0]\n",
    "    # print(pred_label)\n",
    "    # print(y_label)\n",
    "    if y_label == pred_label:\n",
    "        correct += 1            \n",
    "    # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "test_acc = correct / len(X_test)  # Derive ratio of correct predictions.\n",
    "\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'GNN62_58_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].y)\n",
    "print(dataset[10].y)\n",
    "print(dataset[0].y.tolist() == dataset[10].y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_positive_negative_pairs(dataset):\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    for i, data_i in enumerate(dataset):\n",
    "        for j, data_j in enumerate(dataset):\n",
    "            if i != j:\n",
    "                if data_i.y.tolist() == data_j.y.tolist():  # Define your own similarity function\n",
    "                    positive_pairs.append((data_i, data_j))\n",
    "                else:\n",
    "                    negative_pairs.append((data_i, data_j))\n",
    "    return positive_pairs, negative_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6354"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_pairs, negative_pairs = generate_positive_negative_pairs(X_train)\n",
    "len(positive_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88818"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95172\n"
     ]
    }
   ],
   "source": [
    "total_pairs = positive_pairs + negative_pairs\n",
    "print(len(total_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(3-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.022616562414411578\n",
      "Epoch 2, Loss: 0.01669057390476685\n",
      "Epoch 3, Loss: 0.016690571693539026\n",
      "Epoch 4, Loss: 0.01669056635410213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m out_j \u001b[38;5;241m=\u001b[39m model2(data_j)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Label is 1 for positive pairs and 0 for negative pairs\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (data_i, data_j) \u001b[38;5;129;01min\u001b[39;00m positive_pairs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(out_i\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m contrastive_loss_fn(out_i, out_j, label)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have a dataset of netlists\n",
    "# train_loader = DataLoader(X_train, batch_size=1, shuffle=True)\n",
    "model2 = NetlistModel(num_node_features, num_classes)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "contrastive_loss_fn = ContrastiveLoss(margin=0.5)\n",
    "\n",
    "# Generate positive and negative pairs\n",
    "\n",
    "prev = 0\n",
    "for epoch in range(100):\n",
    "    model2.train()\n",
    "    total_loss = 0\n",
    "    for (data_i, data_j) in total_pairs:\n",
    "        # print(c)\n",
    "        # c+=1 \n",
    "        optimizer.zero_grad()\n",
    "        out_i = model2(data_i)\n",
    "        out_j = model2(data_j)\n",
    "        # Label is 1 for positive pairs and 0 for negative pairs\n",
    "        label = 1 if (data_i, data_j) in positive_pairs else 0\n",
    "        label = torch.tensor([label], dtype=torch.float32).to(out_i.device)\n",
    "        loss = contrastive_loss_fn(out_i, out_j, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if abs(prev - total_loss) < 0.0001:\n",
    "        break\n",
    "    prev = total_loss\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(total_pairs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07766990291262135"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()\n",
    "correct = 0\n",
    "for data in X_train:\n",
    "    out = model2(data)  \n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    y_label = (data.y.tolist())\n",
    "    y_label = y_label[0].index(1.0)\n",
    "    pred_label = (pred.tolist())[0]\n",
    "    # print(pred_label)\n",
    "    # print(y_label)\n",
    "    if y_label == pred_label:\n",
    "        correct += 1            \n",
    "    # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "test_acc = correct / len(X_train)  # Derive ratio of correct predictions.\n",
    "\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
