{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (2.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.7.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (5.9.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (4.64.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (3.8.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2022.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.27.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (5.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->torch_geometric) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->torch_geometric) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torch_geometric) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%pip install torch_geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n",
      "['done\\\\adder10_synth.txt', 'done\\\\adder11_synth.txt', 'done\\\\adder12_synth.txt', 'done\\\\adder13_synth.txt', 'done\\\\adder14_synth.txt', 'done\\\\adder15_synth.txt', 'done\\\\adder16_synth.txt', 'done\\\\adder17_synth.txt', 'done\\\\adder18_synth.txt', 'done\\\\adder19_synth.txt', 'done\\\\adder1_synth.txt', 'done\\\\adder20_synth.txt', 'done\\\\adder21_synth.txt', 'done\\\\adder22_synth.txt', 'done\\\\adder23_synth.txt', 'done\\\\adder24_synth.txt', 'done\\\\adder25_synth.txt', 'done\\\\adder26_synth.txt', 'done\\\\adder27_synth.txt', 'done\\\\adder28_synth.txt', 'done\\\\adder2_synth.txt', 'done\\\\adder3_synth.txt', 'done\\\\adder4_synth.txt', 'done\\\\adder5_synth.txt', 'done\\\\adder6_synth.txt', 'done\\\\adder7_synth.txt', 'done\\\\adder8_synth.txt', 'done\\\\adder9_synth.txt', 'done\\\\and10_gate_synth.txt', 'done\\\\and11_gate_synth.txt', 'done\\\\and12_gate_synth.txt', 'done\\\\and13_synth.txt', 'done\\\\and14_synth.txt', 'done\\\\and15_synth.txt', 'done\\\\and16_synth.txt', 'done\\\\and17_synth.txt', 'done\\\\and18_gate_synth.txt', 'done\\\\and19_synth.txt', 'done\\\\and1_synth.txt', 'done\\\\and20_synth.txt', 'done\\\\and21_synth.txt', 'done\\\\and22_synth.txt', 'done\\\\and23_synth.txt', 'done\\\\and24_synth.txt', 'done\\\\and25_synth.txt', 'done\\\\and26_synth.txt', 'done\\\\and27_synth.txt', 'done\\\\and28_synth.txt', 'done\\\\and29_synth.txt', 'done\\\\and2_gate_synth.txt', 'done\\\\and30_synth.txt', 'done\\\\and3_gate_synth.txt', 'done\\\\and4_gate_synth.txt', 'done\\\\and5_gate_synth.txt', 'done\\\\and6_gate_synth.txt', 'done\\\\and7_gate_synth.txt', 'done\\\\and8_gate_synth.txt', 'done\\\\and9_gate_synth.txt', 'done\\\\comparator10_synth.txt', 'done\\\\comparator11_synth.txt', 'done\\\\comparator12_synth.txt', 'done\\\\comparator13_synth.txt', 'done\\\\comparator14_synth.txt', 'done\\\\comparator15_synth.txt', 'done\\\\comparator16_synth.txt', 'done\\\\comparator17_synth.txt', 'done\\\\comparator18_synth.txt', 'done\\\\comparator19_synth.txt', 'done\\\\comparator1_synth.txt', 'done\\\\comparator20_synth.txt', 'done\\\\comparator21_synth.txt', 'done\\\\comparator22_synth.txt', 'done\\\\comparator2_synth.txt', 'done\\\\comparator3_synth.txt', 'done\\\\comparator4_synth.txt', 'done\\\\comparator5_synth.txt', 'done\\\\comparator6_synth.txt', 'done\\\\comparator7_synth.txt', 'done\\\\comparator8_synth.txt', 'done\\\\comparator9_synth.txt', 'done\\\\decoder10_synth.txt', 'done\\\\decoder11_synth.txt', 'done\\\\decoder12_synth.txt', 'done\\\\decoder13_synth.txt', 'done\\\\decoder14_synth.txt', 'done\\\\decoder15_synth.txt', 'done\\\\decoder16_synth.txt', 'done\\\\decoder17_synth.txt', 'done\\\\decoder18_synth.txt', 'done\\\\decoder19_synth.txt', 'done\\\\decoder1_synth.txt', 'done\\\\decoder20_synth.txt', 'done\\\\decoder21_synth.txt', 'done\\\\decoder22_synth.txt', 'done\\\\decoder23_synth.txt', 'done\\\\decoder24_synth.txt', 'done\\\\decoder25_synth.txt', 'done\\\\decoder26_synth.txt', 'done\\\\decoder27_synth.txt', 'done\\\\decoder28_synth.txt', 'done\\\\decoder29_synth.txt', 'done\\\\decoder2_synth.txt', 'done\\\\decoder30_synth.txt', 'done\\\\decoder31_synth.txt', 'done\\\\decoder32_synth.txt', 'done\\\\decoder3_synth.txt', 'done\\\\decoder4_synth.txt', 'done\\\\decoder5_synth.txt', 'done\\\\decoder6_synth.txt', 'done\\\\decoder7_synth.txt', 'done\\\\decoder8_synth.txt', 'done\\\\decoder9_synth.txt', 'done\\\\encoder10_synth.txt', 'done\\\\encoder11_synth.txt', 'done\\\\encoder12_synth.txt', 'done\\\\encoder13_synth.txt', 'done\\\\encoder14_synth.txt', 'done\\\\encoder15_synth.txt', 'done\\\\encoder16_synth.txt', 'done\\\\encoder17_synth.txt', 'done\\\\encoder18_synth.txt', 'done\\\\encoder19_synth.txt', 'done\\\\encoder1_synth.txt', 'done\\\\encoder20_synth.txt', 'done\\\\encoder21_synth.txt', 'done\\\\encoder22_synth.txt', 'done\\\\encoder23_synth.txt', 'done\\\\encoder24_synth.txt', 'done\\\\encoder25_synth.txt', 'done\\\\encoder2_synth.txt', 'done\\\\encoder3_synth.txt', 'done\\\\encoder4_synth.txt', 'done\\\\encoder5_synth.txt', 'done\\\\encoder6_synth.txt', 'done\\\\encoder7_synth.txt', 'done\\\\encoder8_synth.txt', 'done\\\\encoder9_synth.txt', 'done\\\\mult10_synth.txt', 'done\\\\mult11_synth.txt', 'done\\\\mult12_synth.txt', 'done\\\\mult13_synth.txt', 'done\\\\mult14_synth.txt', 'done\\\\mult15_synth.txt', 'done\\\\mult16_synth.txt', 'done\\\\mult17_synth.txt', 'done\\\\mult18_synth.txt', 'done\\\\mult19_synth.txt', 'done\\\\mult1_synth.txt', 'done\\\\mult20_synth.txt', 'done\\\\mult21_synth.txt', 'done\\\\mult22_synth.txt', 'done\\\\mult23_synth.txt', 'done\\\\mult25_synth.txt', 'done\\\\mult26_synth.txt', 'done\\\\mult27_synth.txt', 'done\\\\mult28_synth.txt', 'done\\\\mult29_synth.txt', 'done\\\\mult2_synth.txt', 'done\\\\mult30_synth.txt', 'done\\\\mult3_synth.txt', 'done\\\\mult4_synth.txt', 'done\\\\mult5_synth.txt', 'done\\\\mult6_synth.txt', 'done\\\\mult7_synth.txt', 'done\\\\mult8_synth.txt', 'done\\\\mult9_synth.txt', 'done\\\\mux10_synth.txt', 'done\\\\mux11_synth.txt', 'done\\\\mux12_synth.txt', 'done\\\\mux13_synth.txt', 'done\\\\mux15_synth.txt', 'done\\\\mux16_synth.txt', 'done\\\\mux17_synth.txt', 'done\\\\mux18_synth.txt', 'done\\\\mux19_synth.txt', 'done\\\\mux1_synth.txt', 'done\\\\mux20_synth.txt', 'done\\\\mux21_synth.txt', 'done\\\\mux22_synth.txt', 'done\\\\mux23_synth.txt', 'done\\\\mux24_synth.txt', 'done\\\\mux25_synth.txt', 'done\\\\mux26_synth.txt', 'done\\\\mux27_synth.txt', 'done\\\\mux28_synth.txt', 'done\\\\mux2_synth.txt', 'done\\\\mux3_synth.txt', 'done\\\\mux4_synth.txt', 'done\\\\mux5_synth.txt', 'done\\\\mux6_synth.txt', 'done\\\\mux7_synth.txt', 'done\\\\mux8_synth.txt', 'done\\\\mux9_synth.txt', 'done\\\\nand10_synth.txt', 'done\\\\nand11_synth.txt', 'done\\\\nand12_gate_synth.txt', 'done\\\\nand13_synth.txt', 'done\\\\nand14_synth.txt', 'done\\\\nand15_synth.txt', 'done\\\\nand16_synth.txt', 'done\\\\nand17_synth.txt', 'done\\\\nand18_synth.txt', 'done\\\\nand19_synth.txt', 'done\\\\nand1_synth.txt', 'done\\\\nand20_gate_synth.txt', 'done\\\\nand21_synth.txt', 'done\\\\nand22_synth.txt', 'done\\\\nand23_synth.txt', 'done\\\\nand24_synth.txt', 'done\\\\nand25_synth.txt', 'done\\\\nand26_synth.txt', 'done\\\\nand27_synth.txt', 'done\\\\nand28_synth.txt', 'done\\\\nand29_synth.txt', 'done\\\\nand2_gate_synth.txt', 'done\\\\nand30_synth.txt', 'done\\\\nand31_synth.txt', 'done\\\\nand3_gate_synth.txt', 'done\\\\nand4_gate_synth.txt', 'done\\\\nand5_gate_synth.txt', 'done\\\\nand6_gate_synth.txt', 'done\\\\nand7_gate_synth.txt', 'done\\\\nand8_gate_synth.txt', 'done\\\\nand9_gate_synth.txt', 'done\\\\nor10_synth.txt', 'done\\\\nor11_synth.txt', 'done\\\\nor12_gate_synth.txt', 'done\\\\nor13_synth.txt', 'done\\\\nor14_synth.txt', 'done\\\\nor15_synth.txt', 'done\\\\nor16_synth.txt', 'done\\\\nor17_synth.txt', 'done\\\\nor18_synth.txt', 'done\\\\nor19_synth.txt', 'done\\\\nor1_synth.txt', 'done\\\\nor20_synth.txt', 'done\\\\nor21_synth.txt', 'done\\\\nor22_synth.txt', 'done\\\\nor23_synth.txt', 'done\\\\nor24_synth.txt', 'done\\\\nor25_synth.txt', 'done\\\\nor26_synth.txt', 'done\\\\nor27_gate_synth.txt', 'done\\\\nor28_synth.txt', 'done\\\\nor29_synth.txt', 'done\\\\nor2_gate_synth.txt', 'done\\\\nor30_synth.txt', 'done\\\\nor3_gate_synth.txt', 'done\\\\nor4_gate_synth.txt', 'done\\\\nor5_gate_synth.txt', 'done\\\\nor6_gate_synth.txt', 'done\\\\nor7_synth.txt', 'done\\\\nor8_gate_synth.txt', 'done\\\\nor9_synth.txt', 'done\\\\not10_synth.txt', 'done\\\\not11_synth.txt', 'done\\\\not12_synth.txt', 'done\\\\not13_synth.txt', 'done\\\\not14_synth.txt', 'done\\\\not15_synth.txt', 'done\\\\not16_synth.txt', 'done\\\\not1_synth.txt', 'done\\\\not2_synth.txt', 'done\\\\not3_synth.txt', 'done\\\\not4_synth.txt', 'done\\\\not5_synth.txt', 'done\\\\not6_synth.txt', 'done\\\\not7_synth.txt', 'done\\\\not8_synth.txt', 'done\\\\not9_synth.txt', 'done\\\\or11_synth.txt', 'done\\\\or13_synth.txt', 'done\\\\or14_synth.txt', 'done\\\\or15_synth.txt', 'done\\\\or16_synth.txt', 'done\\\\or17_gate_synth.txt', 'done\\\\or18_synth.txt', 'done\\\\or19_synth.txt', 'done\\\\or1_synth.txt', 'done\\\\or20_synth.txt', 'done\\\\or21_synth.txt', 'done\\\\or22_synth.txt', 'done\\\\or23_synth.txt', 'done\\\\or24_synth.txt', 'done\\\\or25_synth.txt', 'done\\\\or26_synth.txt', 'done\\\\or27_synth.txt', 'done\\\\or28_synth.txt', 'done\\\\or29_synth.txt', 'done\\\\or2_gate_synth.txt', 'done\\\\or3_gate_synth.txt', 'done\\\\or4_gate_synth.txt', 'done\\\\or5_gate_synth.txt', 'done\\\\or6_gate_synth.txt', 'done\\\\or7_synth.txt', 'done\\\\or8_gate_synth.txt', 'done\\\\or9_synth.txt', 'done\\\\pe10_synth.txt', 'done\\\\pe11_synth.txt', 'done\\\\pe12_synth.txt', 'done\\\\pe13_synth.txt', 'done\\\\pe14_synth.txt', 'done\\\\pe15_synth.txt', 'done\\\\pe16_synth.txt', 'done\\\\pe17_synth.txt', 'done\\\\pe18_synth.txt', 'done\\\\pe19_synth.txt', 'done\\\\pe1_synth.txt', 'done\\\\pe20_synth.txt', 'done\\\\pe21_synth.txt', 'done\\\\pe22_synth.txt', 'done\\\\pe2_synth.txt', 'done\\\\pe3_synth.txt', 'done\\\\pe4_synth.txt', 'done\\\\pe5_synth.txt', 'done\\\\pe6_synth.txt', 'done\\\\pe8_synth.txt', 'done\\\\pe9_synth.txt', 'done\\\\sub10_synth.txt', 'done\\\\sub11_synth.txt', 'done\\\\sub12_synth.txt', 'done\\\\sub1_synth.txt', 'done\\\\sub2_synth.txt', 'done\\\\sub4_synth.txt', 'done\\\\sub5_synth.txt', 'done\\\\sub6_synth.txt', 'done\\\\sub7_synth.txt', 'done\\\\sub8_synth.txt', 'done\\\\sub9_synth.txt', 'done\\\\xnor10_synth.txt', 'done\\\\xnor11_synth.txt', 'done\\\\xnor12_synth.txt', 'done\\\\xnor13_synth.txt', 'done\\\\xnor14_synth.txt', 'done\\\\xnor15_synth.txt', 'done\\\\xnor16_synth.txt', 'done\\\\xnor17_synth.txt', 'done\\\\xnor18_synth.txt', 'done\\\\xnor19_synth.txt', 'done\\\\xnor1_synth.txt', 'done\\\\xnor20_synth.txt', 'done\\\\xnor21_synth.txt', 'done\\\\xnor22_synth.txt', 'done\\\\xnor23_synth.txt', 'done\\\\xnor24_synth.txt', 'done\\\\xnor25_synth.txt', 'done\\\\xnor26_synth.txt', 'done\\\\xnor27_synth.txt', 'done\\\\xnor28_synth.txt', 'done\\\\xnor29_synth.txt', 'done\\\\xnor2_synth.txt', 'done\\\\xnor30_synth.txt', 'done\\\\xnor3_synth.txt', 'done\\\\xnor4_synth.txt', 'done\\\\xnor5_synth.txt', 'done\\\\xnor6_synth.txt', 'done\\\\xnor7_synth.txt', 'done\\\\xnor8_synth.txt', 'done\\\\xnor9_synth.txt', 'done\\\\xor10_synth.txt', 'done\\\\xor11_synth.txt', 'done\\\\xor12_synth.txt', 'done\\\\xor13_synth.txt', 'done\\\\xor14_synth.txt', 'done\\\\xor15_synth.txt', 'done\\\\xor16_synth.txt', 'done\\\\xor17_synth.txt', 'done\\\\xor18_synth.txt', 'done\\\\xor19_synth.txt', 'done\\\\xor1_synth.txt', 'done\\\\xor20_synth.txt', 'done\\\\xor21_synth.txt', 'done\\\\xor22_synth.txt', 'done\\\\xor23_synth.txt', 'done\\\\xor24_synth.txt', 'done\\\\xor25_synth.txt', 'done\\\\xor26_synth.txt', 'done\\\\xor27_synth.txt', 'done\\\\xor28_synth.txt', 'done\\\\xor29_synth.txt', 'done\\\\xor2_synth.txt', 'done\\\\xor3_synth.txt', 'done\\\\xor4_synth.txt', 'done\\\\xor5_synth.txt', 'done\\\\xor6_synth.txt', 'done\\\\xor7_synth.txt', 'done\\\\xor8_synth.txt']\n"
     ]
    }
   ],
   "source": [
    "def get_files_in_folder(input_folder):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'done'\n",
    "verilog_files = get_files_in_folder(folder_path)\n",
    "print(len(verilog_files))\n",
    "print(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                label = loaded_data[2]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                num_nodes = x.size(0)\n",
    "                \n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 387 Verilog files.\n",
      "387\n"
     ]
    }
   ],
   "source": [
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[54, 7], edge_index=[2, 72], y=[1, 15], batch=[54])\n",
      "done\\adder10_synth.txt\n",
      "done\\adder10_synth.txt\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(verilog_files[0])\n",
    "print(dataset.verilog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data objects are unique.\n"
     ]
    }
   ],
   "source": [
    "def are_all_data_objects_unique(dataset):\n",
    "    data_objects = []\n",
    "    for data in dataset:\n",
    "        if data in data_objects:\n",
    "            return False\n",
    "        data_objects.append(data)\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "is_unique = are_all_data_objects_unique(dataset)\n",
    "if is_unique:\n",
    "    print(\"All data objects are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate data objects found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = []\n",
    "for data in dataset:\n",
    "    # print(data)\n",
    "    # print(data.y.tolist())\n",
    "    y_labels.append(np.argmax(data.y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y_labels, test_size=0.2, stratify = y_labels, random_state=41)\n",
    "train_loader = DataLoader(X_train, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(X_test, batch_size=16, shuffle = False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[57, 7], edge_index=[2, 69], y=[1, 15], batch=[57])\n"
     ]
    }
   ],
   "source": [
    "# len(train_loader.dataset)\n",
    "print(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(train_loader)\n",
    "batch = next(loader_iter)\n",
    "# print(batch)\n",
    "# print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        emb = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(emb, batch=None)\n",
    "        return x, emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7904129028320312\n",
      "Epoch 10, Loss: 1.7022525072097778\n",
      "Epoch 20, Loss: 1.1181812286376953\n",
      "Epoch 30, Loss: 0.8820726871490479\n",
      "Epoch 40, Loss: 0.6983638405799866\n",
      "Epoch 50, Loss: 0.5130823254585266\n",
      "Epoch 60, Loss: 0.37124955654144287\n",
      "Epoch 70, Loss: 0.27920815348625183\n",
      "Epoch 80, Loss: 0.19076795876026154\n",
      "Epoch 90, Loss: 0.12619730830192566\n",
      "Epoch 100, Loss: 0.09208124876022339\n",
      "Epoch 110, Loss: 0.07312091439962387\n",
      "Epoch 120, Loss: 0.05279003828763962\n",
      "Epoch 130, Loss: 0.04468943923711777\n",
      "Epoch 140, Loss: 0.036209896206855774\n",
      "Epoch 150, Loss: 0.030864594504237175\n",
      "Epoch 160, Loss: 0.0237029530107975\n",
      "Epoch 170, Loss: 0.021002279594540596\n",
      "Epoch 180, Loss: 0.016548868268728256\n",
      "Epoch 190, Loss: 0.013550445437431335\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "in_channels = 7\n",
    "hidden_channels = 16\n",
    "out_channels = 15\n",
    "model = GraphSAGE(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "# Create a simple training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training the model\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    for data in X_train:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        #forward pass\n",
    "        out = model(data.x, data.edge_index)\n",
    "        # print(out.shape)\n",
    "        # print(data.y.shape)\n",
    "        target = torch.argmax(data.y, dim=1)\n",
    "        # calculate the loss\n",
    "        loss = loss_fn(out, target)\n",
    "        # zero the gradients of the weights so that the gradients are not accumulated\n",
    "        # calculate the gradients using backpropagation\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9892, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = 1 - loss\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "for data in X_test:\n",
    "    out = model(data.x, data.edge_index)  \n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    y_label = (data.y.tolist())\n",
    "    y_label = y_label[0].index(1.0)\n",
    "    pred_label = (pred.tolist())[0]\n",
    "    # print(pred_label)\n",
    "    # print(y_label)\n",
    "    if y_label == pred_label:\n",
    "        correct += 1            \n",
    "    # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "acc = correct / len(X_test)  # Derive ratio of correct predictions.\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Get node embeddings\n",
    "dataset_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for data in dataset:\n",
    "        out = model(data.x, data.edge_index)  \n",
    "        dataset_embeddings.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -9.9978,  -20.5545,  -21.2358,  -14.5519,  -23.7793,  -19.2123,\n",
       "          -22.1409,  -17.7232,  -99.1179, -210.9078,  -81.6170,  -95.9362,\n",
       "          -42.0372,  -15.8280,  -16.8025]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'grahpSAGE98_92_200_graph_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 7\n",
    "hidden_channels = 16\n",
    "out_channels = 15\n",
    "graph_sage = GraphSAGE(in_channels, hidden_channels, out_channels)\n",
    "graph_sage.load_state_dict(torch.load('grahpSAGE98_92_200_graph_embeddings.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sage.eval()\n",
    "correct = 0\n",
    "for data in X_train:\n",
    "    out, emb = graph_sage(data.x, data.edge_index)  \n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    y_label = (data.y.tolist())\n",
    "    y_label = y_label[0].index(1.0)\n",
    "    pred_label = (pred.tolist())[0]\n",
    "    # print(pred_label)\n",
    "    # print(y_label)\n",
    "    if y_label == pred_label:\n",
    "        correct += 1            \n",
    "    # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "acc = correct / len(X_train)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9352750809061489"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train acc\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
