{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (2.5.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (5.9.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.11.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.22.4)\n",
      "Requirement already satisfied: requests in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (3.8.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (2022.2.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.7.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mai\\anaconda3\\lib\\site-packages (from torch_geometric) (1.0.2)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (5.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->torch_geometric) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->torch_geometric) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mai\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mai\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torch_geometric) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%pip install torch_geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "['done_all\\\\adder10_synth.txt', 'done_all\\\\adder11_synth.txt', 'done_all\\\\adder12_synth.txt', 'done_all\\\\adder13_synth.txt', 'done_all\\\\adder14_synth.txt', 'done_all\\\\adder15_synth.txt', 'done_all\\\\adder16_synth.txt', 'done_all\\\\adder17_synth.txt', 'done_all\\\\adder18_synth.txt', 'done_all\\\\adder19_synth.txt', 'done_all\\\\adder1_synth.txt', 'done_all\\\\adder20_synth.txt', 'done_all\\\\adder21_synth.txt', 'done_all\\\\adder22_synth.txt', 'done_all\\\\adder23_synth.txt', 'done_all\\\\adder24_synth.txt', 'done_all\\\\adder25_synth.txt', 'done_all\\\\adder26_synth.txt', 'done_all\\\\adder27_synth.txt', 'done_all\\\\adder28_synth.txt', 'done_all\\\\adder2_synth.txt', 'done_all\\\\adder3_synth.txt', 'done_all\\\\adder4_synth.txt', 'done_all\\\\adder5_synth.txt', 'done_all\\\\adder6_synth.txt', 'done_all\\\\adder7_synth.txt', 'done_all\\\\adder8_synth.txt', 'done_all\\\\adder9_synth.txt', 'done_all\\\\and10_gate_synth.txt', 'done_all\\\\and11_gate_synth.txt', 'done_all\\\\and12_gate_synth.txt', 'done_all\\\\and13_synth.txt', 'done_all\\\\and14_synth.txt', 'done_all\\\\and15_synth.txt', 'done_all\\\\and16_synth.txt', 'done_all\\\\and17_synth.txt', 'done_all\\\\and18_gate_synth.txt', 'done_all\\\\and19_synth.txt', 'done_all\\\\and1_synth.txt', 'done_all\\\\and20_synth.txt', 'done_all\\\\and21_synth.txt', 'done_all\\\\and22_synth.txt', 'done_all\\\\and23_synth.txt', 'done_all\\\\and24_synth.txt', 'done_all\\\\and25_synth.txt', 'done_all\\\\and26_synth.txt', 'done_all\\\\and27_synth.txt', 'done_all\\\\and28_synth.txt', 'done_all\\\\and29_synth.txt', 'done_all\\\\and2_gate_synth.txt', 'done_all\\\\and30_synth.txt', 'done_all\\\\and3_gate_synth.txt', 'done_all\\\\and4_gate_synth.txt', 'done_all\\\\and5_gate_synth.txt', 'done_all\\\\and6_gate_synth.txt', 'done_all\\\\and7_gate_synth.txt', 'done_all\\\\and8_gate_synth.txt', 'done_all\\\\and9_gate_synth.txt', 'done_all\\\\comparator10_synth.txt', 'done_all\\\\comparator11_synth.txt', 'done_all\\\\comparator12_synth.txt', 'done_all\\\\comparator13_synth.txt', 'done_all\\\\comparator14_synth.txt', 'done_all\\\\comparator15_synth.txt', 'done_all\\\\comparator16_synth.txt', 'done_all\\\\comparator17_synth.txt', 'done_all\\\\comparator18_synth.txt', 'done_all\\\\comparator19_synth.txt', 'done_all\\\\comparator1_synth.txt', 'done_all\\\\comparator20_synth.txt', 'done_all\\\\comparator21_synth.txt', 'done_all\\\\comparator22_synth.txt', 'done_all\\\\comparator2_synth.txt', 'done_all\\\\comparator3_synth.txt', 'done_all\\\\comparator4_synth.txt', 'done_all\\\\comparator5_synth.txt', 'done_all\\\\comparator6_synth.txt', 'done_all\\\\comparator7_synth.txt', 'done_all\\\\comparator8_synth.txt', 'done_all\\\\comparator9_synth.txt', 'done_all\\\\decoder10_synth.txt', 'done_all\\\\decoder11_synth.txt', 'done_all\\\\decoder12_synth.txt', 'done_all\\\\decoder13_synth.txt', 'done_all\\\\decoder14_synth.txt', 'done_all\\\\decoder15_synth.txt', 'done_all\\\\decoder16_synth.txt', 'done_all\\\\decoder17_synth.txt', 'done_all\\\\decoder18_synth.txt', 'done_all\\\\decoder19_synth.txt', 'done_all\\\\decoder1_synth.txt', 'done_all\\\\decoder20_synth.txt', 'done_all\\\\decoder21_synth.txt', 'done_all\\\\decoder22_synth.txt', 'done_all\\\\decoder23_synth.txt', 'done_all\\\\decoder24_synth.txt', 'done_all\\\\decoder25_synth.txt', 'done_all\\\\decoder26_synth.txt', 'done_all\\\\decoder27_synth.txt', 'done_all\\\\decoder28_synth.txt', 'done_all\\\\decoder29_synth.txt', 'done_all\\\\decoder2_synth.txt', 'done_all\\\\decoder30_synth.txt', 'done_all\\\\decoder31_synth.txt', 'done_all\\\\decoder32_synth.txt', 'done_all\\\\decoder3_synth.txt', 'done_all\\\\decoder4_synth.txt', 'done_all\\\\decoder5_synth.txt', 'done_all\\\\decoder6_synth.txt', 'done_all\\\\decoder7_synth.txt', 'done_all\\\\decoder8_synth.txt', 'done_all\\\\decoder9_synth.txt', 'done_all\\\\encoder10_synth.txt', 'done_all\\\\encoder11_synth.txt', 'done_all\\\\encoder12_synth.txt', 'done_all\\\\encoder13_synth.txt', 'done_all\\\\encoder14_synth.txt', 'done_all\\\\encoder15_synth.txt', 'done_all\\\\encoder16_synth.txt', 'done_all\\\\encoder17_synth.txt', 'done_all\\\\encoder18_synth.txt', 'done_all\\\\encoder19_synth.txt', 'done_all\\\\encoder1_synth.txt', 'done_all\\\\encoder20_synth.txt', 'done_all\\\\encoder21_synth.txt', 'done_all\\\\encoder22_synth.txt', 'done_all\\\\encoder23_synth.txt', 'done_all\\\\encoder24_synth.txt', 'done_all\\\\encoder25_synth.txt', 'done_all\\\\encoder2_synth.txt', 'done_all\\\\encoder3_synth.txt', 'done_all\\\\encoder4_synth.txt', 'done_all\\\\encoder5_synth.txt', 'done_all\\\\encoder6_synth.txt', 'done_all\\\\encoder7_synth.txt', 'done_all\\\\encoder8_synth.txt', 'done_all\\\\encoder9_synth.txt', 'done_all\\\\mult10_synth.txt', 'done_all\\\\mult11_synth.txt', 'done_all\\\\mult12_synth.txt', 'done_all\\\\mult13_synth.txt', 'done_all\\\\mult14_synth.txt', 'done_all\\\\mult15_synth.txt', 'done_all\\\\mult16_synth.txt', 'done_all\\\\mult17_synth.txt', 'done_all\\\\mult18_synth.txt', 'done_all\\\\mult19_synth.txt', 'done_all\\\\mult1_synth.txt', 'done_all\\\\mult20_synth.txt', 'done_all\\\\mult21_synth.txt', 'done_all\\\\mult22_synth.txt', 'done_all\\\\mult23_synth.txt', 'done_all\\\\mult25_synth.txt', 'done_all\\\\mult26_synth.txt', 'done_all\\\\mult27_synth.txt', 'done_all\\\\mult28_synth.txt', 'done_all\\\\mult29_synth.txt', 'done_all\\\\mult2_synth.txt', 'done_all\\\\mult30_synth.txt', 'done_all\\\\mult3_synth.txt', 'done_all\\\\mult4_synth.txt', 'done_all\\\\mult5_synth.txt', 'done_all\\\\mult6_synth.txt', 'done_all\\\\mult7_synth.txt', 'done_all\\\\mult8_synth.txt', 'done_all\\\\mult9_synth.txt', 'done_all\\\\mux10_synth.txt', 'done_all\\\\mux11_synth.txt', 'done_all\\\\mux12_synth.txt', 'done_all\\\\mux13_synth.txt', 'done_all\\\\mux15_synth.txt', 'done_all\\\\mux16_synth.txt', 'done_all\\\\mux17_synth.txt', 'done_all\\\\mux18_synth.txt', 'done_all\\\\mux19_synth.txt', 'done_all\\\\mux1_synth.txt', 'done_all\\\\mux20_synth.txt', 'done_all\\\\mux21_synth.txt', 'done_all\\\\mux22_synth.txt', 'done_all\\\\mux23_synth.txt', 'done_all\\\\mux24_synth.txt', 'done_all\\\\mux25_synth.txt', 'done_all\\\\mux26_synth.txt', 'done_all\\\\mux27_synth.txt', 'done_all\\\\mux28_synth.txt', 'done_all\\\\mux2_synth.txt', 'done_all\\\\mux3_synth.txt', 'done_all\\\\mux4_synth.txt', 'done_all\\\\mux5_synth.txt', 'done_all\\\\mux6_synth.txt', 'done_all\\\\mux7_synth.txt', 'done_all\\\\mux8_synth.txt', 'done_all\\\\mux9_synth.txt', 'done_all\\\\nand10_synth.txt', 'done_all\\\\nand11_synth.txt', 'done_all\\\\nand12_gate_synth.txt', 'done_all\\\\nand13_synth.txt', 'done_all\\\\nand14_synth.txt', 'done_all\\\\nand15_synth.txt', 'done_all\\\\nand16_synth.txt', 'done_all\\\\nand17_synth.txt', 'done_all\\\\nand18_synth.txt', 'done_all\\\\nand19_synth.txt', 'done_all\\\\nand1_synth.txt', 'done_all\\\\nand20_gate_synth.txt', 'done_all\\\\nand21_synth.txt', 'done_all\\\\nand22_synth.txt', 'done_all\\\\nand23_synth.txt', 'done_all\\\\nand24_synth.txt', 'done_all\\\\nand25_synth.txt', 'done_all\\\\nand26_synth.txt', 'done_all\\\\nand27_synth.txt', 'done_all\\\\nand28_synth.txt', 'done_all\\\\nand29_synth.txt', 'done_all\\\\nand2_gate_synth.txt', 'done_all\\\\nand30_synth.txt', 'done_all\\\\nand31_synth.txt', 'done_all\\\\nand3_gate_synth.txt', 'done_all\\\\nand4_gate_synth.txt', 'done_all\\\\nand5_gate_synth.txt', 'done_all\\\\nand6_gate_synth.txt', 'done_all\\\\nand7_gate_synth.txt', 'done_all\\\\nand8_gate_synth.txt', 'done_all\\\\nand9_gate_synth.txt', 'done_all\\\\nor10_synth.txt', 'done_all\\\\nor11_synth.txt', 'done_all\\\\nor12_gate_synth.txt', 'done_all\\\\nor13_synth.txt', 'done_all\\\\nor14_synth.txt', 'done_all\\\\nor15_synth.txt', 'done_all\\\\nor16_synth.txt', 'done_all\\\\nor17_synth.txt', 'done_all\\\\nor18_synth.txt', 'done_all\\\\nor19_synth.txt', 'done_all\\\\nor1_synth.txt', 'done_all\\\\nor20_synth.txt', 'done_all\\\\nor21_synth.txt', 'done_all\\\\nor22_synth.txt', 'done_all\\\\nor23_synth.txt', 'done_all\\\\nor24_synth.txt', 'done_all\\\\nor25_synth.txt', 'done_all\\\\nor26_synth.txt', 'done_all\\\\nor27_gate_synth.txt', 'done_all\\\\nor28_synth.txt', 'done_all\\\\nor29_synth.txt', 'done_all\\\\nor2_gate_synth.txt', 'done_all\\\\nor30_synth.txt', 'done_all\\\\nor3_gate_synth.txt', 'done_all\\\\nor4_gate_synth.txt', 'done_all\\\\nor5_gate_synth.txt', 'done_all\\\\nor6_gate_synth.txt', 'done_all\\\\nor7_synth.txt', 'done_all\\\\nor8_gate_synth.txt', 'done_all\\\\nor9_synth.txt', 'done_all\\\\not10_synth.txt', 'done_all\\\\not11_synth.txt', 'done_all\\\\not12_synth.txt', 'done_all\\\\not13_synth.txt', 'done_all\\\\not14_synth.txt', 'done_all\\\\not15_synth.txt', 'done_all\\\\not16_synth.txt', 'done_all\\\\not1_synth.txt', 'done_all\\\\not2_synth.txt', 'done_all\\\\not3_synth.txt', 'done_all\\\\not4_synth.txt', 'done_all\\\\not5_synth.txt', 'done_all\\\\not6_synth.txt', 'done_all\\\\not7_synth.txt', 'done_all\\\\not8_synth.txt', 'done_all\\\\not9_synth.txt', 'done_all\\\\or11_synth.txt', 'done_all\\\\or13_synth.txt', 'done_all\\\\or14_synth.txt', 'done_all\\\\or15_synth.txt', 'done_all\\\\or16_synth.txt', 'done_all\\\\or17_gate_synth.txt', 'done_all\\\\or18_synth.txt', 'done_all\\\\or19_synth.txt', 'done_all\\\\or1_synth.txt', 'done_all\\\\or20_synth.txt', 'done_all\\\\or21_synth.txt', 'done_all\\\\or22_synth.txt', 'done_all\\\\or23_synth.txt', 'done_all\\\\or24_synth.txt', 'done_all\\\\or25_synth.txt', 'done_all\\\\or26_synth.txt', 'done_all\\\\or27_synth.txt', 'done_all\\\\or28_synth.txt', 'done_all\\\\or29_synth.txt', 'done_all\\\\or2_gate_synth.txt', 'done_all\\\\or3_gate_synth.txt', 'done_all\\\\or4_gate_synth.txt', 'done_all\\\\or5_gate_synth.txt', 'done_all\\\\or6_gate_synth.txt', 'done_all\\\\or7_synth.txt', 'done_all\\\\or8_gate_synth.txt', 'done_all\\\\or9_synth.txt', 'done_all\\\\pe10_synth.txt', 'done_all\\\\pe11_synth.txt', 'done_all\\\\pe12_synth.txt', 'done_all\\\\pe13_synth.txt', 'done_all\\\\pe14_synth.txt', 'done_all\\\\pe15_synth.txt', 'done_all\\\\pe16_synth.txt', 'done_all\\\\pe17_synth.txt', 'done_all\\\\pe18_synth.txt', 'done_all\\\\pe19_synth.txt', 'done_all\\\\pe1_synth.txt', 'done_all\\\\pe20_synth.txt', 'done_all\\\\pe21_synth.txt', 'done_all\\\\pe22_synth.txt', 'done_all\\\\pe2_synth.txt', 'done_all\\\\pe3_synth.txt', 'done_all\\\\pe4_synth.txt', 'done_all\\\\pe5_synth.txt', 'done_all\\\\pe6_synth.txt', 'done_all\\\\pe8_synth.txt', 'done_all\\\\pe9_synth.txt', 'done_all\\\\seg1_synth.txt', 'done_all\\\\seg2_synth.txt', 'done_all\\\\seg3_synth.txt', 'done_all\\\\seg4_synth.txt', 'done_all\\\\seg5_synth.txt', 'done_all\\\\seg6_synth.txt', 'done_all\\\\seg7_synth.txt', 'done_all\\\\seg8_synth.txt', 'done_all\\\\seg9_synth.txt', 'done_all\\\\sub10_synth.txt', 'done_all\\\\sub11_synth.txt', 'done_all\\\\sub12_synth.txt', 'done_all\\\\sub1_synth.txt', 'done_all\\\\sub2_synth.txt', 'done_all\\\\sub4_synth.txt', 'done_all\\\\sub5_synth.txt', 'done_all\\\\sub6_synth.txt', 'done_all\\\\sub7_synth.txt', 'done_all\\\\sub8_synth.txt', 'done_all\\\\sub9_synth.txt', 'done_all\\\\xnor10_synth.txt', 'done_all\\\\xnor11_synth.txt', 'done_all\\\\xnor12_synth.txt', 'done_all\\\\xnor13_synth.txt', 'done_all\\\\xnor14_synth.txt', 'done_all\\\\xnor15_synth.txt', 'done_all\\\\xnor16_synth.txt', 'done_all\\\\xnor17_synth.txt', 'done_all\\\\xnor18_synth.txt', 'done_all\\\\xnor19_synth.txt', 'done_all\\\\xnor1_synth.txt', 'done_all\\\\xnor20_synth.txt', 'done_all\\\\xnor21_synth.txt', 'done_all\\\\xnor22_synth.txt', 'done_all\\\\xnor23_synth.txt', 'done_all\\\\xnor24_synth.txt', 'done_all\\\\xnor25_synth.txt', 'done_all\\\\xnor26_synth.txt', 'done_all\\\\xnor27_synth.txt', 'done_all\\\\xnor28_synth.txt', 'done_all\\\\xnor29_synth.txt', 'done_all\\\\xnor2_synth.txt', 'done_all\\\\xnor30_synth.txt', 'done_all\\\\xnor3_synth.txt', 'done_all\\\\xnor4_synth.txt', 'done_all\\\\xnor5_synth.txt', 'done_all\\\\xnor6_synth.txt', 'done_all\\\\xnor7_synth.txt', 'done_all\\\\xnor8_synth.txt', 'done_all\\\\xnor9_synth.txt', 'done_all\\\\xor10_synth.txt', 'done_all\\\\xor11_synth.txt', 'done_all\\\\xor12_synth.txt', 'done_all\\\\xor13_synth.txt', 'done_all\\\\xor14_synth.txt', 'done_all\\\\xor15_synth.txt', 'done_all\\\\xor16_synth.txt', 'done_all\\\\xor17_synth.txt', 'done_all\\\\xor18_synth.txt', 'done_all\\\\xor19_synth.txt', 'done_all\\\\xor1_synth.txt', 'done_all\\\\xor20_synth.txt', 'done_all\\\\xor21_synth.txt', 'done_all\\\\xor22_synth.txt', 'done_all\\\\xor23_synth.txt', 'done_all\\\\xor24_synth.txt', 'done_all\\\\xor25_synth.txt', 'done_all\\\\xor26_synth.txt', 'done_all\\\\xor27_synth.txt', 'done_all\\\\xor28_synth.txt', 'done_all\\\\xor29_synth.txt', 'done_all\\\\xor2_synth.txt', 'done_all\\\\xor3_synth.txt', 'done_all\\\\xor4_synth.txt', 'done_all\\\\xor5_synth.txt', 'done_all\\\\xor6_synth.txt', 'done_all\\\\xor7_synth.txt', 'done_all\\\\xor8_synth.txt']\n"
     ]
    }
   ],
   "source": [
    "def get_files_in_folder(input_folder):\n",
    "    file_list = []\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    return file_list\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'done_all'\n",
    "verilog_files = get_files_in_folder(folder_path)\n",
    "print(len(verilog_files))\n",
    "print(verilog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_attributes(verilog_file):\n",
    "    try:\n",
    "        if os.path.isfile(verilog_file):\n",
    "            with open(verilog_file, \"r\") as file:\n",
    "                loaded_data = json.load(file)\n",
    "                nodes = loaded_data[0]\n",
    "                edges = loaded_data[1]\n",
    "                label = loaded_data[2]\n",
    "                \n",
    "                x = torch.tensor(nodes, dtype=torch.float)\n",
    "                edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float)\n",
    "                num_nodes = x.size(0)\n",
    "                \n",
    "                # Create batch assignment vector (assuming one graph per file)\n",
    "                batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y = y, batch = batch)\n",
    "                return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 396 Verilog files.\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "class VerilogDataset(Dataset):  # Using Dataset from torch_geometric\n",
    "    def __init__(self, verilog_files):\n",
    "        print(f\"Loaded {len(verilog_files)} Verilog files.\")\n",
    "        self.verilog_files = verilog_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.verilog_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verilog_file = self.verilog_files[idx]\n",
    "        data = extracting_attributes(verilog_file)\n",
    "        return data\n",
    "\n",
    "dataset = VerilogDataset(verilog_files)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[54, 7], edge_index=[2, 72], y=[1, 16], batch=[54])\n",
      "done_all\\adder10_synth.txt\n",
      "done_all\\adder10_synth.txt\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(verilog_files[0])\n",
    "print(dataset.verilog_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data objects are unique.\n"
     ]
    }
   ],
   "source": [
    "def are_all_data_objects_unique(dataset):\n",
    "    data_objects = []\n",
    "    for data in dataset:\n",
    "        if data in data_objects:\n",
    "            return False\n",
    "        data_objects.append(data)\n",
    "    return True\n",
    "\n",
    "# Example usage:\n",
    "is_unique = are_all_data_objects_unique(dataset)\n",
    "if is_unique:\n",
    "    print(\"All data objects are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate data objects found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = []\n",
    "for data in dataset:\n",
    "    # print(data)\n",
    "    # print(data.y.tolist())\n",
    "    y_labels.append(np.argmax(data.y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return default_collate(batch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y_labels, test_size=0.2, stratify = y_labels, random_state=41)\n",
    "train_loader = DataLoader(X_train, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(X_test, batch_size=16, shuffle = False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[82, 7], edge_index=[2, 109], y=[1, 16], batch=[82])\n"
     ]
    }
   ],
   "source": [
    "# len(train_loader.dataset)\n",
    "print(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(7, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x_temp = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x_temp, batch=None)\n",
    "        return x, x_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_pairs(dataset):\n",
    "#     pairs = []\n",
    "#     pair_labels = []\n",
    "#     for i in range(len(dataset)):\n",
    "#         for j in range(i + 1, len(dataset)):\n",
    "#             pairs.append((dataset[i], dataset[j]))\n",
    "#             pair_labels.append(1 if torch.all(dataset[i].y == dataset[j].y).tolist() else 0)\n",
    "#     return pairs, pair_labels\n",
    "\n",
    "# # Example usage\n",
    "# pairs, pair_labels = generate_pairs(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class SiameseGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(SiameseGNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(16, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # print(x)\n",
    "        x = global_mean_pool(x, data.batch)  # [batch_size, hidden_channels]\n",
    "        return x\n",
    "\n",
    "    # def compute_similarity(self, data1, data2):\n",
    "    #     emb1 = self.forward(data1)\n",
    "    #     emb2 = self.forward(data2)\n",
    "    #     return F.cosine_similarity(emb1, emb2), emb1, emb2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Hybrid_Model, self).__init__()\n",
    "        hidden_channels = 16\n",
    "        num_classes = 16\n",
    "        self.sage = GraphSAGE(hidden_channels, 16)\n",
    "        self.siamese = SiameseGNN(hidden_channels, num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # print(f\"Input x shape: {x.shape}\")  # Debugging shape of input features\n",
    "        x, x_temp = self.sage(data)\n",
    "        # print(f\"SAGE output x shape: {x.shape}, x_temp shape: {x_temp.shape}\")  # Debugging shape after GraphSAGE\n",
    "        # data.x = x_temp\n",
    "        # print(f\"Data x shape after SAGE: {data.x.shape}\")\n",
    "        x = self.siamese(x_temp, edge_index)\n",
    "        # print(f\"Siamese output shape: {x.shape}\")  # Debugging shape after SiameseGNN\n",
    "        return x\n",
    "    \n",
    "    def compute_similarity(self, data1, data2):\n",
    "        emb1 = self.forward(data1)\n",
    "        emb2 = self.forward(data2)\n",
    "        return F.cosine_similarity(emb1, emb2), emb1, emb2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contrastive_loss(similarity, label, margin=0.5):\n",
    "#     loss = (label * (1 - similarity)**2 + (1 - label) * F.relu(similarity - margin)**2).mean()\n",
    "#     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4707392272126825, Train Acc: 0.2025\n",
      "Epoch 10, Loss: 0.8680489645783475, Train Acc: 0.7342\n",
      "Epoch 20, Loss: 0.6265674432971735, Train Acc: 0.8133\n",
      "Epoch 30, Loss: 0.5063398475447612, Train Acc: 0.8481\n",
      "Epoch 40, Loss: 0.4390327516641175, Train Acc: 0.8608\n",
      "Epoch 50, Loss: 0.3932381877526125, Train Acc: 0.8703\n",
      "Epoch 60, Loss: 0.35720610774686196, Train Acc: 0.8861\n",
      "Epoch 70, Loss: 0.3258894543317542, Train Acc: 0.8924\n",
      "Epoch 80, Loss: 0.29915498563918175, Train Acc: 0.9019\n",
      "Epoch 90, Loss: 0.27756248018375196, Train Acc: 0.9114\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_channels = 16\n",
    "num_classes = 16\n",
    "sage_model = GraphSAGE(hidden_channels, num_classes)\n",
    "# Create a simple training loop\n",
    "\n",
    "optimizer = torch.optim.Adam(sage_model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "# Training the model\n",
    "sage_model.train()\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data in X_train:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out, _ = sage_model(data)\n",
    "        loss =loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        y_label = (data.y.tolist())\n",
    "        y_label = y_label[0].index(1.0)\n",
    "        pred_label = (pred.tolist())[0]\n",
    "        # print(pred_label)\n",
    "        # print(y_label)\n",
    "        if y_label == pred_label:\n",
    "            correct += 1            \n",
    "    train_acc = correct/len(X_train)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss/len(X_train)}, Train Acc: {train_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9177215189873418"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "sage_model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in X_test:\n",
    "        out,_ = sage_model(data)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        y_label = (data.y.tolist())\n",
    "        y_label = y_label[0].index(1.0)\n",
    "        pred_label = (pred.tolist())[0]\n",
    "        # print(pred_label)\n",
    "        # print(y_label)\n",
    "        if y_label == pred_label:\n",
    "            correct += 1            \n",
    "        # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    acc = correct / len(X_test)  # Derive ratio of correct predictions.\n",
    "\n",
    "print(f'Test Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sage_model.state_dict(), 'grahpSAGE91_85_100_siamese.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sage_model = GraphSAGE(hidden_channels, num_classes)\n",
    "sage_model.load_state_dict(torch.load('grahpSAGE90_87_100_siamese.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.294285904708637, Train Acc: 0.2943\n",
      "Epoch 10, Loss: 1.411292207202643, Train Acc: 0.5411\n",
      "Epoch 20, Loss: 1.1445755970017422, Train Acc: 0.6361\n",
      "Epoch 30, Loss: 0.9778353791078717, Train Acc: 0.6519\n",
      "Epoch 40, Loss: 0.8221197267764724, Train Acc: 0.7278\n",
      "Epoch 50, Loss: 1.007420621106803, Train Acc: 0.6361\n",
      "Epoch 60, Loss: 0.9233191209988623, Train Acc: 0.6551\n",
      "Epoch 70, Loss: 0.9368255968798633, Train Acc: 0.6392\n",
      "Epoch 80, Loss: 0.8753841897201862, Train Acc: 0.6709\n",
      "Epoch 90, Loss: 0.8378846690696073, Train Acc: 0.6930\n",
      "Epoch 99, Loss: 0.9430141320811107, Train Acc: 0.6582\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_channels = 16\n",
    "num_classes = 16\n",
    "siamese_model = SiameseGNN(hidden_channels, num_classes)\n",
    "# Create a simple training loop\n",
    "\n",
    "optimizer = torch.optim.Adam(siamese_model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "sage_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Training the model\n",
    "siamese_model.train()\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for data in X_train:\n",
    "        optimizer.zero_grad()\n",
    "        out,emb = sage_model(data)  \n",
    "        # print(emb.shape)\n",
    "        # break\n",
    "        data = data.to(device)\n",
    "        out = siamese_model(emb, data.edge_index)\n",
    "        loss =loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        y_label = (data.y.tolist())\n",
    "        y_label = y_label[0].index(1.0)\n",
    "        pred_label = (pred.tolist())[0]\n",
    "        # print(pred_label)\n",
    "        # print(y_label)\n",
    "        if y_label == pred_label:\n",
    "            correct += 1            \n",
    "    # break\n",
    "    train_acc = correct/len(X_train)\n",
    "    if epoch % 10 == 0 or epoch == 99:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss/len(X_train)}, Train Acc: {train_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese_model.eval()\n",
    "# correct = 0\n",
    "# with torch.no_grad():\n",
    "#     for data in X_test:\n",
    "#         out,_ = siamese_model(data)  \n",
    "#         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#         y_label = (data.y.tolist())\n",
    "#         y_label = y_label[0].index(1.0)\n",
    "#         pred_label = (pred.tolist())[0]\n",
    "#         # print(pred_label)\n",
    "#         # print(y_label)\n",
    "#         if y_label == pred_label:\n",
    "#             correct += 1            \n",
    "#         # correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "#     acc = correct / len(X_test)  # Derive ratio of correct predictions.\n",
    "\n",
    "# print(f'Test Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(siamese_model.state_dict(), 'siamese_100_sage.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4858641728192947\n",
      "Epoch 10, Loss: 0.9759160267614005\n",
      "Epoch 20, Loss: 0.7507696524236805\n",
      "Epoch 30, Loss: 0.643648432793802\n",
      "Epoch 40, Loss: 0.5761470738562112\n",
      "Epoch 50, Loss: 0.4995561326920692\n",
      "Epoch 60, Loss: 0.43065840111303866\n",
      "Epoch 70, Loss: 0.34306453362453065\n",
      "Epoch 80, Loss: 0.27793076737033356\n",
      "Epoch 90, Loss: 0.22167794258981163\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hybrid_model = Hybrid_Model()\n",
    "# Create a simple training loop\n",
    "\n",
    "optimizer = torch.optim.Adam(hybrid_model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "# Training the model\n",
    "hybrid_model.train()\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    # for i, (data1, data2) in enumerate(pairs):\n",
    "    for data in X_train:\n",
    "        optimizer.zero_grad()\n",
    "        # data1 = data1.to(device)\n",
    "        # data2 = data2.to(device)\n",
    "        data = data.to(device)\n",
    "        # print(data1)\n",
    "        #forward pass\n",
    "        out = hybrid_model(data)\n",
    "        # print(similarity)\n",
    "        # print(loss)\n",
    "        # loss = contrastive_loss(similarity, pair_labels[i])\n",
    "        # zero the gradients of the weights so that the gradients are not accumulated\n",
    "        # calculate the gradients using backpropagation\n",
    "        # print(out)\n",
    "        # print(data.y)\n",
    "        loss =loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss/len(X_train)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
